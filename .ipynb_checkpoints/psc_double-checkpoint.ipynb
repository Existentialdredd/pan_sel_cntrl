{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preamble: Package Loading\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "import ipywidgets as ipw\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "import itertools as iter\n",
    "import os\n",
    "import datetime as dt\n",
    "import json\n",
    "import re\n",
    "# Preamble working directory retreival\n",
    "wkng_folder = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Parametric Panel, Selection, and Control </h1>\n",
    "<h3> Proposal Supplement </h3>\n",
    "By: Eric Penner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1 Model Setup </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.1 Base Model </h3>\n",
    "\n",
    "For each time period $t \\in \\{1,2, \\ldots, T\\}$, component $ d\\in \\{1,2, \\ldots , p_1\\}$ of $Z_{1jt}$, and cross-section $j \\in \\{1,2,\\ldots, q\\}$ where $\\{q,T\\} \\in \\mathbb{N}$ consider the following,\n",
    "\n",
    "\\begin{align} \n",
    "Y_{jt} &= \\beta_0 + [\\; Z_{1jt}' \\;\\; Z_{2jt}' \\;] \\beta_1 + e_j + \\varepsilon_{jt} \\\\[1em]\n",
    "%\n",
    "Z_{1jt,d} &= \\alpha_{0jd} + Z_{2jt}' \\alpha_{1jd} + W_{jt}' \\alpha_{2jd} + V_{jt,d} \\tag{2d} \\\\[1em]\n",
    "%\n",
    "E(&\\varepsilon_{jt} | Z_{1jt} , Z_{2jt}) = E(\\varepsilon_{jt} | Z_{1jt}) \\neq 0  \\\\[1em]\n",
    "%\n",
    "E(& V_{jdt} | Z_{1j(t-1)},\\ldots,Z_{1j(t-c)},Z_{2jt},W_{jt}) = 0 \n",
    "%\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where for $\\{p_1,p_2,c,w,w_1,w_2, \\ldots ,w_q \\} \\subset \\mathbb{N}$, \n",
    "\n",
    "* $Y_{jt}$, and $\\varepsilon_{jt}$ are scalar random variables, \n",
    "\n",
    "* $Z_{2jt}$ is a $p_2$ dimesion vector of exogenous random variables, \n",
    "\n",
    "* $Z_{1jt}$  is a vector of endogenous random variables having dimension $p_1$. \n",
    "\n",
    "* $e_j$ is a scalar fixed effect, \n",
    "\n",
    "* $\\varepsilon_{jt}$ is a scalar error term, \n",
    "\n",
    "* $V_{jt,d}$ is a scalar error term. \n",
    "\n",
    "* $W_t = [ \\; W_{1t} \\;\\; W_{2t} \\;\\; \\cdots \\;\\; W_{wt} \\;]'$ is vector of instrumental variables of dimension $w$, \n",
    "\n",
    "* $W_{jt} = [\\;W_{jt,1} \\;\\; W_{jt,2} \\;\\; \\cdots \\;\\; W_{jt,w_j} \\;]'$ is a vector of instrumental variables of dimension $w_j$ where $\\{W_{jt,l}\\}_{l=1}^{w_j} \\subset \\{W_{t,l}\\}_{l=1}^{w}$ and there exists at least one pair $j,j'  \\in \\{1,2, \\ldots , p\\}$, where $j \\neq j'$, such that $\\{W_{jt,l}\\}_{l=1}^{w_j} \\neq \\{W_{j't,l}\\}_{l=1}^{w_{j'}}$.\n",
    "\n",
    "* $\\beta_0$ and $\\alpha_{0jd}$  are scalars, \n",
    "\n",
    "* $\\beta_1$, $\\alpha_{1d}$, $\\alpha_{2d}$, are $p_1 +p_2 = p$, $p_2$, and $w_j$ dimensional vectors of real numbers respectively. \n",
    "\n",
    "* Lastly for notational convenience let define the following equivalences, \n",
    "\\begin{align*} \n",
    "Z_{jt} = [\\; Z_{1jt}' \\;\\; Z_{2jt}' \\;]'  \\;\\;\\; \\text{ and } \\;\\;\\; V_{jt} = [ \\; V_{jt,1} \\; V_{jt,2} \\; \\cdots \\;V_{jt,p_1}\\;]'\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.2 Differenced Model with Control Functions </h3>\n",
    "\n",
    "As shown in the the main proposal I treat the presence of endogeneity with the control function approach and then first difference the primary equation to eliminate fixed effects, resulting in the following. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align} \n",
    "\\Delta Y_{jt} &= \\Delta Z_{jt}'\\beta_1 + \\sum_{d=1}^{p_1} \\Delta f_{jd}(V_{jt,d}) + \\Delta u_{jt} \\\\[1em]\n",
    "%\n",
    "Z_{1jt,d} &= \\alpha_{0jd} + Z_{2jt}' \\alpha_{1jd} + W_{jt}' \\alpha_{2jd} + V_{jt,d} \\tag{2d}\\\\[1em]\n",
    "E(&\\Delta u_{jt} | \\{Z_{j_k},W_{jk},X_{jk},V_{jk} \\}_{k=1}^t) = 0 \\\\[1em]\n",
    "%\n",
    "E(& V_{jt} | Z_{1jt-1},\\ldots,Z_{1jt-c_1},Z_{2jt},W_{jt}) = 0 \n",
    "%\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>  1.3 Identification of $\\beta_1$ </h3>\n",
    "\n",
    "I use a variation on the identification procedure in Manzan and Zerom to identify $\\beta_1$, this required the construction of the following density ratios, functions, and vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Density Ratio  \n",
    "\\begin{align*} \n",
    "\\phi_{jt} = \\frac{ \\prod_{d=1}^{p_1}p(V_{jt,d},V_{jc,d}) }{p(V_{jt},V_{jc})} \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*} \n",
    "H_{jd}(\\Delta Z_{jt}) &= E[\\phi_{jt} \\Delta Z_{jt} |V_{jt,d},V_{jc,d}] \\hspace{1.25cm} \n",
    "%\n",
    "H_{jd}(\\Delta Y_{jt}) = E[\\phi_{jt} \\Delta Y_{jt} |V_{jt,d},V_{jc,d}] \\\\\n",
    "%\n",
    "H_j(\\Delta Z_{jt}) &= \\sum_{d=1}^{p_1} H_{jd}(\\Delta Z_{jt})  \\hspace{2.5cm}\n",
    "%\n",
    "H_j(\\Delta Y_{jt}) = \\sum_{d=1}^{p_1} H_{jd}(\\Delta Y_{jt})  \\end{align*} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors\n",
    "\n",
    "\\begin{align*}\n",
    "H(\\Delta Y_t)  &= [ \\; H_1(\\Delta Y_{1t}) \\;\\; H_2(\\Delta Y_{2t}) \\;\\; \\cdots \\;\\; H_q(\\Delta Y_{qt}) \\; ]' \\\\[1em]\n",
    "H(\\Delta Z_t)  &= [ \\; H_1(\\Delta Z_{1t}) \\;\\; H_2(\\Delta Z_{2t}) \\;\\; \\cdots \\;\\; H_q(\\Delta Z_{qt}) \\; ]' \\\\[1em]\n",
    "%\n",
    "\\Delta Y_t &= [ \\; \\Delta Y_{1t} \\;\\;  \\Delta Y_{2t} \\;\\; \\cdots \\;\\; \\Delta Y_{qt} \\;] '  \\\\[1em]\n",
    "%\n",
    "\\Delta Z_t &= [ \\; \\Delta Z_{1t} \\;\\;  \\Delta Z_{2t} \\;\\; \\cdots \\;\\; \\Delta Z_{qt} \\;] ' \n",
    "\\\\[1em] \n",
    "\\Delta u_t &=  [ \\; \\Delta u_{1t} \\;\\;  \\Delta u_{2t} \\;\\; \\cdots \\;\\; \\Delta u_{qt} \\;] ' \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 1.3.1 Lemma 1 </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letting $\\phi_{t} = diag\\big( \\{\\phi_{jt}\\;\\}_{j=1}^q \\big)$, if for all $t\\in \\{2, \\ldots , T\\}$\n",
    "\n",
    "1. $E\\big[ f_{jd}(V_{jdt})\\big] = E\\big[ f_{jd}(V_{jdc}) \\big] \n",
    "$ for all $d\\in \\{1,2, \\ldots , p_1\\}$, $j\\in \\{1,2, \\cdots , q\\}$, and \n",
    "\n",
    "2. $E \\Big( [\\Delta Z_t - H(\\Delta Z_t)]' [\\Delta Z_t - H(\\Delta Z_t)] \\Big)$ is positive semi definite,\n",
    "\n",
    "Then $\\beta_1$ is identified, in particular,\n",
    "\n",
    "\\begin{align*} \n",
    "\\beta_1 = E \\Big( [\\Delta Z_t - H(\\Delta Z_t)]' \\phi_{t} [\\Delta Z_t - H(\\Delta Z_t)] \\Big)^{-1}E \\Big( [\\Delta Z_t - H(\\Delta Z_t)]' \\phi_{t} [\\Delta Y_t - H(\\Delta Y_t)] \\Big)\n",
    "\\end{align*} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2.0 Post Secondary Equation Estimation </h2>\n",
    "\n",
    "Here we will discuss the estimation of $\\beta_1$ assuming that estimates $\\hat{V}_{jt,d}$ have already been generated by some as yet unspecified procedure. Estimation of the secondary equation will be discussed in the following sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2.1  Density Estimation </h3>\n",
    "\n",
    "The following function(s) implement a rosenblatt kernel estimator where \n",
    "\n",
    "$$ \\hat{p}(\\hat{V}_{dt,l},\\hat{V}_{d(t-1),l}) \n",
    "= (nh_1h_2)^{-1}\\sum_{ i = 2 }^T k_1[h_1^{-1}(\\hat{V}_{di,l} -\\hat{V}_{dt,l})]k_2[h_2^{-1}(\\hat{V}_{di,l} -\\hat{V}_{d(t-1),l})]  \n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>  2.1.1 Density Estimation: Function to Calculate Kernel Values </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mvkernel(x,kernel):  \n",
    "    \"\"\" \n",
    "PURPOSE: Calculate product kernel values at points x\n",
    "    \n",
    "INPUTS\n",
    "x       points of kernel evaluation (3)x(n)x(dim of x)\n",
    "kernel  Scalar value indicating which kernel to use where\n",
    "           1 = Rectangular\n",
    "           2 = Triangular\n",
    "           3 = Biweight\n",
    "           4 = Silverman's \n",
    "           5 = Epanechnikov order 2\n",
    "           6 = Epanechnikov order 2 alt\n",
    "           7 = Epanechnikov order 4\n",
    "           8 = Epanechnikov order 6\n",
    "           9 = Gaussian order 2\n",
    "           10 = Gaussian order 4\n",
    "           11 = Gaussian order 6\n",
    "\n",
    "OUTPUTS\n",
    "ker     product of kernel values along axis = 0\n",
    "    \"\"\"\n",
    "    \n",
    "    if kernel == 1:\n",
    "        # Rectangular Kernel  \n",
    "        ker = 1/2*(np.absolute(x)<1)\n",
    "    elif kernel == 2:\n",
    "        # Triangular Kernel\n",
    "        ker = (1-np.absolute(x))*(np.absolute(x)<1)\n",
    "    elif kernel == 3:\n",
    "        # Biweight Kernel\n",
    "        ker = (15/16*(1-x**2)**2)*(np.absolute(x)< 1)\n",
    "    elif kernel == 4:\n",
    "        # Silvermans Kernel \n",
    "        ker = 0.5*np.exp(-np.absolute(x)/np.sqrt(2))*np.sin(np.absolute(x)/np.sqrt(2) + np.pi/4)   \n",
    "    elif kernel == 5:\n",
    "        # Epanechnikov order 2\n",
    "        ker = 0.75*(1-x**2)*(np.absolute(x)<=1)\n",
    "    elif kernel == 6:\n",
    "        # Epanechnikov order 2 (alt)?\n",
    "        ker = (0.75/np.sqrt(5))*(1-0.2*x**2)*(np.absolute(x)<=np.sqrt(5))\n",
    "    elif kernel == 7:\n",
    "        # Epanechnikov order 4\n",
    "        ker  = (0.75/np.sqrt(5))*(15/8-7/8*x**2)*(1-0.2*x**2)*(np.absolute(x)<=np.sqrt(5))\n",
    "    elif kernel == 8:\n",
    "        # Epanechnikov order 6\n",
    "        ker  = ((0.75/np.sqrt(5))*(175/64-105/32*x**2+ 231/320*x**4)\n",
    "                                 *(1-0.2*x**2)*(np.absolute(x)<=np.sqrt(5)))\n",
    "    elif kernel == 9:\n",
    "        # Gaussian order 2\n",
    "        ker  = (1/np.sqrt(2*np.pi))*np.exp(-0.5*x**2)\n",
    "    elif kernel == 10:\n",
    "        # Gaussian order 4\n",
    "        ker  = (3/2 -1/2*x**2)*(1/np.sqrt(2*np.pi))*np.exp(-0.5*x**2)\n",
    "    elif kernel == 11:\n",
    "        # Gaussian order 6\n",
    "        ker  = (15/8-5/4*x**2+1/8*x**4)*(1/np.sqrt(2*np.pi))*np.exp(-0.5*x**2)\n",
    "    else:\n",
    "        print('Incorrect Kernel Number')\n",
    "\n",
    "    if x.ndim > 2:\n",
    "        # Value of each product kernel\n",
    "        ker = np.prod(ker,axis = 0)\n",
    "\n",
    "    return ker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 2.1.2 Density Estimation:  Function to Calculate Estimated Density</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mvden(x,p,h,kernel): \n",
    "    \"\"\"\n",
    "Purpose: Multivariate Rosenblatt Kernel Density Estimator at points p based on sample x    \n",
    "    \n",
    "INPUTS\n",
    "x       np.array of data having order (number of observations)x(dimension of X)\n",
    "P       np.array of points of evaluation having order (number of points)x(dimension of X)\n",
    "h       Bandwidth vector of order (1)x(dimension of X)\n",
    "kernel  Scalar value indicating which kernel to use where\n",
    "           1 = Rectangular\n",
    "           2 = Triangular\n",
    "           3 = Biweight\n",
    "           4 = Silverman's \n",
    "           5 = Epanechnikov order 2\n",
    "           6 = Epanechnikov order 2 alt\n",
    "           7 = Epanechnikov order 4\n",
    "           8 = Epanechnikov order 6\n",
    "           9 = Gaussian order 2\n",
    "           10 = Gaussian order 4\n",
    "           11 = Gaussian order 6\n",
    "\n",
    "OUTPUTS\n",
    "den      Density at each point of evaluation (number of observation)x1\n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.array(x)\n",
    "    p = np.array(p)\n",
    "    h = np.array(h)\n",
    "    \n",
    "    if x.ndim == 1:\n",
    "            m0 = (x.reshape(1,x.shape[0])-p.reshape(p.shape[0],1))/h\n",
    "    elif x.ndim == 2:\n",
    "        m0 = np.zeros((x.shape[1],p.shape[0],x.shape[0])) \n",
    "        for i in range(0,x.shape[1]):\n",
    "            m0[i,:,:] = (x[:,i].reshape(1,x.shape[0])-p[:,i].reshape(p.shape[0],1))/h[i]\n",
    "\n",
    "    ker = mvkernel(m0,kernel)/(x.shape[0]*np.prod(h))\n",
    "    den = np.dot(ker,np.ones(x.shape[0]))\n",
    "\n",
    "    return den"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 2.1.3  Density Estimator Demonstration: Setup </h4>\n",
    "\n",
    "In order to validate the density function code in a setting similar to ours I will perform the following,\n",
    "\n",
    "$$ \\hat{p}(X_{t},X_{(t-1)}) \n",
    "= [(T-1)h_1h_2]^{-1}\\sum_{l=2}^Tk_1[h_1^{-1}(X_{l} -X_{t})]k_2[h_2^{-1}(X_{l} - X_{(t-1)})]  \\;\\; \\text{ where } \\;\\; X_t \\sim N(0,1)\n",
    "$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 2.1.4  Density Estimator Demonstration: Data Generation </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of data points\n",
    "n = 100\n",
    "# mean of x\n",
    "mu = np.array([0])\n",
    "# variance of x\n",
    "var = np.array([1])\n",
    "# generation of x\n",
    "x = np.random.normal(mu,var,n).reshape(n,1)\n",
    "# generation of df of x\n",
    "x = pd.DataFrame(x,columns = ['x1'])\n",
    "# adding a backshifted column to x\n",
    "x['Bx1'] = x.x1.shift(1)\n",
    "# removing the NA row\n",
    "x = x[1:]\n",
    "# Points of Evaluation Generation and Stacking\n",
    "npts = 20\n",
    "# Smallest coordinate value\n",
    "pl = -2\n",
    "# Largest coordinate value\n",
    "pu =  2\n",
    "p = np.linspace(pl,pu,npts).reshape((20,1))\n",
    "p = np.hstack((p,pl*np.ones((20,1))))\n",
    "for j in np.arange(1,20,1): \n",
    "    pt = np.hstack((p[0:20,0].reshape((20,1)),p[j,0]*np.ones((20,1))))\n",
    "    p = np.vstack((p,pt))\n",
    "p = pd.DataFrame(p, columns =['p1','p2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 2.1.3  Density Estimator Demonstration: Plotting Function </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def  plot_den(x,p,c_h,ker,el,az):\n",
    "    # Standard Normal pdf\n",
    "    fnrm = lambda x: np.exp(-x**2/2)/np.sqrt(2*np.pi)\n",
    "    # Plug in Bandwidths\n",
    "    h = c_h*x.shape[0]**(-1/5)*x.std(0)\n",
    "    # Collection into an array\n",
    "    h = np.array([h , h])\n",
    "    # Density estimation functions call\n",
    "    p['xden'] = mvden(np.array(x),np.array(p),h[0],ker)\n",
    "    plt.close('all')\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(2,1,subplot_kw = {'projection':'3d'})\n",
    "    fig.set_figheight(15)\n",
    "    fig.set_figwidth(15)\n",
    "    #fig.suptitle(\"Density\",fontsize = 20)\n",
    "    fig.tight_layout( pad = 0, h_pad = 0 , w_pad = 0)\n",
    "    ax[0].plot_trisurf(p.p1,p.p2,p.xden,alpha = 1)\n",
    "    ax[0].set_title('Estimated Density', fontsize = 24)\n",
    "    #ax[0].grid(b=None)\n",
    "    ax[1].plot_trisurf(p.p1,p.p2,fnrm(p.p1)*fnrm(p.p2),alpha = 1, color = 'g')\n",
    "    ax[1].set_title('True Density',fontsize = 24)\n",
    "    #ax[1].grid(b=None)\n",
    "    ax[0].view_init(elev=el, azim=az)\n",
    "    ax[1].view_init(elev=el, azim=az)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 2.1.4  Density Estimator Demonstration: Interactive Plotting </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc834fc1813f4f8dbb94acde9b6b6231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>VBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "VBox(children=(HBox(children=(Output(), IntSlider(value=30, description='Elevation', layout=Layout(align_items='stretch', display='flex', flex_flow='column', height='auto', justify_content='space-between', width='10%'), max=90, min=-90, orientation='vertical', readout=False, step=15))), VBox(children=(IntSlider(value=45, description='Azimuth', layout=Layout(align_items='stretch', display='flex', flex_flow='row', width='90%'), max=360, step=15, style=SliderStyle(description_width='initial')), FloatSlider(value=2.5, description='Bandwidth Constant', layout=Layout(align_items='stretch', display='flex', flex_flow='row', width='90%'), max=4.0, min=0.1, step=0.2, style=SliderStyle(description_width='initial')), IntSlider(value=6, description='Kernel', layout=Layout(align_items='stretch', display='flex', flex_flow='row', width='90%'), max=11, min=1, style=SliderStyle(description_width='initial'))))))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "box_hlayout = ipw.Layout(display='flex',\n",
    "                    flex_flow='row',\n",
    "                    align_items='stretch',\n",
    "                    width='90%')\n",
    "\n",
    "box_vlayout = ipw.Layout(display='flex', flex_flow='column', align_items='stretch',\n",
    "                    width='10%', height = 'auto', justify_content='space-between')\n",
    "\n",
    "s_az = ipw.IntSlider(min = 0 , max = 360, value = 45, step = 15, description = 'Azimuth',width = 'auto'\n",
    "              ,layout = box_hlayout, style = {'description_width': 'initial'} )\n",
    "s_c = ipw.FloatSlider(min = 0.1 , max = 4, value = 2.5, step =0.2, description = 'Bandwidth Constant'\n",
    "              ,width = 'auto',layout = box_hlayout, style = {'description_width': 'initial'})\n",
    "s_ker = ipw.IntSlider(min = 1 , max = 11, value = 6, description = 'Kernel',width = 'auto'\n",
    "              ,layout = box_hlayout, style = {'description_width': 'initial'})\n",
    "s_el = ipw.IntSlider(min = -90 , max = 90, value = 30, step = 15, description = 'Elevation'\n",
    "              ,orientation = 'vertical',length = 'auto',layout = box_vlayout\n",
    "              ,style = {'description_length': 'initial'},readout = False)\n",
    "\n",
    "out = ipw.interactive_output(plot_den,{'x': ipw.fixed(x),'p': ipw.fixed(p),'c_h':s_c, \n",
    "                                       'ker': s_ker,'az':s_az ,'el': s_el})\n",
    "ipw.VBox([ipw.HBox([out,s_el])   \n",
    "          ,ipw.VBox([s_az,s_c,s_ker])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2.2 Density Ratio Construction </h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will construct the following ratios directly within the main function code presented below\n",
    "\n",
    "\\begin{align*} \n",
    "\\hat{\\phi}_{jt} = \\frac{ \\prod_{d=1}^{p_1}\\hat{p}(\\hat{V}_{jt,d},\\hat{V}_{j(t-1),d}) }{\\hat{p}(\\hat{V}_{jt},\\hat{V}_{j(t-1)})}\n",
    "%\n",
    "\\hspace{1cm}\n",
    "%\n",
    "\\hat{\\theta}_{jt,d} = \\frac{ \\prod_{l \\neq d}^{p_1}\\hat{p}(\\hat{V}_{jt,l},\\hat{V}_{j(t-1),l}) }{\\hat{p}(\\hat{V}_{jt},\\hat{V}_{j(t-1)})}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2.3 H Function Estimation:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function implement a nadaraya watson estimator of the H functions \n",
    "$$ \\hat{H}_{j,d}(\\Delta Z_{jt,a}) = [(T-1)b_1b_2]^{-1} \\sum_{l \\neq t , l > 1}^T k_1[b_1^{-1}(V_{jl,d} - V_{jt,d})] k_2[b_2^{-1}(V_{j(l-1),d} - V_{j(t-1),d})] \\hat{\\theta}_{jl,d} \\Delta Z_{jl,a} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 2.3.1 H Function Estimation: Function Code </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Nw_H_pan(regrsnd,theta,regressr,p,h,kernel,leave_one = 1):\n",
    "    \"\"\"\n",
    "INPUTS \n",
    "regrsnd    Regressand of the regression (Delta Y or Delta Z) (# of obs - 1) x 1\n",
    "theta      Density ratio which will multiply regressand (# of obs - 1) x 1\n",
    "regressr   Vector of conditioning variables (V_{j,d}) (# of obsn -1 ) x 1\n",
    "p          points of evaluation (npoints x 2) \n",
    "b          Vector of Bandwidths [b1, b2] \n",
    "leave_one  Leave one out indicator; 1 leave one out, 0 dont\n",
    "\n",
    "OUTPUTS\n",
    "H       Vector of H function estimates corresponding to arguments p\n",
    "    \"\"\"\n",
    "    # Converting regressr input into np.array\n",
    "    reg = np.array(regressr)\n",
    "    # Setting up full regressor array where for a = 1,2,...,T-1, where \n",
    "    #           reg[a,:] =  [ V_{j(a+1),d} ,  V_{j(a),d} ]\n",
    "    #reg = np.vstack((reg[:-1],reg[1:])).T\n",
    "    # Converting regrsnd input to np.array\n",
    "    y = np.array(regrsnd)\n",
    "    # Converting theta input to np.array\n",
    "    tht = np.array(theta)\n",
    "    # Points of evaluation\n",
    "    p = np.array(p)\n",
    "    # Converting bandwidth input to np.array\n",
    "    b = np.array(h)\n",
    "    # Product of regressands and theta ratios\n",
    "    y_tht = y*tht\n",
    "\n",
    "    # Construction of each kernel argument array by broadcasting\n",
    "    m1 = (reg[:,0].reshape(1,reg.shape[0])-p[:,0].reshape(p.shape[0],1))/b[0]\n",
    "    m2 = (reg[:,1].reshape(1,reg.shape[0])-p[:,1].reshape(p.shape[0],1))/b[1]\n",
    "    # Initializing the kernel argument array \n",
    "    m0 = np.zeros((2,p.shape[0],reg.shape[0]))\n",
    "    # Placing broadcasted arrays in m0\n",
    "    m0[0,:,:] = m1\n",
    "    m0[1,:,:] = m2\n",
    "\n",
    "    # Calculating kernel values\n",
    "    ker = mvkernel(m0,kernel)\n",
    "\n",
    "    # Matrix of theta ratios so that observations match the first argument of kernels in H1\n",
    "    H0 = np.tile(y_tht,(p.shape[0],1))\n",
    "    # Multiply these two together\n",
    "    H1 = ker*H0;\n",
    "    # Deleting the diagonal which converts to a leave one out style\n",
    "    if leave_one == 1:\n",
    "        H2 = H1 - H1*np.eye(H1.shape[0]);\n",
    "    else: \n",
    "        H2 = H1\n",
    "    # Finishing the calculation at each point in X\n",
    "    H = 1./((reg.shape[0]-1)*b[0]*b[1])*np.dot(H2,np.ones((H2.shape[1],1)))\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>  2.3.2  H Function Estimation Demonstration: Setup </h4>\n",
    "\n",
    "Here we generate data as follows\n",
    "\n",
    "$$\n",
    "Y_t = f_1(a_1,b_1,X_{t-1}) + f_2(a_2,b_2,X_{t}) + \\varepsilon_t \\;\\;\\; \\text{ where } \\;\\;\\; X_t,\\varepsilon \\sim N(0,1) \n",
    "$$\n",
    "\n",
    "where $f_1$ and $f_2$ are defined in the following code. Its difficult to validate the function code with an example exactly like the dgp we are interested in so instead I demonstrate the following estimation. \n",
    "\n",
    "$$\n",
    "\\hat{E}[Y_t|X_t,X_{t-1}] = [(T-1)b_1b_2]^{-1} \\sum_{l \\neq t , l > 1}^T k_1[b_1^{-1}(X_{l} - X_{t})] k_2[b_2^{-1}(X_{(l-1)} - X_{(t-1)})] \\hat{\\theta}_{l} Y_l \\;\\;\\; \\text{ where } \\;\\;\\; \\hat{\\theta}_{l} = \\frac{1}{\\hat{p}(X_{l},X_{(l-1)})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 2.3.3  H Function Estimation Demonstration: Data Generation </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Regression Function Definition\n",
    "f1 = lambda a,b,x : a*np.cos(b*x)\n",
    "f2 = lambda a,b,x : a*np.sin(b*x)\n",
    "#f1 = lambda a,b,x: a*x**b\n",
    "#f2 = lambda a,b,x: a*x**(b)\n",
    "\n",
    "## Regressor and Error Generation\n",
    "# Number of observations\n",
    "n = 500\n",
    "# Vector of means\n",
    "mu = np.array([0,0])\n",
    "# Variance Covariance Matrix\n",
    "var = np.array([[1,0],[0,1]])\n",
    "# Data generation\n",
    "x = np.random.multivariate_normal(mu,var,n)\n",
    "# Coverting to pandas dataframe\n",
    "x = pd.DataFrame(x,columns = ['x1','ep'])\n",
    "# Backshifted regressor matrix\n",
    "reg = pd.concat([x.x1,x.x1.shift(1)],axis = 1).iloc[1:,:]\n",
    "## Regressand Generation\n",
    "# Parameters for conditional expectation functions\n",
    "a = [ 3 , 1 ]\n",
    "b = [ 2 , 3 ] \n",
    "# Conditional expectation functions at data points\n",
    "x2 = pd.DataFrame(f1(a[0],b[0],np.array(reg.iloc[:,0]))\n",
    "                  +f2(a[1],b[1],np.array(reg.iloc[:,1])),\n",
    "                  columns = ['m'])\n",
    "# Regressand Generation\n",
    "x2['y'] = x2.m + np.array(x.ep[1:])\n",
    "\n",
    "## Points of Evaluation Generation\n",
    "# Smallest coordinate value\n",
    "pl = -2\n",
    "# Largest coordinate value\n",
    "pu =  2\n",
    "# Grid of points will be npts X npts\n",
    "npts = 20\n",
    "# Points generation and Stacking\n",
    "p = np.linspace(pl,pu,npts).reshape((20,1))\n",
    "p = np.hstack((p,pl*np.ones((20,1))))\n",
    "for j in np.arange(1,20,1): \n",
    "    pt = np.hstack((p[0:20,0].reshape((20,1)),p[j,0]*np.ones((20,1))))\n",
    "    p = np.vstack((p,pt))\n",
    "p = pd.DataFrame(p, columns =['p1','p2'])\n",
    "\n",
    "## Constructing the Theta Ratio: 1/p(x_1t,x_1(t-1))\n",
    "# Plug-in Bandwidth\n",
    "h = 1.45*reg.shape[0]**(-1/5)*np.array(reg).std(0)\n",
    "# Density Estimate Function Call\n",
    "den = mvden(reg,reg,h,10)\n",
    "# Theta Ratio\n",
    "tht1 = 1/den"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 2.3.4  H Function Estimation Demonstration: Plotting Function </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Nw_H_plot(c_h,y,tht1,reg,p,ker,a,b,el,az):\n",
    "    plt.close('all')\n",
    "    h = c_h*reg.shape[0]**(-1/5)*np.array(reg).std(0)\n",
    "    ## Function Call\n",
    "    H = Nw_H_pan(y,tht1,reg,p,h,ker,0)\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(2,1,subplot_kw = {'projection':'3d'})\n",
    "    fig.set_figheight(15)\n",
    "    fig.set_figwidth(15)\n",
    "    ax[0].plot_trisurf(p.p1,p.p2,H[:,0])\n",
    "    ax[0].set_title('Estimated Function')\n",
    "    ax[1].plot_trisurf(p.p1,p.p2,f1(a[0],b[0],p.p1)+f2(a[1],b[1],p.p2),color = 'g')\n",
    "    ax[1].set_title('True Function')\n",
    "    ax[0].view_init(elev=el, azim=az)\n",
    "    ax[1].view_init(elev=el, azim=az)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 2.3.5 H Function Estimator Demonstration: Interactive Widgets Setup </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc54cfee72de4336a61c4ddc270397ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>VBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "VBox(children=(HBox(children=(Output(), IntSlider(value=30, description='Elevation', layout=Layout(align_items='stretch', display='flex', flex_flow='column', height='auto', justify_content='space-between', width='10%'), max=90, min=-90, orientation='vertical', readout=False, step=15))), VBox(children=(IntSlider(value=45, description='Azimuth', layout=Layout(align_items='stretch', display='flex', flex_flow='row', width='90%'), max=360, step=15, style=SliderStyle(description_width='initial')), FloatSlider(value=2.5, description='Bandwidth Constant', layout=Layout(align_items='stretch', display='flex', flex_flow='row', width='90%'), max=4.0, min=0.1, step=0.2, style=SliderStyle(description_width='initial')), IntSlider(value=6, description='Kernel', layout=Layout(align_items='stretch', display='flex', flex_flow='row', width='90%'), max=11, min=1, style=SliderStyle(description_width='initial'))))))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "box_hlayout = ipw.Layout(display='flex',\n",
    "                    flex_flow='row',\n",
    "                    align_items='stretch',\n",
    "                    width='90%')\n",
    "\n",
    "box_vlayout = ipw.Layout(display='flex', flex_flow='column', align_items='stretch',\n",
    "                    width='10%', height = 'auto', justify_content='space-between')\n",
    "\n",
    "s_az = ipw.IntSlider(min = 0 , max = 360, value = 45, step = 15, description = 'Azimuth'\n",
    "                     ,width = 'auto',layout = box_hlayout\n",
    "                     ,style = {'description_width': 'initial'} )\n",
    "s_c = ipw.FloatSlider(min = 0.1 , max = 4, value = 2.5, step =0.2\n",
    "                      ,description = 'Bandwidth Constant'\n",
    "                      ,width = 'auto',layout = box_hlayout\n",
    "                      ,style = {'description_width': 'initial'})\n",
    "s_ker = ipw.IntSlider(min = 1 , max = 11, value = 6\n",
    "                      ,description = 'Kernel',width = 'auto'\n",
    "                      ,layout = box_hlayout\n",
    "                      ,style = {'description_width': 'initial'})\n",
    "s_el = ipw.IntSlider(min = -90 , max = 90, value = 30, step = 15\n",
    "                     ,description = 'Elevation'\n",
    "                     ,orientation = 'vertical',length = 'auto'\n",
    "                     ,layout = box_vlayout\n",
    "                     ,style = {'description_length': 'initial'},readout = False)\n",
    "\n",
    "out = ipw.interactive_output(Nw_H_plot,{'c_h':s_c,'y': ipw.fixed(x2['y'])\n",
    "                                        ,'tht1': ipw.fixed(tht1) ,'reg': ipw.fixed(reg)\n",
    "                                        ,'p': ipw.fixed(p),'ker': s_ker,'a':ipw.fixed(a)\n",
    "                                        ,'b': ipw.fixed(b) ,'az':s_az ,'el': s_el})\n",
    "ipw.VBox([ipw.HBox([out,s_el])   \n",
    "          ,ipw.VBox([s_az,s_c,s_ker])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3 Secondary Equation Estimation </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The specification of equation (2d) is quite general where; intercepts $\\alpha_{0jd}$, and coefficients on exogenous regressors $\\alpha_{1jd}$ are unique to each cross-section. Furthermore, although instruments $W_{jt}$ are shared across cross-sections their coefficients $\\alpha_{2jd}$ are unique, and as yet there is no sense in which error terms $V_{jdt}$ are correlated across cross-section. As a result there are a number of restrictions on the regressors and parameters of equation (2d) which can be imposed and have a substantial effect on the manner in which $V_{jdt}$ will be estimated. \n",
    "\n",
    "** Note: **  For all vectors of dimension greater than two, I reference the individual elements of each vector by including a comma followed by a scalar value in the subscript. I reference the entire vector whenever the comma is omitted. For example,\n",
    "\n",
    "\\begin{align*} \n",
    "W_{jt} = \\begin{bmatrix} W_{jt,1} & W_{jt,2} & \\cdots & W_{jt,w_j} \\end{bmatrix}'\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3.1: Case 1 </h3> \n",
    "\n",
    "* Cross-sectional data is not panel, meaning that in this case there is no assumption restricting crossections to have common coefficients in the secondary equations.\n",
    "\n",
    "* $W_{jt}$ is an **known** subset of $W_{t}$.\n",
    "\n",
    "If so, estimation is comprised of q separate OLS regressions.\n",
    "\\begin{align*} \n",
    "(\\hat{\\alpha}_{0jd}, \\hat{\\alpha}_{1jd},\\hat{\\alpha}_{2jd}) \n",
    " = \\arg \\min \\sum_{t=1}^T\\left(Z_{1jdt} - \\alpha_{0} -  Z_{2jt}'\\alpha_{1} - W_{jt}'\\alpha_{2} \\right)^2\n",
    "\\end{align*}\n",
    "where $\\alpha_0 \\in \\mathbb{R}$, $\\alpha_{1} \\in \\mathbb{R}^{p_2}$, and $\\alpha_{2} \\in \\mathbb{R}^{w_j}$. So that \n",
    "$$\\hat{V}_{jdt} = Z_{1jdt} - \\hat{\\alpha}_{0jd} - Z_{2jt}'\\hat{\\alpha}_{1jd} - W_{jt}'\\hat{\\alpha}_{2jd}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 3.1.1: Case 1 Estimation </h4>\n",
    "\n",
    "Below is a function which implements simple ols regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols(df,inpt):\n",
    "    \"\"\"\n",
    "INPUTS\n",
    "df                (pandas df) Data Frame with all regressors\n",
    "inpt              (dict) Dictionary with the following\n",
    "  inpt['dep']     (string) Name of dependent variable contained in df\n",
    "  inpt['reg']     (list of strings) names of regressors in df \n",
    "  inpt['cons']    (0,1) Indicator for whether a constant should be included\n",
    "\n",
    "OUTPUTS \n",
    "out               (list of lists) List of the following\n",
    "  out[0]          (list) Estimated coefficients\n",
    "  out[1]          (list) Residuals\n",
    "  out[2]          (list) Estimated conditional expectation\n",
    "    \"\"\"\n",
    "    # Extracting input variables\n",
    "    dep = inpt['dep']\n",
    "    reg = inpt['reg']\n",
    "    cons = inpt['cons']\n",
    "    # Determining length of df (number of obs)\n",
    "    n = df.shape[0]\n",
    "    # Extracting Dependent Variable from df\n",
    "    Y = df.loc[:,dep].values.reshape(n,1)\n",
    "    # Extracting Regressors from df\n",
    "    if len(reg) == 1:\n",
    "        X = df.loc[:,reg].values.reshape(n,1)\n",
    "    elif len(reg) > 1: \n",
    "        X = df.loc[:,reg].values\n",
    "    # Adding column of ones if a constant is included\n",
    "    if cons == 1: \n",
    "        X = np.hstack((np.ones((n,1)),X))\n",
    "    # Estimated regression coefficients\n",
    "    alpha = np.linalg.inv(X.T.dot(X)).dot(X.T.dot(Y))\n",
    "    # Estimated Conditional Expectation\n",
    "    Yhat = X.dot(alpha)\n",
    "    # Residuals of the regression\n",
    "    res = Y - X.dot(alpha)\n",
    "    # Constructing output list of lists\n",
    "    out = [alpha.T.tolist()[0],res.T.tolist()[0],Yhat.T.tolist()[0]]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3.2 Case 2: </h3>\n",
    "\n",
    "* Cross-sectional data is not panel, meaning that in this case there is no assumption restricting crossections to have common coefficients in the secondary equations.\n",
    "\n",
    "* $W_{jt}$ is an ** unknown ** subset of $W_{t}$.\n",
    "\n",
    "If so, estimation is comprised of $q$ separate regressions, each of which will incorporate a subset selection routine. Let $\\alpha_{2jd} =  [\\;\\alpha_{2jd,1} \\;\\; \\alpha_{2jd,2} \\;\\; \\cdots \\;\\; \\alpha_{2jd,w} \\; ]$ where $\\alpha_{2jd,l} = 0$ whenever $W_{t,l} \\notin W_{jt}$. To facilitate subset selection I apply the lasso estimator by imposing an $\\ell^1$ penalty on estimated coefficients $\\hat{\\alpha}_{2jd}$ .\n",
    "\n",
    "\\begin{align*} \n",
    "(\\hat{\\alpha}_{0jd}, \\hat{\\alpha}_{1jd},\\hat{\\alpha}_{2jd})  = \\arg \\min \\sum_{t=1}^T\\left(Z_{1jdt} - \\alpha_{0} -  Z_{2jt}'\\alpha_{1} - W_{t}'\\alpha_{2} \\right)^2 \\;\\; \\text{ subject to } \\;\\; \\sum_{l = 1}^w |a_{3,l}| \\leq \\lambda\n",
    "\\end{align*}\n",
    "where $\\alpha_0 \\in \\mathbb{R}$, $\\alpha_{1} \\in \\mathbb{R}^{p_2}$, and $\\alpha_{3} \\in \\mathbb{R}^{w}$. So that again,\n",
    " $$\\hat{V}_{jdt} = Z_{1jdt} -\\hat{\\alpha}_{0jd} - Z_{2jt}'\\hat{\\alpha}_{1jd} - W_{t}'\\hat{\\alpha}_{2jd}$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 3.2.1: Case 2 Estimation </h4>\n",
    "\n",
    "Given that the data is not panel q seperate ols regressions will also be estimated but here I will implement the lasso algorithm (Tibshrani (1996) JRSSB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### lasso estimator goes here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3.3 Case 3: </h3>\n",
    "\n",
    "* $W_{jt}$ is an known subset of $W_{t}$,\n",
    "\n",
    "* Cross-sectional data is panel, meaning for all $j,j' \\in \\{1,2, \\ldots,q\\}$\n",
    "\n",
    "    1. $\\alpha_{1jd} =\\alpha_{1j'd} \\equiv \\alpha_{1d}$  \n",
    "\n",
    "    2.  $\\alpha_{2jd,l} = \\alpha_{2j'd,l} \\equiv \\alpha_{2d,l}$ whenever $W_{t,l} \\in W_{jt}$ and $W_{t,l} \\in W_{j't}$ \n",
    "\n",
    "\n",
    "Let\n",
    "\\begin{align*}\n",
    "1[W_{t,l} \\in W_{tj}] = \n",
    "\\begin{cases} \n",
    "1 & \\text{ if } W_{t,l} \\in W_{tj} \\\\ \n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "and $M_j = \\text{diag}(\\{1[ W_{t,l} \\in W_{jt}] \\}_{l=1}^w)$ so that,\n",
    "\n",
    "\\begin{align*}\n",
    "Z_{1jdt} &= \\alpha_{0jd} + Z_{2jt}' \\alpha_{1jd} + W_{jt}' \\alpha_{2jd} + V_{jdt} \\\\\n",
    "& =\\alpha_{0jd} + Z_{2jt}' \\alpha_{1d} + W_{t}'M_j \\alpha_{2d} + V_{jdt}\n",
    "\\end{align*}\n",
    "\n",
    "Now let \n",
    "* $\\Delta Z_{1jdt} = Z_{1jdt} - Z_{1jd(t-1)}$\n",
    "* $\\Delta Z_{2jdt} = Z_{2jdt} - Z_{2jd(t-1)}$ \n",
    "* $\\Delta W_{t} = W_{t} - W_{t-1}$, \n",
    "* $\\Delta V_{jdt} = V_{jdt} - V_{jd(t-1)}$ \n",
    "\n",
    "so that, \n",
    "\n",
    "\\begin{align*} \n",
    "\\Delta Z_{1jdt} =\\Delta Z_{2jt}' \\alpha_{1d} + \\Delta W_{t}'M_j \\alpha_{2d} + \\Delta V_{jdt}\n",
    "\\end{align*}\n",
    "As a result,\n",
    "\n",
    "\\begin{align*} \n",
    "(\\hat{\\alpha}_{1d},\\hat{\\alpha}_{2d})  = \\arg \\min \\sum_{j=1}^q\\sum_{t=1}^T\\left( \\Delta Z_{ijt} -  \\Delta Z_{2jt}'\\alpha_{1} - \\Delta W_{t}'M_j\\alpha_{2} \\right)^2 \n",
    "\\end{align*}\n",
    "\n",
    "So that given\n",
    "\\begin{align*} \n",
    "\\alpha_{0jd} =  E(V_{jdt} + \\alpha_{0jd}) =\n",
    "E( Z_{1jdt} - Z_{2jt}'\\alpha_{1d} - W_{t}'M_j\\alpha_{2d}) \n",
    "\\end{align*}\n",
    "\n",
    "we have\n",
    "\n",
    "$$\\hat{V}_{jdt} = Z_{1jdt} - Z_{2jt}'\\hat{\\alpha}_{1d} - W_{t}'M_j\\hat{\\alpha}_{2d} - T^{-1}\\sum_{t=1}^T  (Z_{1jdt} - Z_{2jt}'\\hat{\\alpha}_{1d} - W_{t}'M_j\\hat{\\alpha}_{2d}) $$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 3.3.1: Case 3 Estimation Function </h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def panel_dbl_est(dp_in,di_in,inpt_p):\n",
    "    \n",
    "    \"\"\"\n",
    "INPUTS\n",
    "dp_in                              (df) panel variables\n",
    "di_in                              (df) of instrments\n",
    "inpt_p                             (dict) composed of the following elements\n",
    "  inpt_p['dep']                    (str) name of dependent variable \n",
    "  inpt_p['reg']                    (str) names of exogenous regressors\n",
    "  inpt_p['in_nm']                  (lst) of lists of \n",
    "    inpt_p['in_nm'][i]               (lst) of names of inst relevant to ith crs\n",
    "  inpt_p['n_alphs']                (int) number of alphas to try in cv routine\n",
    "  inpt_p['n_parts']                (int) number of cv partition elements\n",
    "  inpt_p['cin']                    (str) cross section index name\n",
    "  inpt_p['tin']                    (str) time index name\n",
    "  inpt_p['ncs']                    (int) number of cross sections\n",
    "  inpt_p['ntp']                    (int) number of time periods\n",
    "  inpt_p['cv']                     (int) indicator for cross validated lasso parameter\n",
    "  inpt_p['lasso']                  (int) indicator for lasso estimation\n",
    "  inpt_p['alph']                   (flt) initial lasso tuning parameter \n",
    "  inpt_p['epsil']                  (flt) threshold for averaging \"non zero\" coefficients\n",
    "  inpt_p['inst_partition']         (lst) of the following\n",
    "    inpt_p['inst_partition'][0]      (list) of row numbers in the training set for instruments\n",
    "    inpt_p['inst_partition'][1]      (list) of row numbers in the testing set for instruments\n",
    "  inpt_p['panel_partition']        (lst) of the following\n",
    "    inpt_p['panel_partition'][0]     (list) of row numbers in the training set for panel data\n",
    "    inpt_p['panel_partition'][1]     (list) of row numbers in the testing set for panel data\n",
    "  \n",
    "OUTPUTS\n",
    "out                     (lst) of the following elemements\n",
    "  out[0]                (lst) of coefficients estimated with training set\n",
    "  out[1]                (lst) of the following\n",
    "    out[1][i]           (lst) vector indc the regs and insts relevant to ith cross section \n",
    "  out[2]                (lst) of the following\n",
    "    out[2][i]           (lst) of training set residuals for ith cross section\n",
    "  out[3]                (lst) of the following\n",
    "    out[3][i]           (lst) of testing set residuals for ith cross section\n",
    "  out[4]                (flt) final cross validated tuning parameter from training set\n",
    "    \"\"\"\n",
    "\n",
    "    # Extracting data from input dict\n",
    "    dep = inpt_p['dep']\n",
    "    reg = inpt_p['reg']\n",
    "    in_nm = inpt_p['in_nm']\n",
    "    cin = inpt_p['cin']\n",
    "    tin = inpt_p['tin']\n",
    "    ncs = inpt_p['ncs']\n",
    "    lasso = inpt_p['lasso']\n",
    "    n_parts = inpt_p['n_parts']\n",
    "    n_alphs = inpt_p['n_alphs']\n",
    "    inst_trn_part = inpt_p['inst_partition'][0]\n",
    "    inst_tst_part = inpt_p['inst_partition'][1]\n",
    "    panel_trn_part = inpt_p['panel_partition'][0]\n",
    "    panel_tst_part = inpt_p['panel_partition'][1]\n",
    "    trn_ntp = len(inst_trn_part)\n",
    "    tst_ntp = len(inst_tst_part)\n",
    "\n",
    "    # Input data\n",
    "    di  = di_in\n",
    "    dp  = dp_in\n",
    "    # Training Sets\n",
    "    dp_trn = dp.loc[panel_trn_part,:]\n",
    "    di_trn = di.loc[inst_trn_part,:]\n",
    "    # Testing Sets\n",
    "    dp_tst = dp.loc[panel_tst_part,:]\n",
    "    di_tst = di.loc[inst_tst_part,:]\n",
    "\n",
    "    # Initializing the set of all included instruments as the 1st set relevant inst\n",
    "    inst_incl = set(in_nm[0])\n",
    "    # Collecting rest of relevant instruments\n",
    "    for i in range(len(in_nm)):\n",
    "        # Union of inc and ith set of relevant instruments\n",
    "        inst_incl = inst_incl|set(in_nm[i])\n",
    "    # All included Instruments listed in order index order\n",
    "    inst_incl = [''.join(['W',str(i)]) for i in range(1,di.shape[1])\n",
    "                                       if ''.join(['W',str(i)]) in inst_incl]\n",
    "\n",
    "    # df with time index and all included instruments\n",
    "    di_incl = di.loc[:,[tin] + inst_incl]\n",
    "    di_trn_incl = di_trn.loc[:,[tin] + inst_incl]\n",
    "    di_tst_incl = di_tst.loc[:,[tin] + inst_incl]\n",
    "\n",
    "    # List of logical vectors (as list) of which instruments in inc are relevant to ith crs\n",
    "    inst_incl_vec = [[ 1 if inst_incl[i] in in_nm[j] else 0 \n",
    "                                         for i in range(len(inst_incl))]\n",
    "                                         for j in range(ncs)]\n",
    "    inst_all_vec = [[ 1 for i in range(di.shape[1]-1)] for j in range(ncs)]\n",
    "\n",
    "    # Merging and differencing input dictionary\n",
    "    inpt_m = {'tin' : tin , 'cin': cin , 'ncs': ncs, 'inc_vec': inst_incl_vec}\n",
    "    inpt_m_all = {'tin' : tin , 'cin': cin , 'ncs': ncs, 'inc_vec': inst_all_vec}\n",
    "    \n",
    "    # Merging and differencing function call\n",
    "    trn_mrg_dff = mrg_dff(dp_trn,di_trn_incl,inpt_m)\n",
    "    trn_mrg_dff_all = mrg_dff(dp_trn,di_trn,inpt_m_all)\n",
    "    tst_mrg_dff = mrg_dff(dp_tst,di_tst_incl,inpt_m)\n",
    "    tst_mrg_dff_all = mrg_dff(dp_tst,di_tst,inpt_m_all)\n",
    "\n",
    "    # Merged and differenced panel and instrument df\n",
    "    dpdi_trn_mrg_dff = trn_mrg_dff[0]\n",
    "    dpdi_tst_mrg_dff = tst_mrg_dff[0]\n",
    "    dpdi_trn_mrg_dff_all = tst_mrg_dff_all[0]\n",
    "    \n",
    "    # Merged panel and instrument df\n",
    "    dpdi_trn_mrg = trn_mrg_dff[1]\n",
    "    dpdi_trn_mrg_all = trn_mrg_dff_all[1]\n",
    "    dpdi_tst_mrg = tst_mrg_dff[1]\n",
    "    dpdi_tst_mrg_all = tst_mrg_dff_all[1]\n",
    "\n",
    "    # Panel regression variable names (appending a 'D')\n",
    "    exog_regr_dnames = [''.join(['D',reg[i]]) for i in range(len(reg))]\n",
    "    inst_incl_dnames = [''.join(['D',inst_incl[i]]) for i in range(len(inst_incl))]\n",
    "    dep_var_dname = ''.join(['D',dep])\n",
    "\n",
    "    # List of all relev + irrelev instrument names\n",
    "    all_inst_names = di_in.drop([tin],axis=1).columns.tolist()\n",
    "    # All LHS non panel variable names\n",
    "    all_reg_inst_names =  reg + all_inst_names\n",
    "\n",
    "    if lasso == 0:\n",
    "        # Initializing and fitting training data OLS regression\n",
    "        trn_ols_reg = linear_model.LinearRegression()\n",
    "        # Fitting an OLS regression to the training data\n",
    "        trn_ols_reg.fit(dpdi_trn_mrg_dff.loc[:,exog_regr_dnames + inst_incl_dnames],\n",
    "                    dpdi_trn_mrg_dff.loc[:,dep_var_dname])\n",
    "        # Extracting estimated regression coefficients\n",
    "        trn_ols_out = trn_ols_reg.coef_.tolist()\n",
    "        # Initializing est ceofficient vector with coeffs on exogenous (reg)  variables\n",
    "        trn_est_coeff = trn_ols_out[:len(reg)]\n",
    "        for i in range(len(all_inst_names)):\n",
    "            if inst_incl.count(all_inst_names[i]) > 0:\n",
    "                # If the ith inst in di is in inst_incl append the estimate to est_coeff\n",
    "                trn_est_coeff.append(trn_ols_out[inst_incl.index(all_inst_names[i])+len(reg)])\n",
    "            elif inst_incl.count(all_inst_names[i])==0:\n",
    "                # If the ith inst in di is not in inst_incl append a zero\n",
    "                trn_est_coeff.append(0)  \n",
    "        # List of lists of indicator of relevant ex and inst to each cross section      \n",
    "        trn_relev_regr_vec = [ [1]*len(reg) + [int(in_nm[i].count(all_inst_names[j]) > 0) \n",
    "                                  for j in range(len(all_inst_names))] \n",
    "                                  for i in range(len(in_nm))]\n",
    "        # Setting cv param to null\n",
    "        trn_cv_param = 'NA'\n",
    "\n",
    "    elif lasso == 1:\n",
    "        # Input dictionary for Lasso Estimation\n",
    "        inpt_ls_cv = {'dep': dep_var_dname,\n",
    "                  'odep': dep ,\n",
    "                  'ex_nm': exog_regr_dnames ,\n",
    "                  'oex_nm': reg,\n",
    "                  'ins_nm': inst_incl_dnames,\n",
    "                  'oins_nm': inst_incl, \n",
    "                  'alph': inpt_p['alph'],'epsil':inpt_p['epsil'],'cin': cin,\n",
    "                  'tin' : tin, 'n_alphs': inpt_p['n_alphs'], 'n_parts': inpt_p['n_parts'],\n",
    "                  'ntp': trn_ntp ,'ncs':ncs, 'cv': inpt_p['cv']\n",
    "                     }\n",
    "        # Estimation by panel lasso\n",
    "        trn_ls_cv_out = pan_lasso_cv(dpdi_trn_mrg,dpdi_trn_mrg_dff,inpt_ls_cv)\n",
    "        # Extracting estimated coefficients\n",
    "        trn_est_coeff = trn_ls_cv_out[0]\n",
    "        # Relevant regressor matrix (non zero est_coeff)\n",
    "        trn_relev_regr_vec = trn_ls_cv_out[1]\n",
    "        # Cross validated parameter value\n",
    "        trn_cv_param = trn_ls_cv_out[3]\n",
    "\n",
    "    # Constucting a panel df of estimated errors Vj,i\n",
    "    for i in range(1,ncs+1):\n",
    "        # np.array of dep variable values for ith crs\n",
    "        trn_dep_vals = dpdi_trn_mrg_all.loc[dpdi_trn_mrg[cin] == i,[dep]].values\n",
    "        tst_dep_vals = dpdi_tst_mrg_all.loc[dpdi_tst_mrg[cin] == i,[dep]].values\n",
    "        # Regressor values for ith crs\n",
    "        trn_regr_vals = dpdi_trn_mrg_all.loc[dpdi_trn_mrg[cin] == i,all_reg_inst_names].values\n",
    "        tst_regr_vals = dpdi_tst_mrg_all.loc[dpdi_tst_mrg[cin] == i,all_reg_inst_names].values\n",
    "        # Relevant Regressor values for ith crs\n",
    "        trn_relev_regr_vals = trn_regr_vals.dot(np.diag(trn_relev_regr_vec[i-1]))\n",
    "        tst_relev_regr_vals = tst_regr_vals.dot(np.diag(trn_relev_regr_vec[i-1]))\n",
    "        # Non Centered residuals\n",
    "        trn_non_center_resids = trn_dep_vals - trn_relev_regr_vals.dot(\n",
    "                                        np.array(trn_est_coeff).reshape(len(trn_est_coeff),1))\n",
    "        tst_non_center_resids = tst_dep_vals - tst_relev_regr_vals.dot(\n",
    "                                        np.array(trn_est_coeff).reshape(len(trn_est_coeff),1))\n",
    "        # Centered residuals\n",
    "        trn_center_resids = trn_non_center_resids - np.mean(trn_non_center_resids)\n",
    "        # NOTE here I am centering on the mean of the training residuals\n",
    "        tst_center_resids = tst_non_center_resids - np.mean(trn_non_center_resids)\n",
    "        if i == 1:\n",
    "            # if i = 1 initialize panel df\n",
    "            trn_center_resids_full = [list(trn_center_resids.T[0])]\n",
    "            tst_center_resids_full = [list(tst_center_resids.T[0])]\n",
    "        elif i > 1:\n",
    "            # if i > 1 add onto p_res\n",
    "            trn_center_resids_full.append(list(trn_center_resids.T[0]))\n",
    "            tst_center_resids_full.append(list(tst_center_resids.T[0]))\n",
    "\n",
    "    # Function output\n",
    "    out = [trn_est_coeff,trn_relev_regr_vec,trn_center_resids_full,tst_center_resids_full,trn_cv_param] \n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3.4: Case 4 </h3>\n",
    "\n",
    "* $W_{jt}$ is an **unknown** subset of $W_{t}$,\n",
    "\n",
    "* Cross-sectional data is panel, meaning for all $j,j' \\in \\{1,2, \\ldots,q\\}$\n",
    "\n",
    "    1. $\\alpha_{1jd} =\\alpha_{1j'd} \\equiv \\alpha_{1d}$  \n",
    "\n",
    "    2. $\\alpha_{2jd,l} = \\alpha_{2j'd,l} \\equiv \\alpha_{2d,l}$ whenever $W_{t,l} \\in W_{jt}$ and $W_{t,l} \\in W_{j't}$ \n",
    "\n",
    "Inhereting notation from case 3 we again have\n",
    "\n",
    "\\begin{align*} \n",
    "\\Delta Z_{1jdt} =\\Delta Z_{2jt}' \\alpha_{1d} + \\Delta W_{t}'M_j \\alpha_{2d} + \\Delta V_{jdt}\n",
    "\\end{align*}\n",
    "\n",
    "In order to introduce our selection procedure we will estimate the coefficients on $W_{t}$ as if that are not identical, then average the non zero estimates to construct a single estimate.  Consider, \n",
    "\n",
    "\\begin{align*} \n",
    "(\\hat{\\alpha}_{1d},\\hat{\\alpha}_{2d})  = \\arg \\min \\sum_{j=1}^q\\sum_{t=2}^T\\left( \\Delta Z_{1jdt} -  \\Delta Z_{2jt}'\\alpha_{1} - \\Delta W_{t}'\\alpha_{2j} \\right)^2 \\;\\; \\text{ subject to } \\;\\; \\sum_{l=1}^w|\\alpha_{2j,l}| \\leq \\lambda \\;\\;  \\text{ for all } 1 \\leq j \\leq q\n",
    "\\end{align*}\n",
    "\n",
    "Consequently define for some $\\varepsilon > 0$ \n",
    "\n",
    "\\begin{align*} \n",
    "\\tilde{\\alpha}_{2d,l} = \\frac{\\sum_{l=1}^q \\hat{\\alpha}_{2jd,l} 1[ \\hat{\\alpha}_{2jd,l} > \\varepsilon ] }{ \\sum_{l=1}^q 1[ \\hat{\\alpha}_{2jd,l} > \\varepsilon] }\n",
    "\\end{align*}\n",
    "\n",
    "Now let $\\tilde{\\alpha}_{2d} = [ \\; \\tilde{\\alpha}_{2d,1} \\;\\; \\tilde{\\alpha}_{2d,2} \\;\\; \\cdots \\;\\; \\tilde{\\alpha}_{2d,w}  \\; ]'$ and $\\tilde{M}_{jd} = \\text{diag}( \\{ 1[\\hat{\\alpha}_{2jd,l} > \\varepsilon ] \\}_{l=1}^w)$ so that, \n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{V}_{jdt} = Z_{1jdt} - Z_{2jt}'\\hat{\\alpha}_{1d} - W_{t}'\\tilde{M}_{jd}\\tilde{\\alpha}_{2d} - T^{-1}\\sum_{t=1}^T  (Z_{1jdt} - Z_{2jt}'\\hat{\\alpha}_{1d} - W_{t}'\\tilde{M}_{jd}\\tilde{\\alpha}_{2d}) \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 3.4.1: Case 4 Cross Validated Lasso Wrapper Function </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pan_lasso_cv(data_pan_mr,data_pan_mrdf,inpt_ls):\n",
    "    \n",
    "    \"\"\"\n",
    "INPUTS\n",
    "  data_pan_mr            (df) panel where crs regs are merged with insts\n",
    "  data_pan_mrdf          (df) panel where crs regs are merged with insts and 1st differenced\n",
    "  inpt_ls                (dict) of the following elements\n",
    "    inpt_ls['cv']        (int) indicator for whether cross validation is done\n",
    "    inpt_ls['dep']       (str) names of differenced dependent variable in lasso reg\n",
    "    inpt_ls['odep']      (str) names of non differenced dependent variable\n",
    "    inpt_ls['ex_nm']     (lst) names of differenced exogenous variables\n",
    "    inpt_ls['oex_nm']    (lst) names of non differenced exogenous variables\n",
    "    inpt_ls['ins_nm']    (lst) names of differenced instruments\n",
    "    inpt_ls['oins_nm']   (lst) names of non difference instruments\n",
    "    inpt_ls['alph']      (flt) initial alpha tuning parameter\n",
    "    inpt_ls['epsil']     (flt) threshold for averaging \"non zero\" coefficients\n",
    "    inpt_ls['cin']       (str) name of crs section variable\n",
    "    inpt_ls['tin']       (str) name of time variable\n",
    "    inpt_ls['n_alphs']   (int) number of different alphas tried\n",
    "    inpt_ls['n_parts']   (int) number of partition elements \n",
    "    inpt_ls['ntp']       (int) number of time periods\n",
    "    inpt_ls['ncs']       (int) number of cross sections\n",
    "\n",
    "OUPUTS\n",
    "    out             (lst) with the following elements\n",
    "      out[0]        (lst) Estimated coefficients\n",
    "      out[1]        (lst) containing the following elements\n",
    "        out[1][j]   (lst) binary vector indicating the non zero ceofficients for each crs\n",
    "      out[2]        (lst) list of mean meansquared errors \n",
    "      out[3]        (lst) cross validated alpha tuning parameter\n",
    "    \"\"\"\n",
    "                    \n",
    "    #Extracting info from dictionary\n",
    "    n_alphs = inpt_ls['n_alphs']\n",
    "    oins_nm = inpt_ls['oins_nm']\n",
    "    oex_nm = inpt_ls['oex_nm']\n",
    "    odep = inpt_ls['odep']\n",
    "    n_parts = inpt_ls['n_parts']\n",
    "    n_alphs = inpt_ls['n_alphs'] \n",
    "    cin = inpt_ls['cin']\n",
    "    tin = inpt_ls['tin']\n",
    "    ncs = inpt_ls['ncs']\n",
    "    ntp = inpt_ls['ntp'] \n",
    "    cv = inpt_ls['cv']\n",
    "\n",
    "    if cv == 0:\n",
    "        # Estimating lasso regression with orginal input alpha\n",
    "        ls_soln = pan_lasso(data_pan_mrdf,inpt_ls)\n",
    "        out = ls_soln + ['none'] + ['none']\n",
    "\n",
    "    elif cv == 1:  \n",
    "        # Generating partitions\n",
    "        indx = k_subs(n_parts,ntp-1,ncs)\n",
    "\n",
    "        # Initializing carrier lists\n",
    "        all_msr = []\n",
    "        trl_alph = []\n",
    "\n",
    "        # For each value of lasso tuning parameter\n",
    "        for k in range(5,n_alphs+6):\n",
    "            # Initializing mean squared error list for each partition\n",
    "            msr = []\n",
    "            # Setting the value of the lasso tuning parameter\n",
    "            inpt_ls['alph'] = k/n_alphs\n",
    "            # Appending the current lasso tuning parameter\n",
    "            trl_alph.append(k/n_alphs)\n",
    "            # For each of the training testing set combinations\n",
    "            for part in range(n_parts):\n",
    "                # Merged and differenced training set\n",
    "                trn_mrdf = data_pan_mrdf.iloc[indx[1][part][0],:].copy()\n",
    "                # Merged and differenced testing set\n",
    "                tst_mrdf = data_pan_mrdf.iloc[indx[1][part][1],:].copy()\n",
    "                # Merged only testing set\n",
    "                tst_mr = data_pan_mr.iloc[indx[1][part][1],:].copy()\n",
    "                # lasso estimation on training data set\n",
    "                lcf = pan_lasso(trn_mrdf,inpt_ls)\n",
    "                # Initializng list of allresiduals for each cross section \n",
    "                all_res = []\n",
    "                # for each cross section\n",
    "                for i in range(ncs):\n",
    "                    # Extracting the un difference test regressor matrix for crs i\n",
    "                    c1 = tst_mr.loc[tst_mr[cin]==i+1, oex_nm + oins_nm ].values\n",
    "                    # Creating the diagonal selection matrix for crs i\n",
    "                    c2 = np.diag(lcf[1][i])\n",
    "                    # Reshaping Estimated lasso coefficient matrix\n",
    "                    c3 = np.array(lcf[0]).reshape(len(oex_nm + oins_nm),1)\n",
    "                    # Computing partial estimated values (no constant term included)\n",
    "                    c4 = c1.dot(c2).dot(c3)\n",
    "                    # Computing non centered residuals (no constant term included)\n",
    "                    res1 = tst_mr.loc[tst_mr[cin]==i+1,odep].values.reshape(len(c4),1) - c4\n",
    "                    # Computing centered squared residuals\n",
    "                    cres = (res1 - np.mean(res1))**2\n",
    "                    # Concatenating sqr residuals of ith cross section to all_res\n",
    "                    all_res = all_res + cres.tolist()\n",
    "                # Appending the mean sqrd residuals for the npart th trn and test set \n",
    "                msr.append(np.mean(all_res))\n",
    "            # Appending the mean of mean sqrd residuals for all partitions   \n",
    "            all_msr.append(np.mean(msr))\n",
    "\n",
    "        # Index of smallest mean msr values\n",
    "        ax = all_msr.index(min(all_msr))\n",
    "        # alpha value that produces smallest mean_msr\n",
    "        cv_alph = trl_alph[ax]\n",
    "\n",
    "        # Running Lasso with cv alpha\n",
    "        inpt_ls['alpha'] = cv_alph\n",
    "        # Estimating lasso regression with cros validated alpha\n",
    "        ls_soln = pan_lasso(data_pan_mrdf,inpt_ls)\n",
    "        out = ls_soln\n",
    "        out.append(all_msr)\n",
    "        out.append(cv_alph)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 3.4.2: Case 4 Lasso Estimator </h4>\n",
    "\n",
    "The function below is not called directly in psc_est() below, it is called by panel_fe() after panel_fe() was is called by psc_est() whenever inpt['lasso']=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pan_lasso(l_dp,l_inpt):\n",
    "    \"\"\"\n",
    "INPUTS\n",
    "l_dp                 (df) all panel regression variables \n",
    "l_inpt               (dict) Containing the following elements\n",
    "  l_inpt['dep']        (str) name of dependent variable\n",
    "  l_inpt['ex_nm']      (str) name of all exogenous variables\n",
    "  l_inpt['ins_nm']     (str) name of all instruments\n",
    "  l_inpt['alph']       (flt) lasso penalty parameter\n",
    "  l_inpt['ncs']        (int) number of cross sections\n",
    "  l_inpt['epsil']      (flt) threshold for averaging \"non zero\" coefficients\n",
    "  \n",
    "OUTPUTS\n",
    "out            (lst) with the following elements\n",
    " out[0]        (lst) Estimated coefficients\n",
    " out[1]        (lst) containing the following elements\n",
    "   out[1][j]   (lst) binary vector indicating the non zero ceofficients for each crs\n",
    "    \"\"\"\n",
    "    \n",
    "# Extracting variables from input dictionary\n",
    "    l_dep_nm = l_inpt['dep']\n",
    "    l_ex_nm = l_inpt['ex_nm']\n",
    "    l_ins_nm = l_inpt['ins_nm']\n",
    "    l_alph = l_inpt['alph']\n",
    "    l_epsil = l_inpt['epsil']\n",
    "    n_exo = len(l_ex_nm)\n",
    "    ncs = l_inpt['ncs']\n",
    "    t_inst = len(l_ins_nm)\n",
    "    # Initializing the coefficient list\n",
    "    excf1 = []\n",
    "\n",
    "    # Estimating the coefficients on the exogenous regressors\n",
    "    for k in range(inpt['ncs']):\n",
    "        # Extracting the kth cross sections data \n",
    "        dsl = l_dp.loc[l_dp['crs']==k+1,:]\n",
    "        #Initializing the regression model\n",
    "        lin_reg = linear_model.LinearRegression()\n",
    "        #Fitting the regression model\n",
    "        lin_reg.fit(dsl.loc[:,l_ex_nm+l_ins_nm].values,\n",
    "                    dsl.loc[:,l_dep_nm].values.reshape(dsl.shape[0],1))\n",
    "        # Appending the est coeff for exogenous regressors to coeff list\n",
    "        excf1.append([lin_reg.coef_[0][i] for i in range(n_exo)])\n",
    "\n",
    "    # Averaging coefficient values over cross sections  \n",
    "    excf = [np.mean([excf1[i][j] for i in range(len(excf1))]) for j in range(n_exo)]\n",
    "    # Generating the name of the modified dependent variable\n",
    "    adep_nm = ''.join(['a',l_dep_nm])\n",
    "    # Generating the modified dependent variable\n",
    "    l_dp[adep_nm] = (l_dp.loc[:,l_dep_nm].values.reshape(l_dp.shape[0],1)\n",
    "                     -l_dp.loc[:,l_ex_nm].values.dot(np.array(excf).reshape(n_exo,1)))\n",
    "\n",
    "    # Initialzing the list of estimated lasso coefficients                 \n",
    "    ain_cf1 = []\n",
    "    for k in range(ncs): \n",
    "        # Extracting modified dep and indep regressor for crs k \n",
    "        ds2 = l_dp.loc[l_dp['crs']==k+1,:]\n",
    "        # Initializing lasso regression model\n",
    "        lasso_reg = linear_model.Lasso(alpha = l_inpt['alph'])\n",
    "        # Fitting the regression model\n",
    "        lasso_reg.fit(ds2.loc[:,l_ins_nm].values,\n",
    "                      ds2.loc[:,adep_nm].values.reshape(ds2.shape[0],1))\n",
    "        # Appending lasso coefficient to list\n",
    "        ain_cf1.append(list(lasso_reg.coef_))\n",
    "\n",
    "    # Initializing set of estiamated coeff with those one the exogenous variables\n",
    "    ain_cf = excf\n",
    "    # generating the final averged values of each coefficient\n",
    "    for j in range(len(ain_cf1[0])): \n",
    "        # Collecting all coeff estimated on jth inst greater than threshold l_epsil\n",
    "        a = [ain_cf1[i][j] for i in range(len(ain_cf1)) if np.abs(ain_cf1[i][j]) > l_epsil]\n",
    "        if not a:\n",
    "            # If a is an empty list inst not selected in any crs so append a zero\n",
    "            ain_cf.append(0)\n",
    "        else:\n",
    "            # Averging if a is non empty\n",
    "            ain_cf.append(np.mean(a))    \n",
    "\n",
    "    # Generating a list of lists where ain_cf_rm[j][i]=1 if the |coeff| on the ith inst\n",
    "    # for the jth cross section is greater than l_epsil\n",
    "    ain_cf_rm = [[1,1] + [int(np.abs(ain_cf1[j][i])>l_epsil) \n",
    "                            for i in range(len(ain_cf1[0]))] \n",
    "                            for j in range(len(ain_cf1))]\n",
    "    # Function output\n",
    "    out = [ain_cf, ain_cf_rm]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 3.4.3: Case 4 Cross Validation Partition Generator Function </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_subs(k,ntp,ncs):\n",
    "    \"\"\"\n",
    "INPUTS\n",
    " k       (int) number of partition memebers\n",
    " ntp     (int) number of time periods\n",
    "\n",
    "OUTPUTS\n",
    " out                    (list) containing the following elements\n",
    "   out[0]               (list of tuples) index numbers for instrument partitions\n",
    "     out[0][i]          (tuples of list) ith instrument partition\n",
    "         out[0][i][0]   (list) of row numbers in the ith part. training set for instruments\n",
    "         out[0][i][1]   (list) of row numbers in the ith part. test set for instruments\n",
    "   out[1]               (list of tuples) index numbers for panel data partitions\n",
    "     out[1][i]          (tuples of list) ith panel data partition\n",
    "         out[1][i][0]   (list) of row numbers in the ith part. training set for panel data\n",
    "         out[1][i][1]   (list) of row numbers in the ith part. test set for panel data\n",
    "    \"\"\"\n",
    "    # List of row index numbers for each cross section\n",
    "    a0 = [i for i in range(ntp)]\n",
    "    # Length of the first k-1 partition list\n",
    "    n_sl = int(np.floor(ntp)/k)\n",
    "    # Generating the k-1 partitions as a lists of disjoint exhaustive lists \n",
    "    k_grp = [[a0.pop(np.random.randint(1,ntp-i-j*n_sl)) for i in range(n_sl)] \n",
    "                                                                for j in range(k-1)]\n",
    "    # Adding in the last partition\n",
    "    k_grp.append(a0)\n",
    "    # Inintializing list of train / test tuples \n",
    "    in_indx = []\n",
    "    pan_indx = []\n",
    "    for i in range(k):\n",
    "        # Initializing the ith train list\n",
    "        a2 = []\n",
    "        # Initializing the list of partitions whose union is a training set\n",
    "        a3 = list(range(k))\n",
    "        # Removing the test set index\n",
    "        del a3[i]\n",
    "        for j in a3:\n",
    "            # Taking the union of all training set partition lists\n",
    "            a2 = sorted(a2 + k_grp[j])\n",
    "        # Creating the ith (training,test) tuple\n",
    "        a4 = (a2,sorted(k_grp[i]))\n",
    "        # Appending to full list\n",
    "        in_indx.append(a4)\n",
    "        a5 = a2\n",
    "        a6 = sorted(k_grp[i])\n",
    "        for j in range(1,ncs):\n",
    "            a5 = a5 + (np.array(a2)+j*ntp).tolist()\n",
    "            a6 = a6 + (np.array(sorted(k_grp[i]))+j*ntp).tolist()\n",
    "        a7 = (a5,a6)\n",
    "        pan_indx.append(a7)\n",
    "        \n",
    "    return([in_indx,pan_indx])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 3.4.4: Case 4 Panel Merging and First Differencing Function </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrg_dff(dp_in,di_in,inpt):\n",
    "    \"\"\"\n",
    "INPUTS \n",
    "  dp_in                  (df) panel dataframe \n",
    "  di_in                  (df) instruments dataframe\n",
    "  inpt                   (dict) Containing the following elements\n",
    "    inpt['tin']          (str) time index column name\n",
    "    inpt['cin']          (str) cross section index column name\n",
    "    inpt['ncs']          (int) number of cross sections\n",
    "    inpt['inc_vec']       (lst) of list of binary vectors \n",
    "      inpt['inc_vec'][i]  (lst) of binary vectors listing which instruments in inc are relevant to ith crs\n",
    "    \n",
    "OUTPUTS\n",
    "  out[0]             (df) dp_in and di_in merged on tin for each cin and 1st differenced\n",
    "  out[1]             (df) dp_in and di_in merged on tin for each cin \n",
    "    \"\"\"\n",
    "    # Extracting variables from input dictionary\n",
    "    tin = inpt['tin']\n",
    "    cin = inpt['cin']\n",
    "    ncs = inpt['ncs']\n",
    "    inst_incl_vec = inpt['inc_vec']\n",
    "    \n",
    "    # Looping over each cross section\n",
    "    for i in range(ncs):\n",
    "        # Merging panel and instrument df on tin for ith cross section\n",
    "        di_no_tin = di_in.drop(inpt['tin'],axis=1).copy()\n",
    "        di_no_tin_relev = di_no_tin.values.dot(np.diag(inst_incl_vec[i]))\n",
    "        tin_as_ndarray = di_in.loc[:,inpt['tin']].values.reshape(di_in.shape[0],1)\n",
    "        di_relev = pd.DataFrame(np.hstack((tin_as_ndarray,di_no_tin_relev)),columns = di_in.columns)\n",
    "        dpdi_relev = pd.merge(dp_in.loc[dp_in[cin] == i+1,:],di_relev,how = 'inner', on = tin)\n",
    "        # Initializing the temp difference matrix \n",
    "        dpdi_relev_dff = dpdi_relev.loc[1:,[tin]+[cin]] \n",
    "        # Looping over all the column names in b1\n",
    "        for nm in dpdi_relev.columns[2:].tolist():\n",
    "            # 1st Differencing nm column of b1\n",
    "            dpdi_relev_dff[''.join(['D',nm])] = (dpdi_relev.loc[:,nm].values \n",
    "                                                 - dpdi_relev.loc[:,nm].shift(1).values)[1:]\n",
    "        if i == 0:\n",
    "            # Initializing final matrix out\n",
    "            dpdi_relev_all = dpdi_relev\n",
    "            dpdi_relev_dff_all = dpdi_relev_dff\n",
    "        elif i > 0:\n",
    "            # Concatenating b2 to out\n",
    "            dpdi_relev_all = pd.concat([dpdi_relev_all,dpdi_relev],axis =0)\n",
    "            dpdi_relev_dff_all = pd.concat([dpdi_relev_dff_all,dpdi_relev_dff],axis =0)\n",
    "            out = [dpdi_relev_dff_all, dpdi_relev_all]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4 Double PSC Estimator </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 4.1 Double PSC Estimator Wrapper Function </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def psc_dbl_est(data_pan,data_inst,data_err,inpt):\n",
    "    \n",
    "    \"\"\"\n",
    "INPUTS\n",
    "data_pan                       (df) panel of all primary regressors each crss sect, and time period\n",
    "data_inst                      (df) all possible instruments \n",
    "data_err                       (df) panel df of all error terms, only used if inpt['orcl'] == 1\n",
    "inpt                           (dct) The following input var. \n",
    "  inpt['cin']                    (str) names of cross section var. in data_pan & data_err\n",
    "  inpt['tin']                    (str) names of cross section var. in data_pan & data_err\n",
    "  inpt['ntp']                    (int) number time periods\n",
    "  inpt['ncs']                    (int) number of crossections\n",
    "  inpt['n_end']                  (int) number of endogenous variables \n",
    "  inpt['n_exo']                  (int) number of exogenous variables\n",
    "  inpt['t_inst']\n",
    "  inpt['dep_nm']                 (str) name of dependent varaible in primary regression in data_pan \n",
    "  inpt['en_nm']                  (lst) names of endogenous variables in data_pan\n",
    "  inpt['ex_nm']                  (lst) names of exogenous variables in data_pan\n",
    "  inpt['in_nm']                  (lst) names of instruments relevant to each cross section\n",
    "    inpt['in_nm'][i]               (lst of str) names of var. in data_inst used by cin == i\n",
    "  inpt['n_alphs']                (int) number of alphas to try in cv routine\n",
    "  inpt['n_parts']                (int) number of cv partition elements\n",
    "  inpt['sec_pan']                (int) Indicator for if secondary equation data is panel\n",
    "  inpt['lasso']                  (int) Indicator for if W_{i} is a known subset of W for all cin \n",
    "  inpt['epsil']                  (flt) lasso param threshold for averaging \"non zero\" coefficients\n",
    "  inpt['alph']                   (flt) lasso param penalty value for lasso estimation\n",
    "  inpt['cv']                     (int) indicator for cross validated lasso parameter\n",
    "  inpt['orcl']                   (int) Indicator for if V_i,j is observed for each cin, and endog var. \n",
    "  inpt['k_mvd']                  (int) kernel used for multi var density est. see (?mvkernel)\n",
    "  inpt['k_uvd']                  (int) kernel used for bi var density est. see (?mvkernel)\n",
    "  inpt['k_H']                    (int) kernel used for NW H function  est. see (?mvkernel)\n",
    "  inpt['c_mvd']                  (flt) plug in bandwidth const. for  multi var density est.\n",
    "  inpt['c_uvd']                  (flt) plug in bandwidth const. for bi var density est.\n",
    "  inpt['c_H']                    (flt) plug in bandwidth const. for NW H function  est.\n",
    "  inpt['inst_partition']         (lst) of the following\n",
    "    inpt['inst_partition'][0]      (list) of row numbers in the training set for instruments\n",
    "    inpt['inst_partition'][1]      (list) of row numbers in the testing set for instruments\n",
    "  inpt['panel_partition']        (lst) of the following\n",
    "    inpt['panel_partition'][0]     (list) of row numbers in the training set for panel data\n",
    "    inpt['panel_partition'][1]     (list) of row numbers in the testing set for panel data\n",
    "\n",
    "OUTPUT\n",
    "out                           (lst) of the following elements \n",
    "  out[0]                        (lst) estimated primary function coefficients \n",
    "  out[1]                        (lst) of the following\n",
    "    out[1][i-1]                   (lst) estimated secondary coefficients for ith endog variable     \n",
    "  out[2]                        (lst) of the following\n",
    "    out[2][i-1]                   (lst) relevant secondary coefficient vector for ith endog variable \n",
    "    \"\"\"\n",
    "    \n",
    "    # Extracting variables from input dictionary\n",
    "    cin = inpt['cin']\n",
    "    tin = inpt['tin']\n",
    "    ntp = inpt['ntp']\n",
    "    ncs = inpt['ncs']\n",
    "    n_en = inpt['n_end']\n",
    "    n_ex = inpt['n_exo']\n",
    "    t_inst = inpt['t_inst']\n",
    "    dep_nm = inpt['dep_nm'] \n",
    "    en_nm = inpt['en_nm']\n",
    "    ex_nm = inpt['ex_nm']\n",
    "    in_nm = inpt['in_nm']\n",
    "    sec_pan = inpt['sec_pan']\n",
    "    lasso = inpt['lasso']\n",
    "    epsil = inpt['epsil']\n",
    "    alph  = inpt['alph']\n",
    "    orcl = inpt['orcl']\n",
    "    k_mvd = inpt['k_mvd']\n",
    "    k_uvd = inpt['k_uvd']\n",
    "    k_H = inpt['k_H']\n",
    "    c_mvd = inpt['c_mvd']\n",
    "    c_uvd = inpt['c_uvd']\n",
    "    c_H = inpt['c_H']\n",
    "    inst_trn_part = inpt['inst_partition'][0]\n",
    "    inst_tst_part = inpt['inst_partition'][1]\n",
    "    panel_trn_part = inpt['panel_partition'][0]\n",
    "    panel_tst_part = inpt['panel_partition'][1]\n",
    "\n",
    "    dp = data_pan\n",
    "    di = data_inst\n",
    "    de = data_err\n",
    "\n",
    "    dp_trn = dp.loc[panel_trn_part,:]\n",
    "    di_trn = di.loc[inst_trn_part,:]\n",
    "    dp_tst = dp.loc[panel_tst_part,:]\n",
    "    di_tst = di.loc[inst_tst_part,:]\n",
    "\n",
    "    # Number of time periods in training set\n",
    "    trn_ntp = len(inst_trn_part)\n",
    "    # Number of time periods in test set\n",
    "    tst_ntp = len(inst_tst_part)\n",
    "\n",
    "    ############ Estimation of secondary equations\n",
    "\n",
    "    if orcl == 1:\n",
    "        # If residuals are observed and not estimated\n",
    "        for i in range(1,n_en+1):\n",
    "            # Adding true residuals to panel data\n",
    "            dp[''.join(['h_V',str(i)])] = de.loc[:,''.join(['V',str(i)])]\n",
    "        # List of names of residuals\n",
    "        re_nm = [''.join(['h_V',str(i)]) for i in range(1,n_en+1)]\n",
    "    else:\n",
    "        # If residuals are not observed and must be estimated\n",
    "        if sec_pan == 0:\n",
    "            # IN THIS VERSION I HAVE REMOVED THIS AS IT IS BESIDE THE POINT\n",
    "            pass\n",
    "\n",
    "        ## If Secondary equations are panel\n",
    "        elif sec_pan == 1:\n",
    "            # for each endogenous variable\n",
    "            for j in range(0,n_en):\n",
    "                # Constructing input dictionary for estimator\n",
    "                inpt_p = {'dep':en_nm[j],'reg':ex_nm,\n",
    "                          'in_nm': in_nm,\n",
    "                          'n_alphs':inpt['n_alphs'],\n",
    "                          'n_parts':inpt['n_parts'],\n",
    "                          'cin': cin,'tin': tin,\n",
    "                          'ncs':ncs, 'ntp':trn_ntp,\n",
    "                          'cv':inpt['cv'],'lasso':lasso,\n",
    "                          'alph':alph,'epsil':epsil,\n",
    "                          'inst_partition': [inst_trn_part, inst_tst_part] ,\n",
    "                          'panel_partition': [panel_trn_part, panel_tst_part]\n",
    "                          }\n",
    "                # Estimation note \n",
    "                pan = panel_dbl_est(dp,di,inpt_p)\n",
    "                if j == 0:\n",
    "                    trn_cf = [pan[0]]\n",
    "                    trn_inc = pan[1]\n",
    "                    trn_resid = pan[2]\n",
    "                    tst_resid = pan[3]\n",
    "                elif j>0:\n",
    "                    trn_cf.append(pan[0])\n",
    "                    trn_inc.append(pan[1])\n",
    "                    trn_resid.append(pan[2])\n",
    "                    tst_resid.append(pan[3])\n",
    "                for i in range(ncs):\n",
    "                    # Initializing panel template df\n",
    "                    dp_trn_res_ji = dp_trn.loc[dp_trn[cin] == i+1,[cin]+[tin]]\n",
    "                    dp_tst_res_ji = dp_tst.loc[dp_tst[cin] == i+1,[cin]+[tin]]\n",
    "                    # Adding ith estimated error term to a1 \n",
    "                    dp_trn_res_ji[''.join(['h_V',str(j+1)])] = pan[2][i] \n",
    "                    dp_tst_res_ji[''.join(['h_V',str(j+1)])] = pan[3][i] \n",
    "                    if i == 0: \n",
    "                        # if i = 0 initializing panel version\n",
    "                        dp_trn_res_j = dp_trn_res_ji\n",
    "                        dp_tst_res_j = dp_tst_res_ji\n",
    "                    elif i > 0: \n",
    "                        # if i > 1 adding to panel version\n",
    "                        dp_trn_res_j = pd.concat([dp_trn_res_j,dp_trn_res_ji], axis=0)\n",
    "                        dp_tst_res_j = pd.concat([dp_tst_res_j,dp_tst_res_ji], axis=0)\n",
    "                # merging panel of estimated error terms into dp \n",
    "                dp_trn = pd.merge(dp_trn,dp_trn_res_j,on = [cin,tin],how = 'inner')\n",
    "                dp_tst = pd.merge(dp_tst,dp_tst_res_j,on = [cin,tin],how = 'inner')\n",
    "            # Names of all residuals   \n",
    "            res_nm = [''.join(['h_V',str(i)]) for i in range(1,n_en+1)]\n",
    "\n",
    "    ############ Estimation of primary equation\n",
    "\n",
    "    ## Creating a new dataframe with back shifted columns of each residual\n",
    "    # For each residual\n",
    "    for i in range(n_en):\n",
    "        # For each cross section\n",
    "        for j in range(1,ncs+1):\n",
    "            # Initializing the dataframe ['crs' , 't' , 'h_Vi]\n",
    "            dp_trn_res_b_ij = dp_trn.loc[dp_trn[cin] == j,[cin,tin,res_nm[i]]]\n",
    "            dp_tst_res_b_ij = dp_tst.loc[dp_tst[cin] == j,[cin,tin,res_nm[i]]]\n",
    "            # Adding a column of backshifted residuals note the first element is NA\n",
    "            dp_trn_res_b_ij[''.join(['b',res_nm[i]])] = dp_trn.loc[dp_trn[cin] == j,res_nm[i]].shift(1)\n",
    "            dp_tst_res_b_ij[''.join(['b',res_nm[i]])] = dp_tst.loc[dp_tst[cin] == j,res_nm[i]].shift(1)\n",
    "            # Using only the columns where 't' > 1 eliminating the NA\n",
    "            dp_trn_res_b_ij = dp_trn_res_b_ij.iloc[1:,:]\n",
    "            dp_tst_res_b_ij = dp_tst_res_b_ij.iloc[1:,:]\n",
    "            if j == 1:\n",
    "                # If this first crosssection intialize intermediate df\n",
    "                dp_trn_res_b_i = dp_trn_res_b_ij\n",
    "                dp_tst_res_b_i = dp_tst_res_b_ij\n",
    "            else:\n",
    "                # If not first cross section concatentate current to a2\n",
    "                dp_trn_res_b_i = pd.concat([dp_trn_res_b_i,dp_trn_res_b_ij],axis = 0)\n",
    "                dp_tst_res_b_i = pd.concat([dp_tst_res_b_i,dp_tst_res_b_ij],axis = 0)\n",
    "        if i == 0:\n",
    "            # If first residual initialize den data frame\n",
    "            dp_trn_b_res = dp_trn_res_b_i\n",
    "            dp_tst_b_res = dp_tst_res_b_i\n",
    "        else:\n",
    "            # If not first residul merge result into den df\n",
    "            dp_trn_b_res = pd.merge(dp_trn_b_res,dp_trn_res_b_i,on=[cin,tin], how = 'inner')\n",
    "            dp_tst_b_res = pd.merge(dp_tst_b_res,dp_tst_res_b_i,on=[cin,tin], how = 'inner')\n",
    "\n",
    "    # List of backshifted residual names\n",
    "    res_b_nm = [''.join(['b',res_nm[i]]) for i in range(n_en)]\n",
    "\n",
    "    ## Calculating the joint density of all residuals and their backshifted values.  \n",
    "    # For each cross section\n",
    "    for j in range(1,ncs+1):\n",
    "        # Extracting indexes residuals and the back shifted versions for density est.\n",
    "        dp_trn_b_res_j = dp_trn_b_res.loc[dp_trn_b_res[cin]==j]\n",
    "        dp_tst_b_res_j = dp_tst_b_res.loc[dp_tst_b_res[cin]==j]\n",
    "        # Standard deviation of training data column(only use training data)\n",
    "        sd_dp_trn_b_res_j = np.std(dp_trn_b_res_j.drop([cin]+[tin],axis =1).values,axis = 0)\n",
    "        # Plug in bandwiths for density estimation (only use training data)\n",
    "        h_dp_trn_b_res_j = c_mvd*(trn_ntp-1)**(-1/(4+n_en))*sd_dp_trn_b_res_j\n",
    "        # Initializing b2 temp df\n",
    "        dp_trn_den_j = dp_trn_b_res_j.loc[:,[cin]+[tin]].copy()\n",
    "        dp_tst_den_j = dp_tst_b_res_j.loc[:,[cin]+[tin]].copy()\n",
    "        # Adding a column of the mv density of all resids and their back shifts to b2 for crs j\n",
    "        dp_trn_den_j['V_all_den'] = mvden(dp_trn_b_res_j.drop([cin]+[tin],axis = 1)\n",
    "                                      ,dp_trn_b_res_j.drop([cin]+[tin],axis = 1)\n",
    "                                      ,h_dp_trn_b_res_j,k_mvd)\n",
    "        dp_tst_den_j['V_all_den'] = mvden(dp_trn_b_res_j.drop([cin]+[tin],axis = 1)\n",
    "                                      ,dp_tst_b_res_j.drop([cin]+[tin],axis = 1)\n",
    "                                      ,h_dp_trn_b_res_j,k_mvd)\n",
    "        if j == 1:\n",
    "            # If first cross section intialize long panel version of temp b2 \n",
    "            dp_trn_den = dp_trn_den_j\n",
    "            dp_tst_den = dp_tst_den_j\n",
    "        else:\n",
    "            # If not first cross section adding b2 to the bottom of b3\n",
    "            dp_trn_den = pd.concat([dp_trn_den,dp_trn_den_j],axis = 0)\n",
    "            dp_tst_den = pd.concat([dp_tst_den,dp_tst_den_j],axis = 0)\n",
    "\n",
    "    ## Calculating the bivariate density of each residual and its backshifted values\n",
    "    # For each residual since n_en = #of residuals V_i\n",
    "    for i in range(1,n_en+1):\n",
    "        # Variable names of indexes, ith residuals, and  ith backshifts\n",
    "        clms = [cin , tin , ''.join(['h_V',str(i)]) , ''.join(['bh_V',str(i)])]\n",
    "        # For each cross section\n",
    "        for j in range(1,ncs+1):\n",
    "            # Extracting indexes, ith residuals, and ith backshifts for jth cross section\n",
    "            dp_trn_b_res_ij = dp_trn_b_res.loc[dp_trn_b_res[cin]==j, clms]\n",
    "            dp_tst_b_res_ij = dp_tst_b_res.loc[dp_tst_b_res[cin]==j, clms]\n",
    "            # Standard deviation of training data column(only use training data)\n",
    "            sd_dp_trn_b_res_ij = np.std(dp_trn_b_res_ij.drop([cin]+[tin],axis =1).values,axis = 0)\n",
    "            # Plug in bandwiths for density estimation (only use training data)\n",
    "            h_dp_trn_b_res_ij = c_mvd*(trn_ntp-1)**(-1/6)*sd_dp_trn_b_res_ij\n",
    "            # Initializing c2 temp df\n",
    "            dp_trn_den_ij = dp_trn_b_res_ij.loc[:,[cin]+[tin]].copy()\n",
    "            dp_tst_den_ij = dp_tst_b_res_ij.loc[:,[cin]+[tin]].copy()\n",
    "            # Adding a column of bv densities of ith resid and their backshifts to c2 for crs j\n",
    "            dp_trn_den_ij[''.join(['V',str(i),'_den'])] =  mvden(dp_trn_b_res_ij.drop([cin,tin],axis = 1),\n",
    "                                                                 dp_trn_b_res_ij.drop([cin,tin],axis = 1),\n",
    "                                                                 h_dp_trn_b_res_ij,k_uvd)\n",
    "            # Adding a column of bv densities of ith resid and their backshifts to c2 for crs j\n",
    "            dp_tst_den_ij[''.join(['V',str(i),'_den'])] =  mvden(dp_trn_b_res_ij.drop([cin,tin],axis = 1),\n",
    "                                                                 dp_tst_b_res_ij.drop([cin,tin],axis = 1),\n",
    "                                                                 h_dp_trn_b_res_ij,k_uvd)\n",
    "            if j == 1:\n",
    "                # If first cross section intialize long panel version of temp c2 \n",
    "                dp_trn_den_i = dp_trn_den_ij\n",
    "                # If first cross section intialize long panel version of temp c2 \n",
    "                dp_tst_den_i = dp_tst_den_ij\n",
    "            else:\n",
    "                # If not first cross section adding c2 to the bottom of c3\n",
    "                dp_trn_den_i = pd.concat([dp_trn_den_i,dp_trn_den_ij],axis = 0)  \n",
    "                # If not first cross section adding c2 to the bottom of c3\n",
    "                dp_tst_den_i = pd.concat([dp_tst_den_i,dp_tst_den_ij],axis = 0) \n",
    "\n",
    "        # If not first cross section adding b2 to the bottom of b3\n",
    "        dp_trn_den = pd.merge(dp_trn_den,dp_trn_den_i,on=[cin,tin], how = 'inner')\n",
    "        dp_tst_den = pd.merge(dp_tst_den,dp_tst_den_i,on=[cin,tin], how = 'inner')\n",
    "\n",
    "    ## Constructing phi ratio each theta density ratio\n",
    "    # List of residuals (by index) to be included in the numerator of each ratio\n",
    "    ratio_num_incl_res = [list(iter.filterfalse(lambda x: x==i, range(1,n_en+1))) for i in range(1,n_en+1)]\n",
    "    # List of list of densities included in the numerator or some ratios.\n",
    "    ratio_num_incl_den = [[''.join(['V',str(ratio_num_incl_res[i][j]),'_den']) \n",
    "                                     for j in range(n_en-1)] for i in range(n_en)]\n",
    "\n",
    "    # Constructing each ratio and adding to den df\n",
    "    for i in range(n_en):\n",
    "        ratio_i_nm = ''.join(['th',str(i+1)])\n",
    "        dp_trn_den[ratio_i_nm] = (np.prod(dp_trn_den.loc[:,ratio_num_incl_den[i]].values,axis = 1)\n",
    "                                  /dp_trn_den.loc[:,'V_all_den'].values)\n",
    "        dp_tst_den[ratio_i_nm] = (np.prod(dp_tst_den.loc[:,ratio_num_incl_den[i]].values,axis = 1)\n",
    "                                  /dp_tst_den.loc[:,'V_all_den'].values)\n",
    "    # List of names of all bv densities\n",
    "    biv_den_nms = [''.join(['V',str(i),'_den']) for i in range(1,n_en+1)]\n",
    "\n",
    "    # Constructing phi ratio\n",
    "    dp_trn_den['phi'] = (np.prod(dp_trn_den.loc[:,biv_den_nms].values,axis = 1)\n",
    "                                  /dp_trn_den.loc[:,'V_all_den'].values)\n",
    "    dp_tst_den['phi'] = (np.prod(dp_tst_den.loc[:,biv_den_nms].values,axis = 1)\n",
    "                                  /dp_tst_den.loc[:,'V_all_den'].values)\n",
    "\n",
    "    ## Construction differenced dataframe\n",
    "    # Names of all variables to be differenced\n",
    "    all_dff_nm = [dep_nm] + en_nm + ex_nm\n",
    "    # New names with 'D' concatenated at beginning of string\n",
    "    new_dff_nm = [ ''.join(['D',all_dff_nm[j]]) for j in range(len(all_dff_nm))]\n",
    "\n",
    "    # For each variable to be differenced\n",
    "    for i in range(len(all_dff_nm)):\n",
    "        # For each cross section\n",
    "        for j in range(1,ncs+1):\n",
    "            dp_trn_dff_ij_p = dp_trn.loc[dp_trn[cin]==j,[cin]+[tin]][1:].copy()\n",
    "            dp_tst_dff_ij_p = dp_tst.loc[dp_tst[cin]==j,[cin]+[tin]][1:].copy()\n",
    "\n",
    "            # For each cross section generate temp df with [cin tin D(var)]\n",
    "            dp_trn_dff_ij_p[new_dff_nm[i]] = (dp_trn.loc[dp_trn[cin]==j,all_dff_nm[i]] \n",
    "                                             -dp_trn.loc[dp_trn[cin]==j,all_dff_nm[i]].shift(1))[1:]\n",
    "            dp_tst_dff_ij_p[new_dff_nm[i]] = (dp_tst.loc[dp_tst[cin]==j,all_dff_nm[i]] \n",
    "                                             -dp_tst.loc[dp_tst[cin]==j,all_dff_nm[i]].shift(1))[1:]\n",
    "            if j==1: \n",
    "                # If first cross section intialize long panel version of temp f1 \n",
    "                dp_trn_dff_i = dp_trn_dff_ij_p\n",
    "                dp_tst_dff_i = dp_tst_dff_ij_p\n",
    "            else: \n",
    "                # If not first cross section adding f1 to the bottom of f2\n",
    "                dp_trn_dff_i = pd.concat([dp_trn_dff_i,dp_trn_dff_ij_p],axis = 0)\n",
    "                # If not first cross section adding f1 to the bottom of f2\n",
    "                dp_tst_dff_i = pd.concat([dp_tst_dff_i,dp_tst_dff_ij_p],axis = 0)\n",
    "        #\n",
    "        if i == 0:\n",
    "            dp_trn_dff = dp_trn_dff_i\n",
    "            dp_tst_dff = dp_tst_dff_i\n",
    "        else:\n",
    "            dp_trn_dff = pd.merge(dp_trn_dff,dp_trn_dff_i,on=[cin,tin],how = 'inner')\n",
    "            dp_tst_dff = pd.merge(dp_tst_dff,dp_tst_dff_i,on=[cin,tin],how = 'inner')\n",
    "\n",
    "    ## Constructing the array of H functions\n",
    "    # Initializing the H functions df\n",
    "    dp_trn_H = dp_trn_b_res.iloc[:,:2].copy()\n",
    "    dp_tst_H = dp_tst_b_res.iloc[:,:2].copy()\n",
    "\n",
    "    # For all varaibes in primary regression \n",
    "    for pdep_nm in new_dff_nm:\n",
    "        # For each residual = # of endogenous variables\n",
    "        for j in range(1,n_en+1):\n",
    "            # For each cross section\n",
    "            for k in range(1,ncs+1):\n",
    "                # Initailizing the temporary H function df for testing data\n",
    "                dp_tst_H_ijk = dp_tst_b_res.loc[dp_tst_b_res[cin]==k,[tin]+[cin]].copy()\n",
    "                # ith dependent variable for kth crs and for training data\n",
    "                dp_trn_H_ijk_dep = dp_trn_dff.loc[dp_trn_dff[cin]== k,pdep_nm]\n",
    "                # jth density ratio for kth crs and for training data\n",
    "                dp_trn_H_ijk_tht = dp_trn_den.loc[dp_trn_den[cin] == k,''.join(['th',str(j)])]\n",
    "                # jth regressors for kth crs and for training data \n",
    "                dp_trn_H_ijk_reg = dp_trn_b_res.loc[dp_trn_b_res[cin] == k ,[''.join(['h_V',str(j)])\n",
    "                                                                ,''.join(['bh_V',str(j)])]]\n",
    "                # jth regressors for kth crs and for testing data \n",
    "                dp_tst_H_ijk_reg = dp_tst_b_res.loc[dp_tst_b_res[cin] == k ,[''.join(['h_V',str(j)])\n",
    "                                                                ,''.join(['bh_V',str(j)])]]\n",
    "                # Plug in bandwidths based on testing data\n",
    "                h_tst_H_ijk = c_H*np.std(np.array(dp_trn_H_ijk_reg),axis = 0) \n",
    "                # Adding the estimated function to temp df\n",
    "                H_ijk_nm = ''.join(['H',re.sub('^D','', pdep_nm),';',str(j)])\n",
    "                # H function estimation based on training data evaluated at testing data points.\n",
    "                dp_tst_H_ijk[H_ijk_nm] = Nw_H_pan(dp_trn_H_ijk_dep,dp_trn_H_ijk_tht,dp_trn_H_ijk_reg\n",
    "                                                 ,dp_tst_H_ijk_reg,h_tst_H_ijk,k_H,0)\n",
    "                # Concatenating \n",
    "                if k == 1: \n",
    "                    dp_tst_H_ij = dp_tst_H_ijk\n",
    "                else:\n",
    "                    dp_tst_H_ij = pd.concat([dp_tst_H_ij,dp_tst_H_ijk],axis = 0)\n",
    "            # Merging \n",
    "            if j == 1: \n",
    "                dp_tst_H_i = dp_tst_H_ij\n",
    "            else:\n",
    "                dp_tst_H_i =  pd.merge(dp_tst_H_i,dp_tst_H_ij,on=[cin,tin],how = 'inner')\n",
    "        # Merging into main df       \n",
    "        dp_tst_H =  pd.merge(dp_tst_H,dp_tst_H_i,on=[cin,tin],how = 'inner')\n",
    "\n",
    "    ## Summing all for H functions\n",
    "    # Names of all variables in primary regression\n",
    "    pr_nm = [dep_nm]+en_nm+ex_nm\n",
    "    # Names of all summed H functions\n",
    "    prH_nm = [ ''.join(['H',i ]) for i in pr_nm ]\n",
    "    # Names of all difference variables\n",
    "    prD_nm = [ ''.join(['D',i ]) for i in pr_nm ]\n",
    "    # Names of full [ var - Hvar ]\n",
    "    prS_nm = [ ''.join(['S',i ]) for i in pr_nm ]\n",
    "    # Adding the summed H functions to Hf\n",
    "    for i in range(len(pr_nm)):\n",
    "        dp_tst_H[prH_nm[i]] = np.sum(dp_tst_H.filter(regex = ''.join(['^',prH_nm[i],';'])\n",
    "                                         , axis=1).values,axis =1)\n",
    "\n",
    "    # Initialized the subtracted df\n",
    "    dp_tst_sub = dp_tst_H.loc[:,[cin]+[tin]].copy()\n",
    "    # Construcing the subtracted df\n",
    "    for i in range(len(pr_nm)): \n",
    "        dp_tst_sub[prS_nm[i]] = dp_tst_dff.loc[:,prD_nm[i]].sub(dp_tst_H.loc[:,prH_nm[i]])\n",
    "\n",
    "    # First matrix constructed by extracting the subtracted dependent variable\n",
    "    dep_mat = (dp_tst_sub.loc[:,''.join(['S',dep_nm])].values)\n",
    "    # Reshape so it is 2 dimensional\n",
    "    dep_mat = dep_mat.reshape(dep_mat.shape[0],1)\n",
    "    # Second matrix constructed by droppin cin tin and subtracted dependent var\n",
    "    reg_mat = dp_tst_sub.drop([cin ,tin , ''.join(['S',dep_nm])],axis = 1 ).values\n",
    "    # Constructing the diagonal phi matrix\n",
    "    phi_diag = np.diag(dp_tst_den.loc[:,'phi'])\n",
    "    # Denominator of the estimator\n",
    "    denom_mat = np.linalg.inv(reg_mat.T.dot(phi_diag).dot(reg_mat))\n",
    "    # Numerator\n",
    "    numer_mat = reg_mat.T.dot(phi_diag).dot(dep_mat)\n",
    "    # Final Estimated value\n",
    "    tst_beta_coeff = denom_mat.dot(numer_mat).T.tolist()\n",
    "    # Collecting outputs\n",
    "    out = [tst_beta_coeff , trn_cf , trn_inc ]\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 4.2 Double PSC All Partitions Estimator Wrapper function </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psc_dbl_est_all(data_pan,data_inst,data_err,inpt):\n",
    "\n",
    "    \"\"\"\n",
    "INPUTS\n",
    "data_pan                       (df) panel of all primary regressors each crss sect, and time period\n",
    "data_inst                      (df) all possible instruments \n",
    "data_err                       (df) panel df of all error terms, only used if inpt['orcl'] == 1\n",
    "inpt                           (dct) The following input var. \n",
    "  inpt['cin']                    (str) names of cross section var. in data_pan & data_err\n",
    "  inpt['tin']                    (str) names of cross section var. in data_pan & data_err\n",
    "  inpt['ntp']                    (int) number time periods\n",
    "  inpt['ncs']                    (int) number of crossections\n",
    "  inpt['n_end']                  (int) number of endogenous variables \n",
    "  inpt['n_exo']                  (int) number of exogenous variables\n",
    "  inpt['t_inst']\n",
    "  inpt['dep_nm']                 (str) name of dependent varaible in primary regression in data_pan \n",
    "  inpt['en_nm']                  (lst) names of endogenous variables in data_pan\n",
    "  inpt['ex_nm']                  (lst) names of exogenous variables in data_pan\n",
    "  inpt['in_nm']                  (lst) names of instruments relevant to each cross section\n",
    "    inpt['in_nm'][i]               (lst of str) names of var. in data_inst used by cin == i\n",
    "  inpt['n_alphs']                (int) number of alphas to try in cv routine\n",
    "  inpt['n_parts']                (int) number of cv partition elements\n",
    "  inpt['sec_pan']                (int) Indicator for if secondary equation data is panel\n",
    "  inpt['lasso']                  (int) Indicator for if W_{i} is a known subset of W for all cin \n",
    "  inpt['epsil']                  (flt) lasso param threshold for averaging \"non zero\" coefficients\n",
    "  inpt['alph']                   (flt) lasso param penalty value for lasso estimation\n",
    "  inpt['cv']                     (int) indicator for cross validated lasso parameter\n",
    "  inpt['orcl']                   (int) Indicator for if V_i,j is observed for each cin, and endog var. \n",
    "  inpt['k_mvd']                  (int) kernel used for multi var density est. see (?mvkernel)\n",
    "  inpt['k_uvd']                  (int) kernel used for bi var density est. see (?mvkernel)\n",
    "  inpt['k_H']                    (int) kernel used for NW H function  est. see (?mvkernel)\n",
    "  inpt['c_mvd']                  (flt) plug in bandwidth const. for  multi var density est.\n",
    "  inpt['c_uvd']                  (flt) plug in bandwidth const. for bi var density est.\n",
    "  inpt['c_H']                    (flt) plug in bandwidth const. for NW H function  est.\n",
    " \n",
    "OUTPUT \n",
    "results                       (lst) results for the following\n",
    "  results[0][j-1]                (lst) for jth regr in primary regress\n",
    "    results[0][j-1][i-1]             (lst) for ith part of jth regr in primary regress\n",
    "  results[1][k-1]                (lst) for kth secondary regress w endg dep varaible\n",
    "    results[1][k-1][j-1]             (lst) for jth regr in secondary regress w kth endg dep var\n",
    "      results[1][k-1][j-1][i-1]        (lst) for ith part of jth regr in secondary regress w kth endg dep var\n",
    "  results[2]                     (lst) output from k_subs function (set ?k_subs) for info\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generation of k partitions\n",
    "    k_partitions = k_subs(inpt['k_parts'],inpt['ntp'],inpt['ncs'])\n",
    "\n",
    "    # Loop over each partition\n",
    "    for current_partition in range(inpt['k_parts']):\n",
    "        # Adding current partition elements to the input dictionary\n",
    "        inpt['inst_partition'] = k_partitions[0][current_partition]\n",
    "        inpt['panel_partition'] = k_partitions[1][current_partition]\n",
    "        # Estiamtor function call\n",
    "        psc_single_output = psc_dbl_est(data_pan,data_inst,data_err,inpt)\n",
    "        # collecting result together\n",
    "        if current_partition == 0:\n",
    "            psc_all_output = [psc_single_output[0]]\n",
    "            psc_all_output.append([psc_single_output[1]])\n",
    "        else:\n",
    "            psc_all_output[0].append(psc_single_output[0][0])\n",
    "            psc_all_output[1].append(psc_single_output[1])\n",
    "\n",
    "    # Reshaping output\n",
    "    psc_primary = [[psc_all_output[0][i][j] for i in range(inpt['k_parts'])] \n",
    "                                            for j in range(inpt['n_exo']+inpt['n_end'])]\n",
    "    psc_secondary = [[[psc_all_output[1][i][k][j] for i in range(inpt['k_parts'])] \n",
    "                                                  for j in range(inpt['n_end']+inpt['t_inst'])] \n",
    "                                                  for k in range(inpt['n_end'])]\n",
    "    # Collecting results\n",
    "    results = [psc_primary, psc_secondary, k_partitions]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 5 Monte Carlo </h2>\n",
    "\n",
    "The data used in the following monte carlo exercise was created by 'psc_dgp.ipynb' where is was converted to a .json format and stored in '.../pan_sel_cntrl_repo/data' folder. The following sections load and convert this .json data into a usable form. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 5.1 Monte Carlo: JSON loading and extracting metadata dictionary </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filename = 'pscdata_7_25_1313.json'\n",
    "data_file = '/Users/ericpenner/Google_Drive/Research/pan_sel_cntrl/data'\n",
    "input_file_full = ''.join([data_file,'/',input_filename])\n",
    "with open(input_file_full) as f_obj: \n",
    "    pscdata = json.load(f_obj)\n",
    "    \n",
    "# Initializing the data sets metadata dictionary \n",
    "inpt = pscdata[0][0].copy()\n",
    "# Removing data that should not be passed to estimator\n",
    "for i in ['c_inst','err_vpro','ex_vpro','inst_vpro','r_seed','frc', 'nds']:\n",
    "    del inpt[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 5.2 Monte Carlo: Creating the input dictionary construction </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adding the input file name so output dict will reference the dgp use to gen it\n",
    "inpt['input_filename'] = input_filename\n",
    "\n",
    "# Indicator for  whether in this run the subset of instrument relvant to each crs is known.\n",
    "inpt['kwnsub'] = 1\n",
    "\n",
    "# Indicator for whether residuals are observed i.e. for \"oracle estimation\"\n",
    "inpt['orcl'] = 0\n",
    "\n",
    "# lasso indicator and parameters\n",
    "inpt['alph'] = 0.4\n",
    "inpt['epsil'] = 0.1\n",
    "inpt['lasso'] = 0\n",
    "\n",
    "# Indicator for cv\n",
    "inpt['cv'] = 0\n",
    "inpt['n_parts'] = 4\n",
    "inpt['n_alphs'] = 20\n",
    "\n",
    "# Setting Kernel and bandwidth constant variables.\n",
    "inpt['k_mvd'] = 9\n",
    "inpt['k_uvd'] = 9\n",
    "inpt['k_H'] = 9\n",
    "inpt['c_mvd'] = 1.5\n",
    "inpt['c_uvd'] = 1.4\n",
    "inpt['c_H'] = 1.5\n",
    "\n",
    "# Number of elements in partition\n",
    "inpt['k_parts'] = 4\n",
    "\n",
    "# List of list with the names of the relevant instruments for each crossection\n",
    "in_nm=[]\n",
    "for i in range(inpt['ncs']):\n",
    "    # If the subset is known then list of relevant inst. for each crs is supplied to estimator\n",
    "    if inpt['kwnsub'] == 1:\n",
    "        a=[ True if pscdata[0][1]['coeff'][0][i][k]!=0 else False \n",
    "            for k in range(inpt['n_exo'],inpt['n_exo']+inpt['t_inst'])]\n",
    "        in_nm.append(np.array(pscdata[0][1]['Dins_nms'][1:])[a].tolist())\n",
    "    # If the subset is unknown then list of all inst. will be supplied to est. for each crs\n",
    "    else:\n",
    "        in_nm.append(pscdata[0][1]['Dins_nms'][1:])\n",
    "        \n",
    "# Adding relevant instrument names to the input dictionary\n",
    "inpt['in_nm'] = in_nm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 5.3 Monte Carlo: Single estimator run data extraction </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k=2\n",
    "# data_err = pd.DataFrame(pscdata[k][0]['err_df'], columns = pscdata[0][1]['Derr_nms'])  \n",
    "# data_inst = pd.DataFrame(pscdata[k][0]['inst_df'], columns = pscdata[0][1]['Dins_nms'])\n",
    "# data_pan = pd.DataFrame(pscdata[k][0]['prim_df'], columns = pscdata[0][1]['Dlng_nms'])\n",
    "\n",
    "# test = psc_dbl_est_all(data_pan,data_inst,data_err,inpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pprint\n",
    "inpt['ntp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 5.4 Monte Carlo: Full Exercise Execution </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180369c9c3844e5abd4479683790d5ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>FloatProgress</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='success', description='Running:', max=1000.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-13-e31371983eb2>(169)panel_dbl_est()\n",
      "-> trn_dep_vals = dpdi_trn_mrg_all.loc[dpdi_trn_mrg[cin] == i,[dep]].values\n",
      "(Pdb) l\n",
      "164  \t\n",
      "165  \t    # Constucting a panel df of estimated errors Vj,i\n",
      "166  \t    for i in range(1,ncs+1):\n",
      "167  \t        import pdb; pdb.set_trace()\n",
      "168  \t        # np.array of dep variable values for ith crs\n",
      "169  ->\t        trn_dep_vals = dpdi_trn_mrg_all.loc[dpdi_trn_mrg[cin] == i,[dep]].values\n",
      "170  \t        tst_dep_vals = dpdi_tst_mrg_all.loc[dpdi_tst_mrg[cin] == i,[dep]].values\n",
      "171  \t        # Regressor values for ith crs\n",
      "172  \t        trn_regr_vals = dpdi_trn_mrg_all.loc[dpdi_trn_mrg[cin] == i,all_reg_inst_names].values\n",
      "173  \t        tst_regr_vals = dpdi_tst_mrg_all.loc[dpdi_tst_mrg[cin] == i,all_reg_inst_names].values\n",
      "174  \t        # Relevant Regressor values for ith crs\n",
      "(Pdb) n\n",
      "> <ipython-input-13-e31371983eb2>(170)panel_dbl_est()\n",
      "-> tst_dep_vals = dpdi_tst_mrg_all.loc[dpdi_tst_mrg[cin] == i,[dep]].values\n",
      "(Pdb) n\n",
      "> <ipython-input-13-e31371983eb2>(172)panel_dbl_est()\n",
      "-> trn_regr_vals = dpdi_trn_mrg_all.loc[dpdi_trn_mrg[cin] == i,all_reg_inst_names].values\n",
      "(Pdb) n\n",
      "> <ipython-input-13-e31371983eb2>(173)panel_dbl_est()\n",
      "-> tst_regr_vals = dpdi_tst_mrg_all.loc[dpdi_tst_mrg[cin] == i,all_reg_inst_names].values\n",
      "(Pdb) n\n",
      "> <ipython-input-13-e31371983eb2>(175)panel_dbl_est()\n",
      "-> trn_relev_regr_vals = trn_regr_vals.dot(np.diag(trn_relev_regr_vec[i-1]))\n",
      "(Pdb) n\n",
      "> <ipython-input-13-e31371983eb2>(176)panel_dbl_est()\n",
      "-> tst_relev_regr_vals = tst_regr_vals.dot(np.diag(trn_relev_regr_vec[i-1]))\n",
      "(Pdb) n\n",
      "> <ipython-input-13-e31371983eb2>(178)panel_dbl_est()\n",
      "-> trn_non_center_resids = trn_dep_vals - trn_relev_regr_vals.dot(\n",
      "(Pdb) pp tst_relev_regr_vals\n",
      "array([[-0.05478647,  0.54623207,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.61354089,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        -0.84420704,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        -0.15830673,  0.        ],\n",
      "       [-0.37439055,  1.11348027,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  1.1806415 ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        -0.99489055,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         1.6864021 ,  0.        ],\n",
      "       [-0.78613761, -1.8043735 ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        , -0.14876187,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         1.19480464,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        -0.09339121,  0.        ],\n",
      "       [-0.07973191, -0.36032122,  0.        ,  0.        ,  0.        ,\n",
      "         0.        , -0.20809074,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.8407158 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        -1.19778183,  0.        ],\n",
      "       [ 1.14311162,  0.03877898,  0.        ,  0.        ,  0.        ,\n",
      "         0.        , -0.03465263,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.7806011 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         1.9051275 ,  0.        ],\n",
      "       [ 0.39567824,  0.15749976,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  1.72197363,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        -0.0118202 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.73397338,  0.        ],\n",
      "       [-0.72089476, -1.294703  ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  1.63337561,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.7906405 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        -1.43690708,  0.        ],\n",
      "       [ 1.24654339,  1.59127468,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.85810547,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.96221746,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        -0.76993756,  0.        ],\n",
      "       [-0.51631514, -0.47497193,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.01608633,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.44993526,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         2.09523663,  0.        ],\n",
      "       [ 0.42850048, -0.04816742,  0.        ,  0.        ,  0.        ,\n",
      "         0.        , -3.12955786,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.51307837,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        -0.37587456,  0.        ],\n",
      "       [-0.07467266, -0.41338898,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.61576168,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        -0.13772767,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.28629298,  0.        ],\n",
      "       [-0.86089145, -1.08619715,  0.        ,  0.        ,  0.        ,\n",
      "         0.        , -0.21836133,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.93813354,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        -0.59198035,  0.        ],\n",
      "       [ 0.13555165,  0.41601993,  0.        ,  0.        ,  0.        ,\n",
      "         0.        , -0.5144125 ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.32848925,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.72145586,  0.        ],\n",
      "       [ 0.81696183,  0.70123267,  0.        ,  0.        ,  0.        ,\n",
      "         0.        , -0.83001192,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         2.49214981,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.41267473,  0.        ],\n",
      "       [ 0.4664144 ,  0.8898585 ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        , -0.84412187,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.12540864,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.7050334 ,  0.        ],\n",
      "       [ 0.8000061 ,  1.1793443 ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        , -1.24131699,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        -2.3569843 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.60634812,  0.        ],\n",
      "       [-1.07678323, -1.10212749,  0.        ,  0.        ,  0.        ,\n",
      "         0.        , -0.42873708,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        -0.93665478,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        -1.67103109,  0.        ],\n",
      "       [ 2.99504324,  2.14503898,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  2.05723281,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        -0.06155337,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        -1.12550683,  0.        ],\n",
      "       [-1.1044205 , -1.20188704,  0.        ,  0.        ,  0.        ,\n",
      "         0.        , -0.17782117,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.17286344,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        -1.04739783,  0.        ],\n",
      "       [-0.3611374 ,  0.67171558,  0.        ,  0.        ,  0.        ,\n",
      "         0.        , -1.1174834 ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        -0.70129198,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.19345214,  0.        ],\n",
      "       [ 0.03265589,  0.4374073 ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.02523137,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.42609969,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         2.26829402,  0.        ],\n",
      "       [ 0.09179403, -0.07560378,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  1.12871007,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         2.77389629,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        -1.01472731,  0.        ],\n",
      "       [-0.21722812, -0.59128778,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.47107429,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.25643803,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.59114459,  0.        ],\n",
      "       [-0.43347062,  0.61689328,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.17805005,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         1.08777067,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.33042032,  0.        ],\n",
      "       [-0.40988715, -1.55142455,  0.        ,  0.        ,  0.        ,\n",
      "         0.        , -0.77929568,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        -0.24042991,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        -0.22850099,  0.        ]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Pdb) pp tst_relev_regr_vals.shape\n",
      "(25, 32)\n",
      "(Pdb) n\n",
      "> <ipython-input-13-e31371983eb2>(179)panel_dbl_est()\n",
      "-> np.array(trn_est_coeff).reshape(len(trn_est_coeff),1))\n",
      "(Pdb) n\n",
      "> <ipython-input-13-e31371983eb2>(180)panel_dbl_est()\n",
      "-> tst_non_center_resids = tst_dep_vals - tst_relev_regr_vals.dot(\n",
      "(Pdb) n\n",
      "> <ipython-input-13-e31371983eb2>(181)panel_dbl_est()\n",
      "-> np.array(trn_est_coeff).reshape(len(trn_est_coeff),1))\n",
      "(Pdb) n\n",
      "> <ipython-input-13-e31371983eb2>(183)panel_dbl_est()\n",
      "-> trn_center_resids = trn_non_center_resids - np.mean(trn_non_center_resids)\n",
      "(Pdb) n\n",
      "> <ipython-input-13-e31371983eb2>(185)panel_dbl_est()\n",
      "-> tst_center_resids = tst_non_center_resids - np.mean(trn_non_center_resids)\n",
      "(Pdb) n\n",
      "> <ipython-input-13-e31371983eb2>(186)panel_dbl_est()\n",
      "-> if i == 1:\n",
      "(Pdb) tst_center_resids\n",
      "array([[-1.2601236 ],\n",
      "       [-0.20868167],\n",
      "       [-0.37644386],\n",
      "       [ 0.15502733],\n",
      "       [ 0.02890975],\n",
      "       [-0.35392746],\n",
      "       [ 0.34915923],\n",
      "       [ 0.15206075],\n",
      "       [-0.9900254 ],\n",
      "       [-1.18556908],\n",
      "       [ 0.81909323],\n",
      "       [-0.84023738],\n",
      "       [ 0.30666116],\n",
      "       [ 1.04246032],\n",
      "       [-0.95369924],\n",
      "       [ 0.28554119],\n",
      "       [-0.38339698],\n",
      "       [ 0.9769622 ],\n",
      "       [-2.16723604],\n",
      "       [-0.78378808],\n",
      "       [ 0.96670688],\n",
      "       [ 0.02155102],\n",
      "       [ 1.52158663],\n",
      "       [-0.07688267],\n",
      "       [-2.6486598 ]])\n",
      "(Pdb) n\n",
      "> <ipython-input-13-e31371983eb2>(188)panel_dbl_est()\n",
      "-> trn_center_resids_full = [list(trn_center_resids.T[0])]\n",
      "(Pdb) n\n",
      "> <ipython-input-13-e31371983eb2>(189)panel_dbl_est()\n",
      "-> tst_center_resids_full = [list(tst_center_resids.T[0])]\n",
      "(Pdb) n\n",
      "> <ipython-input-13-e31371983eb2>(166)panel_dbl_est()\n",
      "-> for i in range(1,ncs+1):\n",
      "(Pdb) n\n",
      "> <ipython-input-13-e31371983eb2>(167)panel_dbl_est()\n",
      "-> import pdb; pdb.set_trace()\n",
      "(Pdb) n\n",
      "> <ipython-input-13-e31371983eb2>(169)panel_dbl_est()\n",
      "-> trn_dep_vals = dpdi_trn_mrg_all.loc[dpdi_trn_mrg[cin] == i,[dep]].values\n",
      "(Pdb) q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-1c9c4079a7b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mdata_pan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpscdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prim_df'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpscdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Dlng_nms'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#pdb.run(psc_dbl_est_all(data_pan,data_inst,data_err,inpt))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mpsc_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsc_dbl_est_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_pan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_inst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_err\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mbeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsc_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0malpha\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsc_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-2e0260e25002>\u001b[0m in \u001b[0;36mpsc_dbl_est_all\u001b[0;34m(data_pan, data_inst, data_err, inpt)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0minpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'panel_partition'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk_partitions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_partition\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Estiamtor function call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mpsc_single_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsc_dbl_est\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_pan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_inst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_err\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;31m# collecting result together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcurrent_partition\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-4b8920ca4738>\u001b[0m in \u001b[0;36mpsc_dbl_est\u001b[0;34m(data_pan, data_inst, data_err, inpt)\u001b[0m\n\u001b[1;32m    123\u001b[0m                           }\n\u001b[1;32m    124\u001b[0m                 \u001b[0;31m# Estimation note\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                 \u001b[0mpan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpanel_dbl_est\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minpt_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                     \u001b[0mtrn_cf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpan\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-e31371983eb2>\u001b[0m in \u001b[0;36mpanel_dbl_est\u001b[0;34m(dp_in, di_in, inpt_p)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;31m# np.array of dep variable values for ith crs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mtrn_dep_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdpdi_trn_mrg_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdpdi_trn_mrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcin\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0mtst_dep_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdpdi_tst_mrg_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdpdi_tst_mrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcin\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# Regressor values for ith crs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-e31371983eb2>\u001b[0m in \u001b[0;36mpanel_dbl_est\u001b[0;34m(dp_in, di_in, inpt_p)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;31m# np.array of dep variable values for ith crs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mtrn_dep_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdpdi_trn_mrg_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdpdi_trn_mrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcin\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0mtst_dep_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdpdi_tst_mrg_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdpdi_tst_mrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcin\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# Regressor values for ith crs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Progress bar widget\n",
    "f = ipw.FloatProgress(min=0,max=pscdata[0][0]['nds']\n",
    "                 ,description = 'Running:'\n",
    "                 ,bar_style = 'success') # instantiate the bar\n",
    "display(f) # display the bar\n",
    "\n",
    "# Initializing the estimated coefficient lists\n",
    "beta= []\n",
    "alpha = []\n",
    "kparts = []\n",
    "# Looping through each data set\n",
    "for k in range(1,pscdata[0][0]['nds']+1):\n",
    "    data_err = pd.DataFrame(pscdata[k][0]['err_df'], columns = pscdata[0][1]['Derr_nms'])  \n",
    "    data_inst = pd.DataFrame(pscdata[k][0]['inst_df'], columns = pscdata[0][1]['Dins_nms'])\n",
    "    data_pan = pd.DataFrame(pscdata[k][0]['prim_df'], columns = pscdata[0][1]['Dlng_nms'])\n",
    "    #pdb.run(psc_dbl_est_all(data_pan,data_inst,data_err,inpt))\n",
    "    psc_results = psc_dbl_est_all(data_pan,data_inst,data_err,inpt)\n",
    "    beta.append(psc_results[0])\n",
    "    alpha.append(psc_results[1])\n",
    "    kparts.append(psc_results[1])\n",
    "    if k == 1:\n",
    "        inc_out = psc_results[2]\n",
    "    f.value += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 5.5 Extracting True Secondary Equation Coefficients </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[1]*32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_coeff_sums = [[np.sum([pscdata[0][1]['coeff'][k][j][i] for j in range(inpt['ncs'])]) \n",
    "                               for i in range(inpt['n_end']+inpt['t_inst'])] \n",
    "                               for k in range(inpt['n_end'])]\n",
    "\n",
    "sec_coeff = [[np.sign(sec_coeff_sums[k][i]) for i in range(inpt['n_end']+inpt['t_inst'])] \n",
    "                                            for k in range(inpt['n_end'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 5.6 Coefficient Summary Statistics </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Primary Equation Coefficients \n",
    "beta_means = [[ np.mean(beta[k][i]) for k in range(pscdata[0][0]['nds'])] \n",
    "                                    for i in range(inpt['n_end']+inpt['n_exo'])]\n",
    "\n",
    "beta_means_bias = [[beta_means[i][j] - pscdata[0][1]['pcoeff'][i]\n",
    "                                   for j in range(pscdata[0][0]['nds'])]\n",
    "                                   for i in range(inpt['n_end']+inpt['n_exo'])]\n",
    "                                   \n",
    "beta_means_bias_full = [ np.mean(beta_means_bias[i]) for i in range(inpt['n_end']+inpt['n_exo']) ]\n",
    "\n",
    "beta_means_std_full = [ np.std(beta_means_bias[i]) for i in range(inpt['n_end']+inpt['n_exo'])]\n",
    "\n",
    "beta_means_mse_full = [ beta_means_std_full[i]**2 + beta_means_bias_full[i]**2\n",
    "                                                   for i in range(inpt['n_end']+inpt['n_exo'])]\n",
    "\n",
    "## Secondary Equation Coefficients\n",
    "alpha_means = [[[ np.mean(alpha[k][j][i]) for k in range(pscdata[0][0]['nds'])] \n",
    "                                          for i in range(inpt['n_end']+inpt['t_inst'])]\n",
    "                                          for j in range(inpt['n_end'])]\n",
    "\n",
    "alpha_means_bias = [[[ alpha_means[i][j][k] - sec_coeff[i][j]\n",
    "                                          for k in range(pscdata[0][0]['nds']) ]    \n",
    "                                          for j in range(inpt['n_end']+inpt['t_inst'])]\n",
    "                                          for i in range(inpt['n_end'])]\n",
    "\n",
    "\n",
    "alpha_means_bias_full = [[ np.mean(alpha_means_bias[i][j]) for j in range(inpt['n_end']+inpt['t_inst']) ]\n",
    "                                                           for i in range(inpt['n_end'])]\n",
    "\n",
    "alpha_means_std_full = [[ np.std(alpha_means_bias[i][j]) for j in range(inpt['n_end']+inpt['t_inst']) ]\n",
    "                                                         for i in range(inpt['n_end'])]\n",
    "\n",
    "alpha_means_mse_full = [[alpha_means_bias_full[i][j]**2 + alpha_means_std_full[i][j]**2 \n",
    "                                                         for j in range(inpt['n_end']+inpt['t_inst'])]\n",
    "                                                         for i in range(inpt['n_end'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 5.7 Summary Table Construction </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_summary_columns = ([''.join(['$\\\\hat{\\\\beta}_{1,',str(i),'}$']) \n",
    "                            for i in range(1,pscdata[0][0]['n_end']+1)] \n",
    "                       + [''.join(['$\\\\hat{\\\\beta}_{2,',str(i),'}$']) \n",
    "                            for i in range(1,pscdata[0][0]['n_exo']+1)])\n",
    "\n",
    "tbl_rows = ['Bias','Std','MSE']\n",
    "\n",
    "beta_summary_tbl = [beta_means_bias_full,beta_means_std_full,beta_means_mse_full]\n",
    "\n",
    "alpha_summary_columns = [[''.join(['$\\\\hat{\\\\alpha}_{',str(k+1),'1,',str(i),'}$']) \n",
    "                             for i in range(1,pscdata[0][0]['n_exo']+1)] \n",
    "                          + [''.join(['$\\\\hat{\\\\alpha}_{', str(k+1),'2,',str(i+1),'}$'])\n",
    "                             for i in range(inpt['t_inst'])]\n",
    "                             for k in range(inpt['n_end'])]\n",
    "\n",
    "alpha_summary_tbls = [[alpha_means_bias_full[i],alpha_means_std_full[i],alpha_means_mse_full[i]] \n",
    "                             for i in range(inpt['n_end'])]\n",
    "# Displaying summary tables\n",
    "display(pd.DataFrame(beta_summary_tbl,columns = beta_summary_columns, index = tbl_rows))\n",
    "display(pd.DataFrame(alpha_summary_tbls[0],columns = alpha_summary_columns[0], index = tbl_rows))\n",
    "display(pd.DataFrame(alpha_summary_tbls[1],columns = alpha_summary_columns[1], index = tbl_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 5.8 Output Dictionary construction </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dictionary or summary results for beta\n",
    "beta_results ={'beta_cntrd': beta_means_bias, 'beta_sum_dat': beta_summary_tbl,\n",
    "               'beta_sum_clmn': beta_summary_columns, 'beta_sum_row': tbl_rows }\n",
    "# Dictionary of summary results for alpha\n",
    "alpha_results = {'alpha_cntrd':alpha_means_bias , 'alpha_sum_dat': alpha_summary_tbls,\n",
    "                'alpha_sum_clmn': alpha_summary_columns, 'alpha_sum_row': tbl_rows}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 5.7 Exporting to JSON </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt['inc'] = inc_out\n",
    "psc_out = [inpt, beta_results, alpha_results]\n",
    "out_folder ='/Users/ericpenner/Google_Drive/Research/pan_sel_cntrl/output/'\n",
    "date = dt.datetime.now()\n",
    "output_filename = ''.join(['pscout_',str(date.month),'_'\n",
    "                           ,str(date.day),'_'\n",
    "                           ,str(np.random.randint(1000,2000)),'.json'])\n",
    "output_file_full = ''.join([out_folder,'/',output_filename])\n",
    "\n",
    "with open(output_file_full, 'w') as f_obj:\n",
    "    json.dump(psc_out, f_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 5.8 Interactive plotting of Alpha and Centered Beta List </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Plotting Function\n",
    "def betaden(w,beta,xl,indv,yl):\n",
    "    w = w-1\n",
    "    plt.close('all')\n",
    "    b = np.array(beta[w]).reshape(len(beta[w]),1)\n",
    "    h = [1.04*len(beta[w])**(-1/5)*np.std(b)]\n",
    "    bden = mvden(b,b,h,9).reshape(len(b),1)\n",
    "    bd = np.hstack((b,bden))\n",
    "    bd1 = bd[bd[:,0].argsort(),:]\n",
    "    f,ax = plt.subplots()\n",
    "    f.set_figheight(7)\n",
    "    f.set_figwidth(15)\n",
    "    ax.set_xlim((xl[0],xl[1]))\n",
    "    ax.set_ylim((0,yl))\n",
    "    ax.plot(bd1[:,0],bd1[:,1])\n",
    "    ax.grid(which = 'both')\n",
    "    ax.set_title(''.join(['Distribution of Estimated ',indv[w]]))\n",
    "    plt.show()\n",
    "\n",
    "aj = 0\n",
    "\n",
    "box_hlayout = ipw.Layout(display='flex',flex_flow='row',align_items='stretch'\n",
    "                         ,width='90%')\n",
    "wbs = ipw.IntSlider( min = 1 , max = len(beta_means_bias) , value = 1, step = 1\n",
    "                    , description = 'Coefficient:',width = 'auto',layout = box_hlayout\n",
    "                    , style = {'description_width': 'initial'} )\n",
    "wbsx = ipw.FloatRangeSlider( value=[-0.4, 0.4], min=-0.5,max=0.5, step=0.05\n",
    "                            ,description='x limits:',disabled=False\n",
    "                            ,continuous_update=False, orientation='horizontal',readout=True\n",
    "                            ,readout_format='.1f', width = 'auto', layout=box_hlayout\n",
    "                            ,style = {'description_width': 'initial'})\n",
    "was = ipw.IntSlider( min = 1 , max = len(alpha_means_bias[aj]) , value = 1, step = 1\n",
    "                    ,description = 'Coefficient:'\n",
    "                    ,width = 'auto',layout = box_hlayout\n",
    "                    ,style = {'description_width': 'initial'} )\n",
    "wasx = ipw.FloatRangeSlider( value=[-1.4, 1.4], min=-1.5,max=1.5\n",
    "                            ,step=0.05,description='x limits:',disabled=False\n",
    "                            ,continuous_update=False, orientation='horizontal',readout=True\n",
    "                            ,readout_format='.1f', width = 'auto', layout=box_hlayout\n",
    "                            ,style = {'description_width': 'initial'})\n",
    "\n",
    "bout =  ipw.interactive_output(betaden ,{'w': wbs,'beta':ipw.fixed(beta_means_bias),'xl': wbsx,\n",
    "                                         'indv':ipw.fixed(beta_summary_columns),'yl':ipw.fixed(12)})\n",
    "aout =  ipw.interactive_output(betaden,{'w': was,'beta':ipw.fixed(alpha_means_bias[aj]),'xl': wasx,\n",
    "                                        'indv':ipw.fixed(alpha_summary_columns[aj]),'yl':ipw.fixed(6.5)})\n",
    "ipw.VBox([bout,wbs,wbsx,aout,was,wasx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "289px",
    "left": "910px",
    "right": "20px",
    "top": "120px",
    "width": "333px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
