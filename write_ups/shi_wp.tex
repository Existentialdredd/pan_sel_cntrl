\documentclass[10pt]{article}
\usepackage{geometry}
\usepackage{graphicx} %%%%%%%
%\geometry{height=8.5in,width=6.5in,letterpaper}
\usepackage{amssymb,amsmath,natbib,amsthm,enumerate,graphicx,anysize,epstopdf,dsfont,comment,mathtools,tikz,soul,chngcntr,setspace,fancyhdr,multirow,caption,float,hyperref,fullpage,mathtools,multicol,tagging}
\renewcommand{\bibsection}{}
\usetikzlibrary{calc}
\usetikzlibrary{snakes,positioning}
\usetikzlibrary{shapes,backgrounds}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
%\hypersetup{colorlinks=true}
\newcommand\kh{K\left( \frac{X_i-x}{h_n} \right)}
\newcommand\khd{K\left( \frac{X_{id}-x}{h_n} \right)}
\newcommand\kpsi{K\left( \frac{\psi-x}{h_n} \right)}
\newcommand\xh{\left( \frac{X_i-x}{h_n} \right)}
\newcommand\xhd{\left( \frac{X_{id}-x}{h_n} \right)}
\newcommand\xpsi{\left( \frac{\psi-x}{h_n} \right)}
\newcommand\logf{logf_{y|x}}
\newcommand\sumin{\sum_{i=1}^n}
\newcommand\sumtn{\sum_{t=1}^n}
\newcommand\avein{\frac{1}{n}\sum_{i=1}^n}
\newcommand\ntoi{n \rightarrow \infty}
\newcommand\convd{\stackrel{d}{\rightarrow}}
\newcommand\convas{\stackrel{a.s.}{\rightarrow}}
\newcommand\convp{\stackrel{p}{\rightarrow}}
\newcommand\supg{\underset{x\in G}{\mbox{sup}}}
\newcommand\supt{\underset{\xi\in \xi}{\mbox{sup}}}
\newcommand\fpar{\frac{\partial}{\partial \xi}}
\newcommand\spar{\frac{\partial^2}{\partial \xi \partial \xi'}}
\newcommand{\mbf}[1]{\mathbf{ #1 } }
\newcommand{\dmbf}[1]{ \dot{\mathbf{ #1 }} }
\newcommand{\hmbf}[1]{ \hat{\mathbf{ #1 }} }
\newcommand{\tmbf}[1]{ \tilde{\mathbf{ #1 }} }
\newcommand{\mbs}[1]{\boldsymbol{ #1} }
\newcommand{\dmbs}[1]{ \dot{\boldsymbol{ #1} } }
\newcommand{\hmbs}[1]{ \hat{\boldsymbol{ #1} } }
\newcommand{\tmbs}[1]{ \tilde{\boldsymbol{ #1} } }
\DeclareMathOperator{\diag}{diag}
\newcounter{prfptc}
\newcounter{alem}
\newcounter{athm}


\newtheorem{mydef}{Definition}
\newtheorem{assume}{Assumption}
\newtheorem{thm}{Theorem}
\counterwithin*{thm}{section}
\newtheorem{lemma}{Lemma}
\counterwithin*{lemma}{section}
\newtheorem{corollary}{Corollary}
\newtheorem{proofpart}{Part}[prfptc]
\renewcommand\theproofpart{(\roman{proofpart})}


\def\subsectionautorefname{subsection}
\def\equationautorefname{equation}

\allowdisplaybreaks
\begin{document}

\begin{center}
\doublespacing
\Large Selection of Heterogeneous Instruments in Partially LinearFixed Effects Panel Regression \\ 
\vspace{0.25cm}
\large Eric Penner, \emph{University of California Berkeley} \rm \\
\large August, 2018
\end{center}
\doublespacing
\noindent In general, the study of panel data is concerned with accommodating the varying sources of heterogeneity that exist between cross sectional units. This heterogeneity may arise when cross sectional units are separated by some spatial distance be it physical distance, difference in cultural norms, or degree of separation in a network of trading partners. Regardless of the source of this heterogeneity the goal is to identify and estimate some set of effects which are common to all cross sections. Most panel data estimators are solely concerned with treating sources of heterogeneity that cannot be excluded from the primary regression in the guise of time or fixed effects. Few if any panel data estimators consider heterogeneity in the relevant instruments needed to treat the presence of endogenous regressors. This paper develops an estimator which accommodates the reasonable assumption that the instruments needed to decompose the variation in one or more endogenous variables will vary by cross section. This paper also incorporates the complicating factor that the instruments relevant to each cross section are an unknown subset of a potentially high dimensional ($k>>n$) vector.\\ 

\noindent Motivation for an estimator like the one developed in this paper is most clear in the context of Macroeconomics. There it is common to run cross industry, or cross country regressions in order to identify some common characteristic like total factor productivity (Basu et al. 2006) or growth arising from foreign aid (Burnside et al. 2000). As is common in economics, in each of the supplied examples the authors acknowledge and treat the presence of endogenous regressors but, both are either unable or unwilling to consider heterogenous instruments for those endogenous variables. It is well understood that the treatment of endogeneity relies on the use of instrumental variables to decompose the variation of each endogenous regressor into orthogonal parts, one part that is related to the error term and one that isn't. The validity of any treatment of endogeneity relies on the validity of this decomposition. It is reasonable to assume that this decomposition varies by cross section. For example, in the growth and foreign aid panel regression of Burnside and Dollar (2000) foreign aid is the endogenous regressor and it makes sense that the factors which determine the foreign aid that a country like Egypt receives, being that it is neighbors with Israel, is on the Mediterranean, and sits astride the Suez canal, would be different than a country like Papua New Guinea whose foreign aid is likely more humanitarian in nature.            \\
   
\noindent This paper further assumes that the vector of instrumental variables relevant to each cross section is an unknown subset of a large collection. This assumption is really just an acknowledgement that gathering sufficient domain information regarding each cross section in order to determine what the instruments should be is likely too difficult or not possible. Another aspect of this paper is that this collection of potential instruments can be very high dimensional. This allows the use of more modern and high dimensional data sources to be used for decomposing endogenous regressors for example cell phone, and scanner data. \\

\section*{Methodology}


\noindent Given a linear in parameters fixed effects panel data model, suppose that for all cross-sections and time periods a known subset of regressors is correlated with the error term. This endogeneity problem will be resolved using the control function approach of Newey, Powell, and Vella (1999), with the additional complication of having the set of instrumental variables which are appropriate/available for at least one cross-section  not identical to those available/appropriate for a non empty subset of other cross-sections. 



\noindent Suppose that for each time period $t \in \{1,2, \ldots, T\}$, component of $Z_{1jt}$ $ d\in \{1,2, \ldots , p_1\}$, and cross-section $j \in \{1,2,\ldots, q\}$ where $\{q,T\} \in \mathbb{N}$,
\begin{align}
Y_{jt} &= \beta_0 + [\; Z_{1jt}' \;\; Z_{2jt}' \;] \beta_1 + e_j + \varepsilon_{jt} \\
%
Z_{1jdt} &= \alpha_{jd0} + Z_{2jt}' \alpha_{1jd} + W_{jt}' \alpha_{2jd} + V_{jdt} \tag{2d} \\
%
\addtocounter {equation} {1}
E(&\varepsilon_{jt} | Z_{1jt} , Z_{2jt}) = E(\varepsilon_{jt} | Z_{1jt}) \neq 0  \\
%
E(& V_{jdt} | Z_{2jt},W_{jt}) = 0
%
\end{align}
Where for $\{p_1,p_2,c,w,w_1,w_2, \ldots ,w_q \} \subset \mathbb{N}$, $Y_{jt}$, and $\varepsilon_{jt}$ are scalar random variables, $ Z_{2jt}$ is a $p_2$ dimesion vector of exogenous random variables, $Z_{1jt}$  is a vector of endogenous random variables having dimension $p_1$. $e_j$ is a scalar fixed effect, $\varepsilon_{jt}$ is a scalar error term, $V_{jdt}$ is a scalar error term. $W_t = [ \; W_{1t} \;\; W_{2t} \;\; \cdots \;\; W_{wt} \;]'$ is vector of instrumental variables of dimension $w$, $W_{jt} = [\;W_{j1t} \;\; W_{j2t} \;\; \cdots \;\; W_{jw_jt} \;]'$ is a vector of instrumental variables of dimension $w_j$ where $\{W_{jit}\}_{i=1}^{w_j} \subset \{W_{it}\}_{i=1}^{w}$ and there exists at least one pair $j,j'  \in \{1,2, \ldots , p\}$, where $j \neq j'$, such that $\{W_{jit}\}_{i=1}^{w_j} \neq \{W_{j'it}\}_{i=1}^{w_{j'}}$. $\beta_0$ and $\alpha_{d0}$  are scalars, $\beta_1$, $\alpha_{1d}$, $\alpha_{2d}$ are $p_1 +p_2 = p$, $p_2$, and $w_j$ dimensional vectors of real numbers respectively. Lastly for notational convenience let define the following equivalences,
\begin{align*}
Z_{jt} = [\; Z_{1jt}' \;\; Z_{2jt}' \;]'  \;\;\; \text{ and } \;\;\; V_{jt} = [ \; V_{j1t} \; V_{j2t} \; \cdots \;V_{jp_1t}\;]'
\end{align*}
%
\noindent \bf Remarks \rm:
\begin{enumerate}[a.)]
 \item We have in equation (1) a linear in parameters function of primary interest having endogenous random variable $Z_{1jt}$. The focus of this proposal is the identification and estimation of finite dimensional parameter $\beta_1$.
% \item In regards to the underlying stochastic process at work, the current level of generalization allows for either the assumption that, $\alpha_{d1} = 0$, and $\{Z_{2jt} , W_{t}, V_{jt}, \varepsilon_{jt} \}_{t=1}^T $ is an i.i.d sequence of random vectors. Or that
%$\{Z_{2jt} , W_{t}, V_{jt}\}_{t=1}^T $  is a stationary mixing sequence of d.i.d random vectors and  $\{ \varepsilon_{jt} \}_{t=1}^T$ is i.i.d or i.n.i.d.

\end{enumerate}



 \noindent
As mentioned above $E(\varepsilon_{jt} | Z_{1jt} ) \neq 0$ means that the error term $\varepsilon_{jt}$ is not orthogonal to the space of functions of $Z_{1jt}$. Following the control function approach of Newey, Powell, and Vella (1999), I assume that there exists a random variable $u_{jt}$ and unknown function $f_j(\cdot)$ such that;
 \begin{align*}
 \varepsilon_{jt} = f_j(V_{jt}) + u_{jt}  \;\;\;\ \text{ where } \;\;\; E(u_{jt} | \{Z_{j_k},W_{jk},X_{jk},V_{jk} \}_{k=1}^t) = 0
 \end{align*}
 Furthermore I assume that all unknown functions heretofore define are additive, that is, $f_j(V_{jt}) = \sum_{d=1}^{p_1} f_{jd}(V_{jdt})$
%
Where each $f_{jd}(\cdot)$ is a unknown function.
Now where $t>1$, we can rewrite (1) as;
\begin{align*}
Y_{jt} - Y_{j(t-1)} = [ Z_{jt}' - Z_{j(t-1)}' ]\beta_1 +  \sum_{d=1}^{p_1} [ f_{jd}(V_{jdt}) - f_{jd}(V_{jd(t-1)})] + u_{jt} - u_{j(t-1)}
\end{align*}
Adopting the notation that for any finite dimensional random vector $A_{jt}$ define $\Delta A_{jt}  \equiv A_{jt} - A_{j(t-1)}$ we can further rewrite (1) as;
\begin{align*}
\Delta Y_{jt} = \Delta Z_{jt}'\beta_1  + \sum_{l=1}^{p_1} \Delta f_{jl}(V_{jlt}) + \Delta u_{jt}
\end{align*}
As a result of the foregoing assumptions and definitions, we can rewrite the model as the following,
\begin{align}
\Delta Y_{jt} &= \Delta Z_{jt}'\beta_1 + \sum_{d=1}^{p_1} \Delta f_{jd}(V_{jdt}) + \Delta u_{jt} \\
%
Z_{1jdt} &= \alpha_{d0} + Z_{2jt}' \alpha_{1jd} + W_{jt}' \alpha_{2jd} + V_{jdt} \tag{2d}\\
%
E(&\Delta u_{jt} | \{Z_{j_k},W_{jk},X_{jk},V_{jk} \}_{k=1}^t) = 0 \\
%
E(& V_{jt} | Z_{2jt},W_{jt}) = 0
%
\end{align}
%
\noindent \bf Remarks \rm:
\begin{enumerate}[a.)]
    \item In equation (5), by differencing equation (1), including the control function, and imposing additivity, we have eliminated fixed effect $e_j$, but in return we now have a collection of equations $\{f_{j1}(\cdot),f_{j2}(\cdot), \ldots , f_{jp_1}(\cdot)\}$ unique to each cross-section $j$, a fate seemingly even worse than the presence of fixed effects.
    \item In the following I will show that using a variation on the identification technique of Manzan and Zerom (2005) SaPL, it is possible under mild conditions to identify and estimate finite dimensional parameter $\beta_1$  and hopefully achieve $\sqrt{n}$ asymptotic normality  without having to jointly estimate $f_j(V_{jt})$,
\end{enumerate}

\noindent Next in preparation for the following lemma regarding the identification of parameter vector $\beta_1$ I define the following density ratios,
\begin{align*}
\phi_{jt} = \frac{ \prod_{d=1}^{p_1}p(V_{jdt},V_{jd(t-1)}) }{p(V_{jt},V_{j(t-1)})}
\end{align*}
where for any random vector $A_{j}$, $p(A_{j})$ is the joint density of that vector. Define following conditional expectations,
\begin{align*}
H_{jd}(\Delta Z_{jt}) &= E[\phi_{jt} \Delta Z_{jt} |V_{jdt},V_{jd(t-1)}] \hspace{1.25cm}
%
H_{jd}(\Delta Y_{jt}) = E[\phi_{jt} \Delta Y_{jt} |V_{jdt},V_{jd(t-1)}] \\
%
H_j(\Delta Z_{jt}) &= \sum_{d=1}^{p_1} H_{jd}(\Delta Z_{jt})  \hspace{2.5cm}
%
H_j(\Delta Y_{jt}) = \sum_{d=1}^{p_1} H_{jd}(\Delta Y_{jt})  \end{align*}
Lastly, define the following collection of vectors,
\begin{align*}
H(\Delta Y_t)  &= [ \; H_1(\Delta Y_{1t}) \;\; H_2(\Delta Y_{2t}) \;\; \cdots \;\; H_q(\Delta Y_{qt}) \; ]'
\hspace{1cm}
H(\Delta Z_t)  = [ \; H_1(\Delta Z_{1t}) \;\; H_2(\Delta Z_{2t}) \;\; \cdots \;\; H_q(\Delta Z_{qt}) \; ]' \\
%
\Delta Y_t &= [ \; \Delta Y_{1t} \;\;  \Delta Y_{2t} \;\; \cdots \;\; \Delta Y_{qt} \;] '  \;\;\;\;\;\; \Delta Z_t = [ \; \Delta Z_{1t} \;\;  \Delta Z_{2t} \;\; \cdots \;\; \Delta Z_{qt} \;] '
\;\;\;\; \Delta u_t =  [ \; \Delta u_{1t} \;\;  \Delta u_{2t} \;\; \cdots \;\; \Delta u_{qt} \;] '
\end{align*}
Now that all necessary definitions have been made, the following lemma gives the sufficient conditions for the identification of parameter $\beta_1$,
\begin{lemma}
Letting $\phi_{t} = diag\big( \{\phi_{jt}\;\}_{j=1}^q \big)$, if
\begin{enumerate}[i.)]
\item $E\big[ f_{jd}(V_{jdt})\big] = E\big[ f_{jd}(V_{jd(t-1)}) \big]
$ for all $d\in \{1,2, \ldots , p_1\}$, $j\in \{1,2, \cdots , q\}$, and $t\in \{2, \ldots , T\}$
%
\item $E \Big( [\Delta Z_t - H(\Delta Z_t)]' [\Delta Z_t - H(\Delta Z_t)] \Big)$ is positive semi definite,
%
\end{enumerate}
Then $\beta_1$ is identified, in particular,
\begin{align*}
\beta_1 = E \Big( [\Delta Z_t - H(\Delta Z_t)]' \phi_{t} [\Delta Z_t - H(\Delta Z_t)] \Big)^{-1}E \Big( [\Delta Z_t - H(\Delta Z_t)]' \phi_{t} [\Delta Y_t - H(\Delta Y_t)] \Big)
\end{align*}
%
\begin{proof} I begin with a series of preliminary results, which combine to show the main result. First for $d \neq k$ consider,
\begin{align*}
E \big[ \phi_{jt} & \Delta f_{jk}(V_{jkt}) |V_{jdt},V_{jd(t-1)} \big]  = \int \phi_{jt} \Delta f_{jk}(V_{jkt}) p(V_{jt},V_{j(t-1)})p(V_{jdt},V_{jd(t-1)})^{-1}dV_{j-dt}V_{j-d(t-1)} \\
%
& = \int \Delta f_{jk}(V_{jkt}) p(V_{jkt},V_{jk(t-1)}) dV_{jkt} V_{jk(t-1)}\prod_{a \notin\{d,k\}}\int p(V_{jat},V_{ja(t-1)}) dV_{jat} V_{ja(t-1)} \\
& = E\big[ \Delta f_{jk}(V_{jkt}) \big] = E\big[ f_{jk}(V_{jkt}) \big]  - E\big[ f_{jk}(V_{jk(t-1)}) \big] = 0
\end{align*}
Next consider
\begin{align*}
E\big[\phi_{jt} | V_{jdt},V_{jd(t-1)}\big] &= \int \phi_{jt}p(V_{jt},V_{j(t-1)})p(V_{jdt},V_{jd(t-1)})^{-1}dV_{j-dt}V_{j-d(t-1)} \\
%
&= \prod_{a \neq d }\int p(V_{jat},V_{ja(t-1)}) dV_{jat} V_{ja(t-1)}  = 1
\end{align*}
Consequently,
\begin{align*}
H_{jd}(\Delta Y_{jt}) &= E\big[ \phi_{jt} \Delta Y_{jt} | V_{jdt},V_{jd(t-1)} \big] \\
%
& =E \big[ \phi_{jt} \Delta Z_{jt}' | V_{jdt},V_{jd(t-1)} \big]\beta_1 + \Delta f_{jd}(V_{jdt})E\big[\phi_{jt} | V_{jdt},V_{jd(t-1)}\big] \\
%
 &\hspace{4.3cm} + \sum_{k\neq d}^{p_1} E \big[ \phi_{jt} \Delta f_{jk}(V_{jkt}) | V_{jdt},V_{jd(t-1)} \big] + E\big[  \phi_{jt}\Delta u_{jt} | V_{jdt},V_{jd(t-1)} \big]  \\
 %
 & = H_{jd}(\Delta Z_{jt}')\beta_1 + \Delta f_{jd}(V_{jdt}) + E\big[  \phi_{jt} E\big(\Delta u_{jt} |\{Z_{j_k},W_{jk},V_{jk} \}_{k=1}^t \big)| V_{jdt},V_{jd(t-1)} \big] \\
 %
 &= H_{jd}(\Delta Z_{jt}')\beta_1 + \Delta f_{jd}(V_{jdt})
\end{align*}
Furthermore
\begin{align*}
H_j(\Delta Y_{jt} ) &=\sum_{d=1}^{p_1} H_{jd}(\Delta Y_{jt})
%
 = \sum_{d=1}^{p_1} H_{jd}(\Delta Z_{jt}')\beta_1 + \sum_{d=1}^{p_1} \Delta f_{jd}(V_{jdt})
%
= H_j(\Delta Z_{jt})\beta_1  + \sum_{d=1}^{p_1} \Delta f_{jl}(V_{jlt})
\end{align*}
Consequently
\begin{align*}
\sqrt{\phi_{jt}}[\Delta Y_{jt} - H_j(\Delta Y_{jt} ) ]   =  \sqrt{\phi_{jt}}\big[ \Delta Z_{jt} - H_j(\Delta Z_{jt}) \big]\beta_1 + \sqrt{\phi_{jt}}\Delta u_{jt}
\end{align*}
and stacking these equations into a vector,
\begin{align*}
\sqrt{\phi_{t}}[\Delta Y_{t} - H(\Delta Y_{t} )]    = \sqrt{\phi_t} \big[ \Delta Z_{t} - H(\Delta Z_{t}) \big]\beta_1 + \sqrt{\phi_t}\Delta u_{t}
\end{align*}
The result follows since $E\big[ \sqrt{\phi_t} E\big(\Delta u_{t}|\{Z_{j_k},W_{jk},V_{jk} \}_{k=1}^t\big) \big] = 0$
\end{proof}
\end{lemma}
\section*{Estimation}
\subsection*{Secondary Equations}
The specification of equation (2d) is quite general where; intercepts $\alpha_{0jd}$, and coefficients on exogenous regressors $\alpha_{1jd}$ are unique to each cross-section. Furthermore, although instruments $W_{jt}$ are shared across cross-sections their coefficients $\alpha_{2jd}$ are unique, and as yet there is no sense in which error terms $V_{jdt}$ are correlated across cross-section. As a result there are a number of restrictions on the regressors and parameters of equation (2d) which can be imposed and have a substantial effect on the manner in which $V_{jdt}$ will be estimated. \\

\noindent \bf Note: \rm For all vectors of dimension greater than two, I reference the individual elements of each vector by including a comma followed by a scalar value in the subscript. I reference the entire vector whenever the comma is omitted. For example,
\begin{align*}
W_{jt} = \begin{bmatrix} W_{jt,1} & W_{jt,2} & \cdots & W_{jt,w_j} \end{bmatrix}'
\end{align*}

%%%
\noindent \bf Case 1: \rm $W_{jt}$ is an known subset of $W_{t}$.   \

\noindent If so, estimation is comprised of q separate OLS regressions.
\begin{align*}
(\hat{\alpha}_{0jd}, \hat{\alpha}_{1jd},\hat{\alpha}_{2jd})
 = \arg \min \sum_{t=1}^T\left(Z_{1jdt} - \alpha_{0} -  Z_{2jt}'\alpha_{1} - W_{jt}'\alpha_{2} \right)^2
\end{align*}
where $\alpha_0 \in \mathbb{R}$, $\alpha_{1} \in \mathbb{R}^{p_2}$, and $\alpha_{3} \in \mathbb{R}^{w_j}$. So that
$$\hat{V}_{jdt} = Z_{1jdt} - \hat{\alpha}_{0jd} - Z_{2jt}'\hat{\alpha}_{1jd} - W_{jt}'\hat{\alpha}_{2jd}$$

%%%%
\noindent\bf Case 2: \rm $W_{jt}$ is an unknown subset of $W_{t}$. \

\noindent If so, estimation is comprised of $q$ separate regressions, each of which will incorporate a subset selection routine. Let $\alpha_{2jd} =  [\;\alpha_{2jd,1} \;\; \alpha_{2jd,2} \;\; \cdots \;\; \alpha_{2jd,w} \; ]$ where $\alpha_{2jd,l} = 0$ whenever $W_{t,l} \notin W_{jt}$. To facilitate subset selection I apply the lasso estimator by imposing an $\ell^1$ penalty on estimated coefficients $\hat{\alpha}_{2jd}$ .
%
\begin{align*}
(\hat{\alpha}_{0jd}, \hat{\alpha}_{1jd},\hat{\alpha}_{2jd})  = \arg \min \sum_{t=1}^T\left(Z_{1jdt} - \alpha_{0} -  Z_{2jt}'\alpha_{1} - W_{t}'\alpha_{2} \right)^2 \;\; \text{ subject to } \;\; \sum_{l = 1}^w |a_{2,l}| \leq \lambda
\end{align*}
where $\alpha_0 \in \mathbb{R}$, $\alpha_{1} \in \mathbb{R}^{p_2}$, and $\alpha_{2} \in \mathbb{R}^{w}$. So that again,
 $$\hat{V}_{jdt} = Z_{1jdt} -\hat{\alpha}_{0jd} - Z_{2jt}'\hat{\alpha}_{1jd} - W_{t}'\hat{\alpha}_{2jd}$$

%%%%
\noindent \bf Case 3: \rm $W_{jt}$ is an known subset of $W_{t}$, $\alpha_{1jd} =\alpha_{1j'd} \equiv \alpha_{1d}$ for all $j,j' \in \{1,2, \ldots,q\}$, and $\alpha_{2jd,l} = \alpha_{2j'd,l} \equiv \alpha_{2d,l}$ whenever $W_{t,l} \in W_{jt}$ and $W_{t,l} \in W_{j't}$ . Let
\begin{align*}
1[W_{t,l} \in W_{tj}] =
\begin{cases}
1 & \text{ if } W_{t,l} \in W_{tj} \\
0 & \text{otherwise}
\end{cases}
%
\end{align*}
%
and $M_j = \diag(\{1[ W_{tl} \in W_{jt}] \}_{l=1}^w)$ so that,
%
\begin{align*}
Z_{1jdt} &= \alpha_{0jd} + Z_{2jt}' \alpha_{1jd} + W_{jt}' \alpha_{2jd} + V_{jdt} \\
& =\alpha_{0jd} + Z_{2jt}' \alpha_{1d} + W_{t}'M_j \alpha_{2d} + V_{jdt}
\end{align*}
Now let $\Delta Z_{1jdt} = Z_{1jdt} - Z_{1jd(t-1)}$, $\Delta Z_{2jdt} = Z_{2jdt} - Z_{2jd(t-1)}$, $\Delta W_{t} = W_{t} - W_{t-1}$, and $\Delta V_{jdt} = V_{jdt} - V_{jd(t-1)}$ so that,
\begin{align*}
\Delta Z_{1jdt} =\Delta Z_{2jt}' \alpha_{1d} + \Delta W_{t}'M_j \alpha_{2d} + \Delta V_{jdt}
\end{align*}
%
As a result,
%
\begin{align*}
(\hat{\alpha}_{1d},\hat{\alpha}_{2d})  = \arg \min \sum_{j=1}^q\sum_{t=1}^T\left( \Delta Z_{ijt} -  \Delta Z_{2jt}'\alpha_{1} - \Delta W_{t}'M_j\alpha_{2} \right)^2
\end{align*}
%
\noindent so that given
\begin{align*}
\alpha_{0jd} =  E(V_{jdt} + \alpha_{0jd}) =
E( Z_{1jdt} - Z_{2jt}'\alpha_{1d} - W_{t}'M_j\alpha_{2d})
\end{align*}
we have
 $$\hat{V}_{jdt} = Z_{1jdt} - Z_{2jt}'\hat{\alpha}_{1d} - W_{t}'M_j\hat{\alpha}_{2d} - T^{-1}\sum_{t=1}^T  (Z_{1jdt} - Z_{2jt}'\hat{\alpha}_{1d} - W_{t}'M_j\hat{\alpha}_{2d}) $$.

\noindent \bf Case 4: \rm $W_{jt}$ is an unknown subset of $W_{t}$, $\alpha_{1jd} =\alpha_{1j'd} \equiv \alpha_{1d}$ for all $j,j' \in \{1,2, \ldots,q\}$, and $\alpha_{2jd,l} = \alpha_{2j'd,l} \equiv \alpha_{2d,l}$ whenever $W_{t,l} \in W_{jt}$ and $W_{t,l} \in W_{j't}$ . Inheriting notation from case 3 we again have,
%
\begin{align*}
\Delta Z_{1jdt} =\Delta Z_{2jt}' \alpha_{1d} + \Delta W_{t}'M_j \alpha_{2d} + \Delta V_{jdt}
\end{align*}
%
In order to introduce our selection procedure we will estimate the coefficients on $W_{t}$ as if that are not identical, then average the non zero estimates to construct a single estimate.  Consider,
\begin{align*}
(\hat{\alpha}_{1d},\hat{\alpha}_{2d})  = \arg \min \sum_{j=1}^q\sum_{t=2}^T\left( \Delta Z_{1jdt} -  \Delta Z_{2jt}'\alpha_{1} - \Delta W_{t}'\alpha_{2j} \right)^2 \;\; \text{ subject to } \;\; \sum_{l=1}^w|\alpha_{2j,l}| \leq \lambda \;\;  \text{ for all } 1 \leq j \leq q
\end{align*}
Consequently define for some $\varepsilon > 0$
\begin{align*}
\tilde{\alpha}_{2d,l} = \frac{\sum_{l=1}^q \hat{\alpha}_{2jd,l} 1[ \hat{\alpha}_{2jd,l} > \varepsilon ] }{ \sum_{l=1}^q 1[ \hat{\alpha}_{2jd,l} > \varepsilon] }
\end{align*}
Now let $\tilde{\alpha}_{2d} = [ \; \tilde{\alpha}_{2d,1} \;\; \tilde{\alpha}_{2d,2} \;\; \cdots \;\; \tilde{\alpha}_{2d,w}  \; ]'$ and $\tilde{M}_{jd} = \diag( \{ 1[\hat{\alpha}_{2jd,l} > \varepsilon ] \}_{l=1}^w)$ so that,
%
\begin{align*}
\hat{V}_{jdt} = Z_{1jdt} - Z_{2jt}'\hat{\alpha}_{1d} - W_{t}'\tilde{M}_{jd}\tilde{\alpha}_{2d} - T^{-1}\sum_{t=1}^T  (Z_{1jdt} - Z_{2jt}'\hat{\alpha}_{1d} - W_{t}'\tilde{M}_{jd}\tilde{\alpha}_{2d})
\end{align*}

\subsection*{Primary Equation}
%
Now having estimates of all error terms, I construct the following estimates of the primary equation coefficients in the following way. \\
%
\noindent \bf Density Estimation: \rm  \\
%
Obtain Rosenblatt Kernel Density Estimates of, $p(V_{jdt},V_{jdc})$, and $p(V_{jt},V_{jc})$ using $\{\hat{V}_{jt}, \hat{V}_{jc}\}_{t=2}^T$.
\begin{align*}
 \hat{p}(\hat{V}_{jdt,l},\hat{V}_{jd(t-1),l})
&= (nh_1h_2)^{-1}\sum_{ i = 2 }^T k_1[h_1^{-1}(\hat{V}_{jdi,l} -\hat{V}_{jdt,l})]k_2[h_2^{-1}(\hat{V}_{jdi,l} -\hat{V}_{jd(t-1),l})]  \\
%
 \hat{p}(\hat{V}_{jt,l},\hat{V}_{j(t-1),l})
&= (nh_1^{p_1}h_2^{p_1})^{-1}\sum_{ i = 2 }^T \prod_{d=1}^{p_1} k_1[h_1^{-1}(\hat{V}_{jdi,l} -\hat{V}_{jdt,l})]k_2[h_2^{-1}(\hat{V}_{jdi,l} -\hat{V}_{jd(t-1),l})]
\end{align*}
 Form estimated density ratio $\hat{\phi}_{jt}$ with densities estimated in previous step.
\begin{align*}
\hat{\phi}_{jt} = \frac{ \prod_{d=1}^{p_1}\hat{p}(\hat{V}_{jt,d},\hat{V}_{j(t-1),d}) }{\hat{p}(\hat{V}_{jt},\hat{V}_{j(t-1)})}
%
\hspace{1cm}
%
\hat{\theta}_{jt,d} = \frac{ \prod_{l \neq d}^{p_1}\hat{p}(\hat{V}_{jt,l},\hat{V}_{j(t-1),l}) }{\hat{p}(\hat{V}_{jt},\hat{V}_{j(t-1)})}
\end{align*}
%
\noindent \bf H Function Estimation:  \rm  \\
%
Now estimate the conditional expectation functions by a variation on a Nadaraya Watson estimation.
\begin{align*}
 \hat{H}_{j,d}(\Delta Z_{jt,a}) = [(T-1)b_1b_2]^{-1} \sum_{l \neq t , l > 1}^T k_1[b_1^{-1}(V_{jl,d} - V_{jt,d})] k_2[b_2^{-1}(V_{j(l-1),d} - V_{j(t-1),d})] \hat{\theta}_{jl,d} \Delta Z_{jl,a}
 \end{align*}
then construct
    \begin{align*}
    \hat{H}_j(\Delta Z_{jt}) &= \sum_{d=1}^{p_1} \hat{H}_{jd}(\Delta Z_{jt})  \hspace{1.5cm}
%
\hat{H}_j(\Delta Y_{jt}) = \sum_{d=1}^{p_1} \hat{H}_{jd}(\Delta Y_{jt})
    \end{align*}
where $ \Delta Z_{jt}  = [ \; \; \Delta Z_{jt,1}    \;\;    \Delta Z_{jt,2}     \;   \cdots \; \Delta Z_{jt,(p_1+p_2)} \;\; ]'$. Then construct the following vectors.
%
 \begin{align*}
   \hat{H}(\Delta Y_t)  &= [ \; \hat{H}_1(\Delta Y_{1t}) \;\; \hat{H}_2(\Delta Y_{2t}) \;\; \cdots \;\; \hat{H}_q(\Delta Y_{qt}) \; ]'\\
%
\hat{H}(\Delta Z_t)  &= [ \; \hat{H}_1(\Delta Z_{1t}) \;\; \hat{H}_2(\Delta Z_{2t}) \;\; \cdots \;\; \hat{H}_q(\Delta Z_{qt}) \; ]'\\
%
\Delta Z &= [ \; \Delta Z_{2}' \;\; \Delta Z_{3}' \;\; \cdots \;\; \Delta Z_{T}' \;]' \\
%
 \Delta Y &= [ \; \Delta Y_{2} \;\; \Delta Y_{3} \;\; \cdots \;\; \Delta Y_{T} \;]' \\
%
\hat{H}(\Delta Z) &= \big[\; \hat{H}(\Delta Z_{2})' \;\; \hat{H}(\Delta Z_{3})' \;\;\cdots \;\; \hat{H}(\Delta Z_{T})' \; \big] \\
%
 \hat{H}(\Delta Y) &= \big[\; \hat{H}(\Delta Y_{2}) \;\; \hat{H}(\Delta Y_{3}) \;\;\cdots \;\; \hat{H}(\Delta Y_{T}) \; \big]
\end{align*}
%
\noindent \bf $\beta_1$ Coefficient Estimation:\rm \\
%
 Let $\hat{\phi} = diag\big( \{\hat{\phi}_t\}_{c+1}^T \big)$ and calculate $\hat{\beta}_1$ as follows,
    \begin{align*}
  \hat{\beta}_1 = \Big( [\Delta Z - \hat{H}(\Delta Z)]' \hat{\phi} [\Delta Z - \hat{H}(\Delta Z)] \Big)^{-1} \Big( [\Delta Z - \hat{H}(\Delta Z)]' \hat{\phi} [\Delta Y - \hat{H}(\Delta Y)] \Big)
    \end{align*}
    
\section*{Monte Carlo Exercise}

Please see accompanying file \begin{verbatim} shi_monte_carlo.html \end{verbatim}
    






\end{document}
