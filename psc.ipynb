{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Preamble: Package Loading\n",
    "import numpy as np\n",
    "import ipywidgets as ipw\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "import itertools as iter\n",
    "import os\n",
    "import datetime as dt\n",
    "import json\n",
    "import re\n",
    "import regex\n",
    "# Preamble working directory retreival\n",
    "wkng_folder = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Parametric Panel, Selection, and Control </h1>\n",
    "<h3> Proposal Supplement </h3>\n",
    "By: Eric Penner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1 Model Setup </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.1 Base Model </h3>\n",
    "\n",
    "For each time period $t \\in \\{1,2, \\ldots, T\\}$, component $ d\\in \\{1,2, \\ldots , p_1\\}$ of $Z_{1jt}$, and cross-section $j \\in \\{1,2,\\ldots, q\\}$ where $\\{q,T\\} \\in \\mathbb{N}$ consider the following,\n",
    "\n",
    "\\begin{align} \n",
    "Y_{jt} &= \\beta_0 + [\\; Z_{1jt}' \\;\\; Z_{2jt}' \\;] \\beta_1 + e_j + \\varepsilon_{jt} \\\\[1em]\n",
    "%\n",
    "Z_{1jt,d} &= \\alpha_{0jd} + Z_{2jt}' \\alpha_{1jd} + W_{jt}' \\alpha_{2jd} + V_{jt,d} \\tag{2d} \\\\[1em]\n",
    "%\n",
    "E(&\\varepsilon_{jt} | Z_{1jt} , Z_{2jt}) = E(\\varepsilon_{jt} | Z_{1jt}) \\neq 0  \\\\[1em]\n",
    "%\n",
    "E(& V_{jdt} | Z_{1j(t-1)},\\ldots,Z_{1j(t-c)},Z_{2jt},W_{jt}) = 0 \n",
    "%\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where for $\\{p_1,p_2,c,w,w_1,w_2, \\ldots ,w_q \\} \\subset \\mathbb{N}$, \n",
    "\n",
    "* $Y_{jt}$, and $\\varepsilon_{jt}$ are scalar random variables, \n",
    "\n",
    "* $Z_{2jt}$ is a $p_2$ dimesion vector of exogenous random variables, \n",
    "\n",
    "* $Z_{1jt}$  is a vector of endogenous random variables having dimension $p_1$. \n",
    "\n",
    "* $e_j$ is a scalar fixed effect, \n",
    "\n",
    "* $\\varepsilon_{jt}$ is a scalar error term, \n",
    "\n",
    "* $V_{jt,d}$ is a scalar error term. \n",
    "\n",
    "* $W_t = [ \\; W_{1t} \\;\\; W_{2t} \\;\\; \\cdots \\;\\; W_{wt} \\;]'$ is vector of instrumental variables of dimension $w$, \n",
    "\n",
    "* $W_{jt} = [\\;W_{jt,1} \\;\\; W_{jt,2} \\;\\; \\cdots \\;\\; W_{jt,w_j} \\;]'$ is a vector of instrumental variables of dimension $w_j$ where $\\{W_{jt,l}\\}_{l=1}^{w_j} \\subset \\{W_{t,l}\\}_{l=1}^{w}$ and there exists at least one pair $j,j'  \\in \\{1,2, \\ldots , p\\}$, where $j \\neq j'$, such that $\\{W_{jt,l}\\}_{l=1}^{w_j} \\neq \\{W_{j't,l}\\}_{l=1}^{w_{j'}}$.\n",
    "\n",
    "* $\\beta_0$ and $\\alpha_{0jd}$  are scalars, \n",
    "\n",
    "* $\\beta_1$, $\\alpha_{1d}$, $\\alpha_{2d}$, are $p_1 +p_2 = p$, $p_2$, and $w_j$ dimensional vectors of real numbers respectively. \n",
    "\n",
    "* Lastly for notational convenience let define the following equivalences, \n",
    "\\begin{align*} \n",
    "Z_{jt} = [\\; Z_{1jt}' \\;\\; Z_{2jt}' \\;]'  \\;\\;\\; \\text{ and } \\;\\;\\; V_{jt} = [ \\; V_{jt,1} \\; V_{jt,2} \\; \\cdots \\;V_{jt,p_1}\\;]'\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.2 Differenced Model with Control Functions </h3>\n",
    "\n",
    "As shown in the the main proposal I treat the presence of endogeneity with the control function approach and then first difference the primary equation to eliminate fixed effects, resulting in the following. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align} \n",
    "\\Delta Y_{jt} &= \\Delta Z_{jt}'\\beta_1 + \\sum_{d=1}^{p_1} \\Delta f_{jd}(V_{jt,d}) + \\Delta u_{jt} \\\\[1em]\n",
    "%\n",
    "Z_{1jt,d} &= \\alpha_{0jd} + Z_{2jt}' \\alpha_{1jd} + W_{jt}' \\alpha_{2jd} + V_{jt,d} \\tag{2d}\\\\[1em]\n",
    "E(&\\Delta u_{jt} | \\{Z_{j_k},W_{jk},X_{jk},V_{jk} \\}_{k=1}^t) = 0 \\\\[1em]\n",
    "%\n",
    "E(& V_{jt} | Z_{1jt-1},\\ldots,Z_{1jt-c_1},Z_{2jt},W_{jt}) = 0 \n",
    "%\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>  1.3 Identification of $\\beta_1$ </h3>\n",
    "\n",
    "I use a variation on the identification procedure in Manzan and Zerom to identify $\\beta_1$, this required the construction of the following density ratios, functions, and vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Density Ratio  \n",
    "\\begin{align*} \n",
    "\\phi_{jt} = \\frac{ \\prod_{d=1}^{p_1}p(V_{jt,d},V_{jc,d}) }{p(V_{jt},V_{jc})} \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*} \n",
    "H_{jd}(\\Delta Z_{jt}) &= E[\\phi_{jt} \\Delta Z_{jt} |V_{jt,d},V_{jc,d}] \\hspace{1.25cm} \n",
    "%\n",
    "H_{jd}(\\Delta Y_{jt}) = E[\\phi_{jt} \\Delta Y_{jt} |V_{jt,d},V_{jc,d}] \\\\\n",
    "%\n",
    "H_j(\\Delta Z_{jt}) &= \\sum_{d=1}^{p_1} H_{jd}(\\Delta Z_{jt})  \\hspace{2.5cm}\n",
    "%\n",
    "H_j(\\Delta Y_{jt}) = \\sum_{d=1}^{p_1} H_{jd}(\\Delta Y_{jt})  \\end{align*} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors\n",
    "\n",
    "\\begin{align*}\n",
    "H(\\Delta Y_t)  &= [ \\; H_1(\\Delta Y_{1t}) \\;\\; H_2(\\Delta Y_{2t}) \\;\\; \\cdots \\;\\; H_q(\\Delta Y_{qt}) \\; ]' \\\\[1em]\n",
    "H(\\Delta Z_t)  &= [ \\; H_1(\\Delta Z_{1t}) \\;\\; H_2(\\Delta Z_{2t}) \\;\\; \\cdots \\;\\; H_q(\\Delta Z_{qt}) \\; ]' \\\\[1em]\n",
    "%\n",
    "\\Delta Y_t &= [ \\; \\Delta Y_{1t} \\;\\;  \\Delta Y_{2t} \\;\\; \\cdots \\;\\; \\Delta Y_{qt} \\;] '  \\\\[1em]\n",
    "%\n",
    "\\Delta Z_t &= [ \\; \\Delta Z_{1t} \\;\\;  \\Delta Z_{2t} \\;\\; \\cdots \\;\\; \\Delta Z_{qt} \\;] ' \n",
    "\\\\[1em] \n",
    "\\Delta u_t &=  [ \\; \\Delta u_{1t} \\;\\;  \\Delta u_{2t} \\;\\; \\cdots \\;\\; \\Delta u_{qt} \\;] ' \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 1.3.1 Lemma 1 </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letting $\\phi_{t} = diag\\big( \\{\\phi_{jt}\\;\\}_{j=1}^q \\big)$, if for all $t\\in \\{2, \\ldots , T\\}$\n",
    "\n",
    "1. $E\\big[ f_{jd}(V_{jdt})\\big] = E\\big[ f_{jd}(V_{jdc}) \\big] \n",
    "$ for all $d\\in \\{1,2, \\ldots , p_1\\}$, $j\\in \\{1,2, \\cdots , q\\}$, and \n",
    "\n",
    "2. $E \\Big( [\\Delta Z_t - H(\\Delta Z_t)]' [\\Delta Z_t - H(\\Delta Z_t)] \\Big)$ is positive semi definite,\n",
    "\n",
    "Then $\\beta_1$ is identified, in particular,\n",
    "\n",
    "\\begin{align*} \n",
    "\\beta_1 = E \\Big( [\\Delta Z_t - H(\\Delta Z_t)]' \\phi_{t} [\\Delta Z_t - H(\\Delta Z_t)] \\Big)^{-1}E \\Big( [\\Delta Z_t - H(\\Delta Z_t)]' \\phi_{t} [\\Delta Y_t - H(\\Delta Y_t)] \\Big)\n",
    "\\end{align*} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2.0 Post Secondary Equation Estimation </h2>\n",
    "\n",
    "Here we will discuss the estimation of $\\beta_1$ assuming that estimates $\\hat{V}_{jt,d}$ have already been generated by some as yet unspecified procedure. Estimation of the secondary equation will be discussed in the following sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2.1  Density Estimation </h3>\n",
    "\n",
    "The following function(s) implement a rosenblatt kernel estimator where \n",
    "\n",
    "$$ \\hat{p}(\\hat{V}_{dt,l},\\hat{V}_{d(t-1),l}) \n",
    "= (nh_1h_2)^{-1}\\sum_{ i = 2 }^T k_1[h_1^{-1}(\\hat{V}_{di,l} -\\hat{V}_{dt,l})]k_2[h_2^{-1}(\\hat{V}_{di,l} -\\hat{V}_{d(t-1),l})]  \n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>  2.1.1 Density Estimation: Function to Calculate Kernel Values </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mvkernel(x,kernel):  \n",
    "    \"\"\" \n",
    "PURPOSE: Calculate product kernel values at points x\n",
    "    \n",
    "INPUTS\n",
    "x       points of kernel evaluation (3)x(n)x(dim of x)\n",
    "kernel  Scalar value indicating which kernel to use where\n",
    "           1 = Rectangular\n",
    "           2 = Triangular\n",
    "           3 = Biweight\n",
    "           4 = Silverman's \n",
    "           5 = Epanechnikov order 2\n",
    "           6 = Epanechnikov order 2 alt\n",
    "           7 = Epanechnikov order 4\n",
    "           8 = Epanechnikov order 6\n",
    "           9 = Gaussian order 2\n",
    "           10 = Gaussian order 4\n",
    "           11 = Gaussian order 6\n",
    "\n",
    "OUTPUTS\n",
    "ker     product of kernel values along axis = 0\n",
    "    \"\"\"\n",
    "    \n",
    "    if kernel == 1:\n",
    "        # Rectangular Kernel  \n",
    "        ker = 1/2*(np.absolute(x)<1)\n",
    "    elif kernel == 2:\n",
    "        # Triangular Kernel\n",
    "        ker = (1-np.absolute(x))*(np.absolute(x)<1)\n",
    "    elif kernel == 3:\n",
    "        # Biweight Kernel\n",
    "        ker = (15/16*(1-x**2)**2)*(np.absolute(x)< 1)\n",
    "    elif kernel == 4:\n",
    "        # Silvermans Kernel \n",
    "        ker = 0.5*np.exp(-np.absolute(x)/np.sqrt(2))*np.sin(np.absolute(x)/np.sqrt(2) + np.pi/4)   \n",
    "    elif kernel == 5:\n",
    "        # Epanechnikov order 2\n",
    "        ker = 0.75*(1-x**2)*(np.absolute(x)<=1)\n",
    "    elif kernel == 6:\n",
    "        # Epanechnikov order 2 (alt)?\n",
    "        ker = (0.75/np.sqrt(5))*(1-0.2*x**2)*(np.absolute(x)<=np.sqrt(5))\n",
    "    elif kernel == 7:\n",
    "        # Epanechnikov order 4\n",
    "        ker  = (0.75/np.sqrt(5))*(15/8-7/8*x**2)*(1-0.2*x**2)*(np.absolute(x)<=np.sqrt(5))\n",
    "    elif kernel == 8:\n",
    "        # Epanechnikov order 6\n",
    "        ker  = ((0.75/np.sqrt(5))*(175/64-105/32*x**2+ 231/320*x**4)\n",
    "                                 *(1-0.2*x**2)*(np.absolute(x)<=np.sqrt(5)))\n",
    "    elif kernel == 9:\n",
    "        # Gaussian order 2\n",
    "        ker  = (1/np.sqrt(2*np.pi))*np.exp(-0.5*x**2)\n",
    "    elif kernel == 10:\n",
    "        # Gaussian order 4\n",
    "        ker  = (3/2 -1/2*x**2)*(1/np.sqrt(2*np.pi))*np.exp(-0.5*x**2)\n",
    "    elif kernel == 11:\n",
    "        # Gaussian order 6\n",
    "        ker  = (15/8-5/4*x**2+1/8*x**4)*(1/np.sqrt(2*np.pi))*np.exp(-0.5*x**2)\n",
    "    else:\n",
    "        print('Incorrect Kernel Number')\n",
    "\n",
    "    if x.ndim > 2:\n",
    "        # Value of each product kernel\n",
    "        ker = np.prod(ker,axis = 0)\n",
    "\n",
    "    return ker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 2.1.2 Density Estimation:  Function to Calculate Estimated Density</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mvden(x,p,h,kernel): \n",
    "    \"\"\"\n",
    "Purpose: Multivariate Rosenblatt Kernel Density Estimator at points p based on sample x    \n",
    "    \n",
    "INPUTS\n",
    "x       np.array of data having order (number of observations)x(dimension of X)\n",
    "P       np.array of points of evaluation having order (number of points)x(dimension of X)\n",
    "h       Bandwidth vector of order (1)x(dimension of X)\n",
    "kernel  Scalar value indicating which kernel to use where\n",
    "           1 = Rectangular\n",
    "           2 = Triangular\n",
    "           3 = Biweight\n",
    "           4 = Silverman's \n",
    "           5 = Epanechnikov order 2\n",
    "           6 = Epanechnikov order 2 alt\n",
    "           7 = Epanechnikov order 4\n",
    "           8 = Epanechnikov order 6\n",
    "           9 = Gaussian order 2\n",
    "           10 = Gaussian order 4\n",
    "           11 = Gaussian order 6\n",
    "\n",
    "OUTPUTS\n",
    "den      Density at each point of evaluation (number of observation)x1\n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.array(x)\n",
    "    p = np.array(p)\n",
    "    h = np.array(h)\n",
    "    \n",
    "    if x.ndim == 1:\n",
    "            m0 = (x.reshape(1,x.shape[0])-p.reshape(p.shape[0],1))/h\n",
    "    elif x.ndim == 2:\n",
    "        m0 = np.zeros((x.shape[1],p.shape[0],x.shape[0])) \n",
    "        for i in range(0,x.shape[1]):\n",
    "            m0[i,:,:] = (x[:,i].reshape(1,x.shape[0])-p[:,i].reshape(p.shape[0],1))/h[i]\n",
    "\n",
    "    ker = mvkernel(m0,kernel)/(x.shape[0]*np.prod(h))\n",
    "    den = np.dot(ker,np.ones(x.shape[0]))\n",
    "\n",
    "    return den"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 2.1.3  Density Estimator Demonstration: Setup </h4>\n",
    "\n",
    "In order to validate the density function code in a setting similar to ours I will perform the following,\n",
    "\n",
    "$$ \\hat{p}(X_{t},X_{(t-1)}) \n",
    "= [(T-1)h_1h_2]^{-1}\\sum_{l=2}^Tk_1[h_1^{-1}(X_{l} -X_{t})]k_2[h_2^{-1}(X_{l} - X_{(t-1)})]  \\;\\; \\text{ where } \\;\\; X_t \\sim N(0,1)\n",
    "$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 2.1.4  Density Estimator Demonstration: Data Generation </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of data points\n",
    "n = 100\n",
    "# mean of x\n",
    "mu = np.array([0])\n",
    "# variance of x\n",
    "var = np.array([1])\n",
    "# generation of x\n",
    "x = np.random.normal(mu,var,n).reshape(n,1)\n",
    "# generation of df of x\n",
    "x = pd.DataFrame(x,columns = ['x1'])\n",
    "# adding a backshifted column to x\n",
    "x['Bx1'] = x.x1.shift(1)\n",
    "# removing the NA row\n",
    "x = x[1:]\n",
    "# Points of Evaluation Generation and Stacking\n",
    "npts = 20\n",
    "# Smallest coordinate value\n",
    "pl = -2\n",
    "# Largest coordinate value\n",
    "pu =  2\n",
    "p = np.linspace(pl,pu,npts).reshape((20,1))\n",
    "p = np.hstack((p,pl*np.ones((20,1))))\n",
    "for j in np.arange(1,20,1): \n",
    "    pt = np.hstack((p[0:20,0].reshape((20,1)),p[j,0]*np.ones((20,1))))\n",
    "    p = np.vstack((p,pt))\n",
    "p = pd.DataFrame(p, columns =['p1','p2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 2.1.3  Density Estimator Demonstration: Plotting Function </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def  plot_den(x,p,c_h,ker,el,az):\n",
    "    # Standard Normal pdf\n",
    "    fnrm = lambda x: np.exp(-x**2/2)/np.sqrt(2*np.pi)\n",
    "    # Plug in Bandwidths\n",
    "    h = c_h*x.shape[0]**(-1/5)*x.std(0)\n",
    "    # Collection into an array\n",
    "    h = np.array([h , h])\n",
    "    # Density estimation functions call\n",
    "    p['xden'] = mvden(np.array(x),np.array(p),h[0],ker)\n",
    "    plt.close('all')\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(2,1,subplot_kw = {'projection':'3d'})\n",
    "    fig.set_figheight(15)\n",
    "    fig.set_figwidth(15)\n",
    "    #fig.suptitle(\"Density\",fontsize = 20)\n",
    "    fig.tight_layout( pad = 0, h_pad = 0 , w_pad = 0)\n",
    "    ax[0].plot_trisurf(p.p1,p.p2,p.xden,alpha = 1)\n",
    "    ax[0].set_title('Estimated Density', fontsize = 24)\n",
    "    #ax[0].grid(b=None)\n",
    "    ax[1].plot_trisurf(p.p1,p.p2,fnrm(p.p1)*fnrm(p.p2),alpha = 1, color = 'g')\n",
    "    ax[1].set_title('True Density',fontsize = 24)\n",
    "    #ax[1].grid(b=None)\n",
    "    ax[0].view_init(elev=el, azim=az)\n",
    "    ax[1].view_init(elev=el, azim=az)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 2.1.4  Density Estimator Demonstration: Interactive Plotting </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "857be57a53df4429aa981a35291f653b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Output(), IntSlider(value=30, description='Elevation', layout=Layout(align_items…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "box_hlayout = ipw.Layout(display='flex',\n",
    "                    flex_flow='row',\n",
    "                    align_items='stretch',\n",
    "                    width='90%')\n",
    "\n",
    "box_vlayout = ipw.Layout(display='flex', flex_flow='column', align_items='stretch',\n",
    "                    width='10%', height = 'auto', justify_content='space-between')\n",
    "\n",
    "s_az = ipw.IntSlider(min = 0 , max = 360, value = 45, step = 15, description = 'Azimuth',width = 'auto'\n",
    "              ,layout = box_hlayout, style = {'description_width': 'initial'} )\n",
    "s_c = ipw.FloatSlider(min = 0.1 , max = 4, value = 2.5, step =0.2, description = 'Bandwidth Constant'\n",
    "              ,width = 'auto',layout = box_hlayout, style = {'description_width': 'initial'})\n",
    "s_ker = ipw.IntSlider(min = 1 , max = 11, value = 6, description = 'Kernel',width = 'auto'\n",
    "              ,layout = box_hlayout, style = {'description_width': 'initial'})\n",
    "s_el = ipw.IntSlider(min = -90 , max = 90, value = 30, step = 15, description = 'Elevation'\n",
    "              ,orientation = 'vertical',length = 'auto',layout = box_vlayout\n",
    "              ,style = {'description_length': 'initial'},readout = False)\n",
    "\n",
    "out = ipw.interactive_output(plot_den,{'x': ipw.fixed(x),'p': ipw.fixed(p),'c_h':s_c, \n",
    "                                       'ker': s_ker,'az':s_az ,'el': s_el})\n",
    "ipw.VBox([ipw.HBox([out,s_el])   \n",
    "          ,ipw.VBox([s_az,s_c,s_ker])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2.2 Density Ratio Construction </h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will construct the following ratios directly within the main function code presented below\n",
    "\n",
    "\\begin{align*} \n",
    "\\hat{\\phi}_{jt} = \\frac{ \\prod_{d=1}^{p_1}\\hat{p}(\\hat{V}_{jt,d},\\hat{V}_{j(t-1),d}) }{\\hat{p}(\\hat{V}_{jt},\\hat{V}_{j(t-1)})}\n",
    "%\n",
    "\\hspace{1cm}\n",
    "%\n",
    "\\hat{\\theta}_{jt,d} = \\frac{ \\prod_{l \\neq d}^{p_1}\\hat{p}(\\hat{V}_{jt,l},\\hat{V}_{j(t-1),l}) }{\\hat{p}(\\hat{V}_{jt},\\hat{V}_{j(t-1)})}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2.3 H Function Estimation:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function implement a nadaraya watson estimator of the H functions \n",
    "$$ \\hat{H}_{j,d}(\\Delta Z_{jt,a}) = [(T-1)b_1b_2]^{-1} \\sum_{l \\neq t , l > 1}^T k_1[b_1^{-1}(V_{jl,d} - V_{jt,d})] k_2[b_2^{-1}(V_{j(l-1),d} - V_{j(t-1),d})] \\hat{\\theta}_{jl,d} \\Delta Z_{jl,a} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 2.3.1 H Function Estimation: Function Code </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Nw_H_pan(regrsnd,theta,regressr,p,h,kernel,l10 = 1):\n",
    "    \"\"\"\n",
    "INPUTS \n",
    "regrsnd    Regressand of the regression (Delta Y or Delta Z) (# of obs - 1) x 1\n",
    "theta      Density ratio which will multiply regressand (# of obs - 1) x 1\n",
    "regressr   Vector of conditioning variables (V_{j,d}) (# of obsn -1 ) x 1\n",
    "p          points of evaluation (npoints x 2) \n",
    "b          Vector of Bandwidths [b1, b2] \n",
    "l10        Leave one out indicator; 1 leave one out, 0 dont\n",
    "\n",
    "OUTPUTS\n",
    "H       Vector of H function estimates corresponding to arguments p\n",
    "    \"\"\"\n",
    "    # Converting regressr input into np.array\n",
    "    reg = np.array(regressr)\n",
    "    # Setting up full regressor array where for a = 1,2,...,T-1, where \n",
    "    #           reg[a,:] =  [ V_{j(a+1),d} ,  V_{j(a),d} ]\n",
    "    #reg = np.vstack((reg[:-1],reg[1:])).T\n",
    "    # Converting regrsnd input to np.array\n",
    "    y = np.array(regrsnd)\n",
    "    # Converting theta input to np.array\n",
    "    tht = np.array(theta)\n",
    "    # Points of evaluation\n",
    "    p = np.array(p)\n",
    "    # Converting bandwidth input to np.array\n",
    "    b = np.array(h)\n",
    "    # Product of regressands and theta ratios\n",
    "    y_tht = y*tht\n",
    "\n",
    "    # Construction of each kernel argument array by broadcasting\n",
    "    m1 = (reg[:,0].reshape(1,reg.shape[0])-p[:,0].reshape(p.shape[0],1))/b[0]\n",
    "    m2 = (reg[:,1].reshape(1,reg.shape[0])-p[:,1].reshape(p.shape[0],1))/b[1]\n",
    "    # Initializing the kernel argument array \n",
    "    m0 = np.zeros((2,p.shape[0],reg.shape[0]))\n",
    "    # Placing broadcasted arrays in m0\n",
    "    m0[0,:,:] = m1\n",
    "    m0[1,:,:] = m2\n",
    "\n",
    "    # Calculating kernel values\n",
    "    ker = mvkernel(m0,kernel)\n",
    "\n",
    "    # Matrix of theta ratios so that observations match the first argument of kernels in H1\n",
    "    H0 = np.tile(y_tht,(p.shape[0],1))\n",
    "    # Multiply these two together\n",
    "    H1 = ker*H0;\n",
    "    # Deleting the diagonal which converts to a leave one out style\n",
    "    if l10 == 1:\n",
    "        H2 = H1 - H1*np.eye(H1.shape[0]);\n",
    "    else: \n",
    "        H2 = H1\n",
    "    # Finishing the calculation at each point in X\n",
    "    H = 1./((reg.shape[0]-1)*b[0]*b[1])*np.dot(H2,np.ones((H2.shape[1],1)))\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>  2.3.2  H Function Estimation Demonstration: Setup </h4>\n",
    "\n",
    "Here we generate data as follows\n",
    "\n",
    "$$\n",
    "Y_t = f_1(a_1,b_1,X_{t-1}) + f_2(a_2,b_2,X_{t}) + \\varepsilon_t \\;\\;\\; \\text{ where } \\;\\;\\; X_t,\\varepsilon \\sim N(0,1) \n",
    "$$\n",
    "\n",
    "where $f_1$ and $f_2$ are defined in the following code. Its difficult to validate the function code with an example exactly like the dgp we are interested in so instead I demonstrate the following estimation. \n",
    "\n",
    "$$\n",
    "\\hat{E}[Y_t|X_t,X_{t-1}] = [(T-1)b_1b_2]^{-1} \\sum_{l \\neq t , l > 1}^T k_1[b_1^{-1}(X_{l} - X_{t})] k_2[b_2^{-1}(X_{(l-1)} - X_{(t-1)})] \\hat{\\theta}_{l} Y_l \\;\\;\\; \\text{ where } \\;\\;\\; \\hat{\\theta}_{l} = \\frac{1}{\\hat{p}(X_{l},X_{(l-1)})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 2.3.3  H Function Estimation Demonstration: Data Generation </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Regression Function Definition\n",
    "f1 = lambda a,b,x : a*np.cos(b*x)\n",
    "f2 = lambda a,b,x : a*np.sin(b*x)\n",
    "#f1 = lambda a,b,x: a*x**b\n",
    "#f2 = lambda a,b,x: a*x**(b)\n",
    "\n",
    "## Regressor and Error Generation\n",
    "# Number of observations\n",
    "n = 500\n",
    "# Vector of means\n",
    "mu = np.array([0,0])\n",
    "# Variance Covariance Matrix\n",
    "var = np.array([[1,0],[0,1]])\n",
    "# Data generation\n",
    "x = np.random.multivariate_normal(mu,var,n)\n",
    "# Coverting to pandas dataframe\n",
    "x = pd.DataFrame(x,columns = ['x1','ep'])\n",
    "# Backshifted regressor matrix\n",
    "reg = pd.concat([x.x1,x.x1.shift(1)],axis = 1).iloc[1:,:]\n",
    "## Regressand Generation\n",
    "# Parameters for conditional expectation functions\n",
    "a = [ 3 , 1 ]\n",
    "b = [ 2 , 3 ] \n",
    "# Conditional expectation functions at data points\n",
    "x2 = pd.DataFrame(f1(a[0],b[0],np.array(reg.iloc[:,0]))\n",
    "                  +f2(a[1],b[1],np.array(reg.iloc[:,1])),\n",
    "                  columns = ['m'])\n",
    "# Regressand Generation\n",
    "x2['y'] = x2.m + np.array(x.ep[1:])\n",
    "\n",
    "## Points of Evaluation Generation\n",
    "# Smallest coordinate value\n",
    "pl = -2\n",
    "# Largest coordinate value\n",
    "pu =  2\n",
    "# Grid of points will be npts X npts\n",
    "npts = 20\n",
    "# Points generation and Stacking\n",
    "p = np.linspace(pl,pu,npts).reshape((20,1))\n",
    "p = np.hstack((p,pl*np.ones((20,1))))\n",
    "for j in np.arange(1,20,1): \n",
    "    pt = np.hstack((p[0:20,0].reshape((20,1)),p[j,0]*np.ones((20,1))))\n",
    "    p = np.vstack((p,pt))\n",
    "p = pd.DataFrame(p, columns =['p1','p2'])\n",
    "\n",
    "## Constructing the Theta Ratio: 1/p(x_1t,x_1(t-1))\n",
    "# Plug-in Bandwidth\n",
    "h = 1.45*reg.shape[0]**(-1/5)*np.array(reg).std(0)\n",
    "# Density Estimate Function Call\n",
    "den = mvden(reg,reg,h,10)\n",
    "# Theta Ratio\n",
    "tht1 = 1/den"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 2.3.4  H Function Estimation Demonstration: Plotting Function </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Nw_H_plot(c_h,y,tht1,reg,p,ker,a,b,el,az):\n",
    "    plt.close('all')\n",
    "    h = c_h*reg.shape[0]**(-1/5)*np.array(reg).std(0)\n",
    "    ## Function Call\n",
    "    H = Nw_H_pan(y,tht1,reg,p,h,ker,0)\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(2,1,subplot_kw = {'projection':'3d'})\n",
    "    fig.set_figheight(15)\n",
    "    fig.set_figwidth(15)\n",
    "    ax[0].plot_trisurf(p.p1,p.p2,H[:,0])\n",
    "    ax[0].set_title('Estimated Function')\n",
    "    ax[1].plot_trisurf(p.p1,p.p2,f1(a[0],b[0],p.p1)+f2(a[1],b[1],p.p2),color = 'g')\n",
    "    ax[1].set_title('True Function')\n",
    "    ax[0].view_init(elev=el, azim=az)\n",
    "    ax[1].view_init(elev=el, azim=az)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 2.3.5 H Function Estimator Demonstration: Interactive Widgets Setup </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e8ea0f99e854e169939d7c38a8fa3cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Output(), IntSlider(value=30, description='Elevation', layout=Layout(align_items…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "box_hlayout = ipw.Layout(display='flex',\n",
    "                    flex_flow='row',\n",
    "                    align_items='stretch',\n",
    "                    width='90%')\n",
    "\n",
    "box_vlayout = ipw.Layout(display='flex', flex_flow='column', align_items='stretch',\n",
    "                    width='10%', height = 'auto', justify_content='space-between')\n",
    "\n",
    "s_az = ipw.IntSlider(min = 0 , max = 360, value = 45, step = 15, description = 'Azimuth'\n",
    "                     ,width = 'auto',layout = box_hlayout\n",
    "                     ,style = {'description_width': 'initial'} )\n",
    "s_c = ipw.FloatSlider(min = 0.1 , max = 4, value = 2.5, step =0.2\n",
    "                      ,description = 'Bandwidth Constant'\n",
    "                      ,width = 'auto',layout = box_hlayout\n",
    "                      ,style = {'description_width': 'initial'})\n",
    "s_ker = ipw.IntSlider(min = 1 , max = 11, value = 6\n",
    "                      ,description = 'Kernel',width = 'auto'\n",
    "                      ,layout = box_hlayout\n",
    "                      ,style = {'description_width': 'initial'})\n",
    "s_el = ipw.IntSlider(min = -90 , max = 90, value = 30, step = 15\n",
    "                     ,description = 'Elevation'\n",
    "                     ,orientation = 'vertical',length = 'auto'\n",
    "                     ,layout = box_vlayout\n",
    "                     ,style = {'description_length': 'initial'},readout = False)\n",
    "\n",
    "out = ipw.interactive_output(Nw_H_plot,{'c_h':s_c,'y': ipw.fixed(x2['y'])\n",
    "                                        ,'tht1': ipw.fixed(tht1) ,'reg': ipw.fixed(reg)\n",
    "                                        ,'p': ipw.fixed(p),'ker': s_ker,'a':ipw.fixed(a)\n",
    "                                        ,'b': ipw.fixed(b) ,'az':s_az ,'el': s_el})\n",
    "ipw.VBox([ipw.HBox([out,s_el])   \n",
    "          ,ipw.VBox([s_az,s_c,s_ker])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3 Secondary Equation Estimation </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The specification of equation (2d) is quite general where; intercepts $\\alpha_{0jd}$, and coefficients on exogenous regressors $\\alpha_{1jd}$ are unique to each cross-section. Furthermore, although instruments $W_{jt}$ are shared across cross-sections their coefficients $\\alpha_{2jd}$ are unique, and as yet there is no sense in which error terms $V_{jdt}$ are correlated across cross-section. As a result there are a number of restrictions on the regressors and parameters of equation (2d) which can be imposed and have a substantial effect on the manner in which $V_{jdt}$ will be estimated. \n",
    "\n",
    "** Note: **  For all vectors of dimension greater than two, I reference the individual elements of each vector by including a comma followed by a scalar value in the subscript. I reference the entire vector whenever the comma is omitted. For example,\n",
    "\n",
    "\\begin{align*} \n",
    "W_{jt} = \\begin{bmatrix} W_{jt,1} & W_{jt,2} & \\cdots & W_{jt,w_j} \\end{bmatrix}'\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3.1: Case 1 </h3> \n",
    "\n",
    "* Cross-sectional data is not panel, meaning that in this case there is no assumption restricting crossections to have common coefficients in the secondary equations.\n",
    "\n",
    "* $W_{jt}$ is an **known** subset of $W_{t}$.\n",
    "\n",
    "If so, estimation is comprised of q separate OLS regressions.\n",
    "\\begin{align*} \n",
    "(\\hat{\\alpha}_{0jd}, \\hat{\\alpha}_{1jd},\\hat{\\alpha}_{2jd}) \n",
    " = \\arg \\min \\sum_{t=1}^T\\left(Z_{1jdt} - \\alpha_{0} -  Z_{2jt}'\\alpha_{1} - W_{jt}'\\alpha_{2} \\right)^2\n",
    "\\end{align*}\n",
    "where $\\alpha_0 \\in \\mathbb{R}$, $\\alpha_{1} \\in \\mathbb{R}^{p_2}$, and $\\alpha_{2} \\in \\mathbb{R}^{w_j}$. So that \n",
    "$$\\hat{V}_{jdt} = Z_{1jdt} - \\hat{\\alpha}_{0jd} - Z_{2jt}'\\hat{\\alpha}_{1jd} - W_{jt}'\\hat{\\alpha}_{2jd}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 3.1.1: Case 1 Estimation </h4>\n",
    "\n",
    "Below is a function which implements simple ols regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ols(df,inpt):\n",
    "    \"\"\"\n",
    "INPUTS\n",
    "df                (pandas df) Data Frame with all regressors\n",
    "inpt              (dict) Dictionary with the following\n",
    "  inpt['dep']     (string) Name of dependent variable contained in df\n",
    "  inpt['reg']     (list of strings) names of regressors in df \n",
    "  inpt['cons']    (0,1) Indicator for whether a constant should be included\n",
    "\n",
    "OUTPUTS \n",
    "out               (list of lists) List of the following\n",
    "  out[0]          (list) Estimated coefficients\n",
    "  out[1]          (list) Residuals\n",
    "  out[2]          (list) Estimated conditional expectation\n",
    "    \"\"\"\n",
    "    # Extracting input variables\n",
    "    dep = inpt['dep']\n",
    "    reg = inpt['reg']\n",
    "    cons = inpt['cons']\n",
    "    # Determining length of df (number of obs)\n",
    "    n = df.shape[0]\n",
    "    # Extracting Dependent Variable from df\n",
    "    Y = df.loc[:,dep].values.reshape(n,1)\n",
    "    # Extracting Regressors from df\n",
    "    if len(reg) == 1:\n",
    "        X = df.loc[:,reg].values.reshape(n,1)\n",
    "    elif len(reg) > 1: \n",
    "        X = df.loc[:,reg].values\n",
    "    # Adding column of ones if a constant is included\n",
    "    if cons == 1: \n",
    "        X = np.hstack((np.ones((n,1)),X))\n",
    "    # Estimated regression coefficients\n",
    "    alpha = np.linalg.inv(X.T.dot(X)).dot(X.T.dot(Y))\n",
    "    # Estimated Conditional Expectation\n",
    "    Yhat = X.dot(alpha)\n",
    "    # Residuals of the regression\n",
    "    res = Y - X.dot(alpha)\n",
    "    # Constructing output list of lists\n",
    "    out = [alpha.T.tolist()[0],res.T.tolist()[0],Yhat.T.tolist()[0]]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3.2 Case 2: </h3>\n",
    "\n",
    "* Cross-sectional data is not panel, meaning that in this case there is no assumption restricting crossections to have common coefficients in the secondary equations.\n",
    "\n",
    "* $W_{jt}$ is an ** unknown ** subset of $W_{t}$.\n",
    "\n",
    "If so, estimation is comprised of $q$ separate regressions, each of which will incorporate a subset selection routine. Let $\\alpha_{2jd} =  [\\;\\alpha_{2jd,1} \\;\\; \\alpha_{2jd,2} \\;\\; \\cdots \\;\\; \\alpha_{2jd,w} \\; ]$ where $\\alpha_{2jd,l} = 0$ whenever $W_{t,l} \\notin W_{jt}$. To facilitate subset selection I apply the lasso estimator by imposing an $\\ell^1$ penalty on estimated coefficients $\\hat{\\alpha}_{2jd}$ .\n",
    "\n",
    "\\begin{align*} \n",
    "(\\hat{\\alpha}_{0jd}, \\hat{\\alpha}_{1jd},\\hat{\\alpha}_{2jd})  = \\arg \\min \\sum_{t=1}^T\\left(Z_{1jdt} - \\alpha_{0} -  Z_{2jt}'\\alpha_{1} - W_{t}'\\alpha_{2} \\right)^2 \\;\\; \\text{ subject to } \\;\\; \\sum_{l = 1}^w |a_{3,l}| \\leq \\lambda\n",
    "\\end{align*}\n",
    "where $\\alpha_0 \\in \\mathbb{R}$, $\\alpha_{1} \\in \\mathbb{R}^{p_2}$, and $\\alpha_{3} \\in \\mathbb{R}^{w}$. So that again,\n",
    " $$\\hat{V}_{jdt} = Z_{1jdt} -\\hat{\\alpha}_{0jd} - Z_{2jt}'\\hat{\\alpha}_{1jd} - W_{t}'\\hat{\\alpha}_{2jd}$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 3.2.1: Case 2 Estimation </h4>\n",
    "\n",
    "Given that the data is not panel q seperate ols regressions will also be estimated but here I will implement the lasso algorithm (Tibshrani (1996) JRSSB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### lasso estimator goes here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3.3 Case 3: </h3>\n",
    "\n",
    "* $W_{jt}$ is an known subset of $W_{t}$,\n",
    "\n",
    "* Cross-sectional data is panel, meaning for all $j,j' \\in \\{1,2, \\ldots,q\\}$\n",
    "\n",
    "    1. $\\alpha_{1jd} =\\alpha_{1j'd} \\equiv \\alpha_{1d}$  \n",
    "\n",
    "    2.  $\\alpha_{2jd,l} = \\alpha_{2j'd,l} \\equiv \\alpha_{2d,l}$ whenever $W_{t,l} \\in W_{jt}$ and $W_{t,l} \\in W_{j't}$ \n",
    "\n",
    "\n",
    "Let\n",
    "\\begin{align*}\n",
    "1[W_{t,l} \\in W_{tj}] = \n",
    "\\begin{cases} \n",
    "1 & \\text{ if } W_{t,l} \\in W_{tj} \\\\ \n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "and $M_j = \\text{diag}(\\{1[ W_{t,l} \\in W_{jt}] \\}_{l=1}^w)$ so that,\n",
    "\n",
    "\\begin{align*}\n",
    "Z_{1jdt} &= \\alpha_{0jd} + Z_{2jt}' \\alpha_{1jd} + W_{jt}' \\alpha_{2jd} + V_{jdt} \\\\\n",
    "& =\\alpha_{0jd} + Z_{2jt}' \\alpha_{1d} + W_{t}'M_j \\alpha_{2d} + V_{jdt}\n",
    "\\end{align*}\n",
    "\n",
    "Now let \n",
    "* $\\Delta Z_{1jdt} = Z_{1jdt} - Z_{1jd(t-1)}$\n",
    "* $\\Delta Z_{2jdt} = Z_{2jdt} - Z_{2jd(t-1)}$ \n",
    "* $\\Delta W_{t} = W_{t} - W_{t-1}$, \n",
    "* $\\Delta V_{jdt} = V_{jdt} - V_{jd(t-1)}$ \n",
    "\n",
    "so that, \n",
    "\n",
    "\\begin{align*} \n",
    "\\Delta Z_{1jdt} =\\Delta Z_{2jt}' \\alpha_{1d} + \\Delta W_{t}'M_j \\alpha_{2d} + \\Delta V_{jdt}\n",
    "\\end{align*}\n",
    "As a result,\n",
    "\n",
    "\\begin{align*} \n",
    "(\\hat{\\alpha}_{1d},\\hat{\\alpha}_{2d})  = \\arg \\min \\sum_{j=1}^q\\sum_{t=1}^T\\left( \\Delta Z_{ijt} -  \\Delta Z_{2jt}'\\alpha_{1} - \\Delta W_{t}'M_j\\alpha_{2} \\right)^2 \n",
    "\\end{align*}\n",
    "\n",
    "So that given\n",
    "\\begin{align*} \n",
    "\\alpha_{0jd} =  E(V_{jdt} + \\alpha_{0jd}) =\n",
    "E( Z_{1jdt} - Z_{2jt}'\\alpha_{1d} - W_{t}'M_j\\alpha_{2d}) \n",
    "\\end{align*}\n",
    "\n",
    "we have\n",
    "\n",
    "$$\\hat{V}_{jdt} = Z_{1jdt} - Z_{2jt}'\\hat{\\alpha}_{1d} - W_{t}'M_j\\hat{\\alpha}_{2d} - T^{-1}\\sum_{t=1}^T  (Z_{1jdt} - Z_{2jt}'\\hat{\\alpha}_{1d} - W_{t}'M_j\\hat{\\alpha}_{2d}) $$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 3.3.1: Case 3 Estimation Function </h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def panel_fe(dp_in,di_in,inpt):\n",
    "    \"\"\"\n",
    "INPUTS\n",
    "dp_in                     (pandas df) df with dependent var. and all exogenous regs\n",
    "di_in                     (pandas df) df with all instruments\n",
    "inpt                      (dict) Dictionary with the following\n",
    "  inpt['dep']               (string) Name of dependent variable contained in dp\n",
    "  inpt['reg']               (list of strings) names of exogenous regressors\n",
    "  inpt['cin']               (string) name of crossection index in dp\n",
    "  inpt['tin']               (string) name of time index in dp\n",
    "  inpt['ncs']               (int) number of crossections\n",
    "  inpt['in_nm']             (list of lists of strings) with the following components\n",
    "    input['in_nm'][i-1]       (list of names) names of instruments relevant to crs i \n",
    "  \n",
    "\n",
    "OUTPUTS \n",
    "out               (list of lists) List of the following\n",
    "  out[0]          (list) Estimated coefficients\n",
    "  out[1]          (list) list of all relevant instrument\n",
    "  out[2]          (list) Estimated error terms Vi_j\n",
    "    \"\"\"\n",
    "\n",
    "    ## Extracting Variables from inpt dictionary\n",
    "    dep = inpt['dep']\n",
    "    reg = inpt['reg']\n",
    "    in_nm = inpt['in_nm']\n",
    "    cin = inpt['cin']\n",
    "    tin = inpt['tin']\n",
    "    ncs = inpt['ncs']\n",
    "    di  = di_in\n",
    "    dp  = dp_in\n",
    "\n",
    "    ## Constructing a df of all instrument relevant to at least 1 crossection\n",
    "    # Initializing the set of all included instruments as the 1st set relevant inst\n",
    "    inc = set(in_nm[0])\n",
    "    # Collecting rest of relevant instruments\n",
    "    for i in range(len(in_nm)):\n",
    "        # Union of inc and ith set of relevant instruments\n",
    "        inc = inc|set(in_nm[i])\n",
    "    # All included Instruments listed in order index order\n",
    "    inc = [''.join(['W',str(i)]) for i in range(1,di.shape[1]) if ''.join(['W',str(i)]) in inc ]\n",
    "    # df with time index and all included instruments\n",
    "    di_inc = di.loc[:,[tin] + inc]\n",
    "\n",
    "    # List of logical vectors (as list) of which instruments in inc are relevant to ith crs\n",
    "    in_vec = [[ 1 if inc[i] in in_nm[j] else 0 for i in range(len(inc))]\n",
    "                  for j in range(ncs)]\n",
    "\n",
    "    # First differenced included instrument df\n",
    "    Di = di_inc.loc[:,[tin]]\n",
    "    for j in range(0,len(inc)):\n",
    "        # Adding D to name of jth instrument in inc\n",
    "        D_nm = ''.join(['D', di_inc.columns[j+1]])\n",
    "        # First difference of jth instrument in inc\n",
    "        Di[D_nm] = (di_inc.loc[:,di_inc.columns[j+1]].values \n",
    "                    - di_inc.loc[:,di_inc.columns[j+1]].shift(1).values)\n",
    "\n",
    "    ## Constructing Panel Version of relevant instrument data\n",
    "    for i in range(ncs):\n",
    "        # Initializing differenced panel template df\n",
    "        a1 = Di.loc[:,tin].copy()\n",
    "        # Adding the crossection variable\n",
    "        a1 = pd.concat([a1,pd.DataFrame(np.ones((di_inc.shape[0],1))*(i+1)\n",
    "                                        ,columns = [cin])],axis = 1)\n",
    "        # Initializing the panel template df\n",
    "        b1 = a1\n",
    "        # Product of Di with in_vec[i-1] s.t. inst. not relevant to i are zero \n",
    "        a2 = pd.DataFrame(Di.iloc[:,1:].values.dot(np.diag(in_vec[i]))\n",
    "                          ,columns = Di.columns[1:])\n",
    "        # Product of di_inc with in_vec[i-1] s.t. inst. not relevant to i are zero\n",
    "        b2 = pd.DataFrame(di_inc.iloc[:,1:].values.dot(np.diag(in_vec[i]))\n",
    "                          ,columns = di_inc.columns[1:])\n",
    "        # Concatenating a2 onto panel template adding [cin] and [tin]\n",
    "        a2 = pd.concat([a1,a2],axis = 1)\n",
    "        # Concatenating b2 onto panel template adding [cin] and [tin]\n",
    "        b2 = pd.concat([b1,b2],axis = 1)\n",
    "        if i == 0:\n",
    "            # if  i = 0 initialize final differenced panel df\n",
    "            Ddi_pan = a2.iloc[1:,:]\n",
    "            # if i = 0 initialize final panel df\n",
    "            di_pan = b2\n",
    "        elif i > 0: \n",
    "            # if i > 0 add ith crosssection rows onto final\n",
    "            Ddi_pan = pd.concat([Ddi_pan,a2.iloc[1:,:]], axis = 0)\n",
    "            # if i > 0 add ith crosssection rows onto final\n",
    "            di_pan =  pd.concat([di_pan,b2], axis = 0)\n",
    "\n",
    "    ## First Difference dependent and exogenous regressor matrix..\n",
    "    for i in range(1,ncs+1):\n",
    "        # Initializing panle template df\n",
    "        c1 = dp.loc[dp[cin]== i,[cin]+[tin]].copy() \n",
    "        # First differencing al relevant variables for i crs\n",
    "        for j in [inpt['dep']] + inpt['reg']:\n",
    "            c1[''.join(['D',j])] = (dp.loc[dp[cin]== i,j].values \n",
    "                        - dp.loc[dp[cin]== i,j].shift(1).values)\n",
    "        if i == 1:\n",
    "            # If i = 1 initialize final panel df\n",
    "            c2 = c1.iloc[1:,:]  \n",
    "        elif i > 1:\n",
    "            # If i > 1 add onto final panel df\n",
    "            c2 = pd.concat([c2,c1.iloc[1:,:]],axis = 0)\n",
    "\n",
    "    ## OLS estimation\n",
    "    # Merging all differenced panel df's together\n",
    "    Ddi_pan = pd.merge(c2,Ddi_pan,on=[cin,tin],how = 'inner')\n",
    "    # List of all regressor names in Ddi_pan for use in ols()\n",
    "    Dregs = ([''.join(['D',reg[i]]) for i in range(len(reg))] \n",
    "            + [''.join(['D',inc[i]]) for i in range(len(inc))])\n",
    "    # Dictionary for input into ols()\n",
    "    inpt_ols = {'dep': ''.join(['D',dep]) , 'reg': Dregs, 'cons': 0}\n",
    "    # Estimation by ols()\n",
    "    ols_out = ols(Ddi_pan,inpt_ols)\n",
    "\n",
    "\n",
    "    # Names of all exogenous regressor and inst. included in di_pan\n",
    "    regs = ([ reg[i] for i in range(len(reg))] \n",
    "            + [ inc[i] for i in range(len(inc))])\n",
    "    # Estimated coefficient from estimator above \n",
    "    e_cf = np.array(ols_out[0]).reshape(len(ols_out[0]),1)\n",
    "\n",
    "    # Constucting a panel df of esitmated errors Vj,i\n",
    "    for i in range(1,ncs+1):\n",
    "        # np.array of dep variable values for ith crs\n",
    "        d1 = dp.loc[dp[cin] == i,[dep]].values\n",
    "        # df of exogenous regressor values for ith crs\n",
    "        d21 = dp.loc[dp[cin] == i ,[cin]+[tin]+reg]\n",
    "        # included instruments panel with indexes for i \n",
    "        d22 = di_pan.loc[di_pan[cin] == i,[cin]+[tin]+inc]\n",
    "        # merging d21 and d22 making a df with all RHS regressors \n",
    "        d23 = pd.merge(d21,d22,on = [cin,tin], how = 'inner')\n",
    "        # Estimated time varying component of dependent variable\n",
    "        d2 = d23.drop([cin,tin],axis = 1).values.dot(e_cf)\n",
    "        # Residual of time varying component \n",
    "        d3 = d1.reshape(d1.shape[0],1) - d2\n",
    "        # Centered residula of time varying component\n",
    "        d3 = d3 - np.mean(d3)\n",
    "        if i == 1:\n",
    "            # if i = 1 initialize panel df\n",
    "            p_res = [list(d3.T[0])]\n",
    "        elif i > 1:\n",
    "            # if i > 1 add onto p_res\n",
    "            p_res.append(list(d3.T[0]))\n",
    "\n",
    "    # Output of the function\n",
    "    out = [list(e_cf.T[0]), inc , p_res ]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3.4: Case 4 </h3>\n",
    "\n",
    "* $W_{jt}$ is an **unknown** subset of $W_{t}$,\n",
    "\n",
    "* Cross-sectional data is panel, meaning for all $j,j' \\in \\{1,2, \\ldots,q\\}$\n",
    "\n",
    "    1. $\\alpha_{1jd} =\\alpha_{1j'd} \\equiv \\alpha_{1d}$  \n",
    "\n",
    "    2. $\\alpha_{2jd,l} = \\alpha_{2j'd,l} \\equiv \\alpha_{2d,l}$ whenever $W_{t,l} \\in W_{jt}$ and $W_{t,l} \\in W_{j't}$ \n",
    "\n",
    "Inhereting notation from case 3 we again have\n",
    "\n",
    "\\begin{align*} \n",
    "\\Delta Z_{1jdt} =\\Delta Z_{2jt}' \\alpha_{1d} + \\Delta W_{t}'M_j \\alpha_{2d} + \\Delta V_{jdt}\n",
    "\\end{align*}\n",
    "\n",
    "In order to introduce our selection procedure we will estimate the coefficients on $W_{t}$ as if that are not identical, then average the non zero estimates to construct a single estimate.  Consider, \n",
    "\n",
    "\\begin{align*} \n",
    "(\\hat{\\alpha}_{1d},\\hat{\\alpha}_{2d})  = \\arg \\min \\sum_{j=1}^q\\sum_{t=2}^T\\left( \\Delta Z_{1jdt} -  \\Delta Z_{2jt}'\\alpha_{1} - \\Delta W_{t}'\\alpha_{2j} \\right)^2 \\;\\; \\text{ subject to } \\;\\; \\sum_{l=1}^w|\\alpha_{2j,l}| \\leq \\lambda \\;\\;  \\text{ for all } 1 \\leq j \\leq q\n",
    "\\end{align*}\n",
    "\n",
    "Consequently define for some $\\varepsilon > 0$ \n",
    "\n",
    "\\begin{align*} \n",
    "\\tilde{\\alpha}_{2d,l} = \\frac{\\sum_{l=1}^q \\hat{\\alpha}_{2jd,l} 1[ \\hat{\\alpha}_{2jd,l} > \\varepsilon ] }{ \\sum_{l=1}^q 1[ \\hat{\\alpha}_{2jd,l} > \\varepsilon] }\n",
    "\\end{align*}\n",
    "\n",
    "Now let $\\tilde{\\alpha}_{2d} = [ \\; \\tilde{\\alpha}_{2d,1} \\;\\; \\tilde{\\alpha}_{2d,2} \\;\\; \\cdots \\;\\; \\tilde{\\alpha}_{2d,w}  \\; ]'$ and $\\tilde{M}_{jd} = \\text{diag}( \\{ 1[\\hat{\\alpha}_{2jd,l} > \\varepsilon ] \\}_{l=1}^w)$ so that, \n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{V}_{jdt} = Z_{1jdt} - Z_{2jt}'\\hat{\\alpha}_{1d} - W_{t}'\\tilde{M}_{jd}\\tilde{\\alpha}_{2d} - T^{-1}\\sum_{t=1}^T  (Z_{1jdt} - Z_{2jt}'\\hat{\\alpha}_{1d} - W_{t}'\\tilde{M}_{jd}\\tilde{\\alpha}_{2d}) \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 3.4.1: Case 4 Estimation </h4>\n",
    "\n",
    "This is fixed effect panel data estimator with cross-section wise lasso estimation, I will implement it here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## panel estimator with lasso goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4 Estimator </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 4.1 Estimator function </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def psc_est(data_pan,data_inst,data_err,inpt):\n",
    "    \"\"\"\n",
    "INPUTS\n",
    "data_pan           (pandas df) panel of all primary regressors each crss sect, and time period\n",
    "data_inst          (pandas df) all possible instruments \n",
    "data_err           (pandas df) panel df of all error terms, only used if inpt['orcl'] == 1\n",
    "inpt               (dict) The following input var. \n",
    "  inpt['cin']        (string) names of cross section var. in data_pan & data_err\n",
    "  inpt['tin']        (string) names of cross section var. in data_pan & data_err\n",
    "  inpt['ntp']        (int) number time periods\n",
    "  inpt['ncs']        (int) number of crossections\n",
    "  inpt['n_end']      (int) number of endogenous variables \n",
    "  inpt['n_exo']      (int) number of exogenous variables\n",
    "  inpt['t_inst']\n",
    "  inpt['dep_nm']     (string) name of dependent varaible in primary regression in data_pan \n",
    "  inpt['en_nm']      (list of strings) names of endogenous variables in data_pan\n",
    "  inpt['ex_nm']      (list of strings) names of exogenous variables in data_pan\n",
    "  inpt['in_nm']      (list of lists of string) names of instruments relevant to each cross section\n",
    "    inpt['in_nm'][i]   (list of strings) names of var. in data_inst used by cin == i (all if kwnsub=0)   \n",
    "  inpt['sec_pan']    (int: 0,1) Indicator for if secondary equation data is panel\n",
    "  inpt['lasso']      (int: 0,1) Indicator for if W_{i} is a known subset of W for all cin \n",
    "  inpt['orcl']       (int: 0,1) Indicator for if V_i,j is observed for each cin = j, and endog var. j \n",
    "  inpt['k_mvd']      (int: [0-11]) kernel used for multi var density est. see (?mvkernel)\n",
    "  inpt['k_uvd']      (int: [0-11]) kernel used for bi var density est. see (?mvkernel)\n",
    "  inpt['k_H']        (int: [0-11]) kernel used for NW H function  est. see (?mvkernel)\n",
    "  inpt['c_mvd']      (float) plug in bandwidth const. for  multi var density est.\n",
    "  inpt['c_uvd']      (float) plug in bandwidth const. for bi var density est.\n",
    "  inpt['c_H']        (float) plug in bandwidth const. for NW H function  est.\n",
    "\n",
    "OUTPUT\n",
    "beta                 (list) estimated common primary function coefficients\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Extracting variables from input dictionary\n",
    "    cin = inpt['cin']\n",
    "    tin = inpt['tin']\n",
    "    ntp = inpt['ntp']\n",
    "    ncs = inpt['ncs']\n",
    "    n_en = inpt['n_end']\n",
    "    n_ex = inpt['n_exo']\n",
    "    t_inst = inpt['t_inst']\n",
    "    dep_nm = inpt['dep_nm'] \n",
    "    en_nm = inpt['en_nm']\n",
    "    ex_nm = inpt['ex_nm']\n",
    "    in_nm = inpt['in_nm']\n",
    "    sec_pan = inpt['sec_pan']\n",
    "    knw = inpt['lasso']\n",
    "    orcl = inpt['orcl']\n",
    "    k_mvd = inpt['k_mvd']\n",
    "    k_uvd = inpt['k_uvd']\n",
    "    k_H = inpt['k_H']\n",
    "    c_mvd = inpt['c_mvd']\n",
    "    c_uvd = inpt['c_uvd']\n",
    "    c_H = inpt['c_H']\n",
    "    dp = data_pan\n",
    "    di = data_inst\n",
    "    de = data_err\n",
    "\n",
    "    ############ Estimation of secondary equations\n",
    "\n",
    "    ## For the moment I am forsaking this estimation in favor of making sure the estimation of the\n",
    "    ## primary equation is working, as you see below I use the true values of V instead of estimates\n",
    "\n",
    "    if orcl == 1:\n",
    "        # If residuals are observed and not estimated\n",
    "        for i in range(1,n_en+1):\n",
    "            dp[''.join(['h_V',str(i)])] = de.loc[:,''.join(['V',str(i)])]\n",
    "        re_nm = [''.join(['h_V',str(i)]) for i in range(1,n_en+1)]\n",
    "    else:\n",
    "        # If residuals are not observed and must be estimated\n",
    "        if sec_pan == 0:\n",
    "            # If secondary equations are not panel\n",
    "            if knw == 1:\n",
    "                ## This is estimation in accordance with case 1 in section 3\n",
    "                # For each endogenous variable\n",
    "                for j in range(1,len(en_nm)+1):\n",
    "                    # For each Cross Section\n",
    "                    for k in range(1,ncs+1):\n",
    "                        #Initializing temp df\n",
    "                        aa1 = dp.loc[dp[cin] == k,[cin]+[tin]]\n",
    "                        #Initailizing the regressor matrix\n",
    "                        pdt_in = dp.loc[dp[cin]==k,[cin]+[tin]+[en_nm[j-1]]+ex_nm]\n",
    "                        # Merging in the array of instruments\n",
    "                        dt_in = pd.merge(pdt_in, di, on = tin,how = 'inner')\n",
    "                        # Input variables\n",
    "                        inptd = {'dep':en_nm[j-1],'reg':ex_nm + in_nm[i-1],'cons':0}\n",
    "                        # Adding estimated residuals to df\n",
    "                        sec_result = ols(dt_in,inptd)\n",
    "                        aa1[''.join(['h_V',str(j)])] = sec_result[1]\n",
    "                        if k == 1:\n",
    "                            # If first cross section intialize long panel version of temp aa1\n",
    "                            aa2 = aa1\n",
    "                        elif k > 1:\n",
    "                            # If not first cross section adding aa1 to the bottom of aa2\n",
    "                            aa2 = pd.concat([aa2,aa1],axis = 0)\n",
    "                            sec_cf.append(sec_result[0])\n",
    "                    # Merging aa2 into dp\n",
    "                    dp = pd.merge(dp,aa2,on = [cin,tin],how = 'inner')\n",
    "                    \n",
    "            elif knw == 0:\n",
    "                # This is estimation in accordance with case 2 in section 3\n",
    "                pass\n",
    "        ## If Secondary equations are panel\n",
    "        elif sec_pan == 1:\n",
    "            # If instruments relevant to each crs is a known subset\n",
    "            if knw == 1:\n",
    "                # for each endogenous variable\n",
    "                for j in range(0,n_en):\n",
    "                    # constructing input dictionary for estimator\n",
    "                    inpt = {'dep':en_nm[j],'reg':ex_nm,'in_nm': in_nm ,'cin': cin,'tin': tin,'ncs':ncs}\n",
    "                    # estimation\n",
    "                    pan = panel_fe(dp,di,inpt)\n",
    "                    if j == 0:\n",
    "                        sec_cf = [pan[0]]\n",
    "                        sec_inc = pan[1]\n",
    "                    elif j>0:\n",
    "                        sec_cf.append(pan[0])\n",
    "                    for i in range(ncs):\n",
    "                        # Initializing panel template df\n",
    "                        a1 = dp.loc[dp[cin] == i+1,[cin]+[tin]]\n",
    "                        # Adding ith estimated error term to a1 \n",
    "                        a1[''.join(['h_V',str(j+1)])] = pan[2][i] \n",
    "                        if i == 0: \n",
    "                            # if i = 0 initializing panel version\n",
    "                            a2 = a1\n",
    "                        elif i > 0: \n",
    "                            # if i > 1 adding to panel version\n",
    "                            a2 = pd.concat([a2,a1],axis=0)\n",
    "                    # merging panel of estimated error terms into dp \n",
    "                    dp = pd.merge(dp,a2,on = [cin,tin],how = 'inner')\n",
    "                re_nm = [''.join(['h_V',str(i)]) for i in range(1,n_en+1)]\n",
    "            # If instruments relevant to each crs is an unknown subset\n",
    "            elif knw == 0:\n",
    "                # This is where estimation in accordance with case 4 in section 3 will go\n",
    "                pass\n",
    "\n",
    "\n",
    "    ############ Estimation of primary equation\n",
    "\n",
    "    ## Creating a new dataframe with back shifted columns of each residual\n",
    "    # For each residual\n",
    "    for i in range(n_en):\n",
    "        # For each cross section\n",
    "        for j in range(1,ncs+1):\n",
    "            # Initializing the dataframe ['crs' , 't' , 'h_Vi]\n",
    "            a1 = dp.loc[dp[cin] == j,[cin,tin,re_nm[i]]]\n",
    "            # Adding a column of backshifted residuals note the first element is NA\n",
    "            a1[''.join(['b',re_nm[i]])] = dp.loc[dp[cin] == j,re_nm[i]].shift(1)\n",
    "            # Using only the columns where 't' > 1 eliminating the NA\n",
    "            a1 = a1.iloc[1:,:]\n",
    "            if j == 1:\n",
    "                # If this first crosssection intialize intermediate df\n",
    "                a2 = a1\n",
    "            else:\n",
    "                # If not first cross section concatentate current to a2\n",
    "                a2 = pd.concat([a2,a1],axis = 0)\n",
    "        if i == 0:\n",
    "            # If first residual initialize den data frame\n",
    "            den = a2\n",
    "        else:\n",
    "            # If not first residul merge result into den df\n",
    "            den=pd.merge(den,a2,on=[cin,tin], how = 'inner')\n",
    "\n",
    "    # List of backshifted residual names\n",
    "    reb_nm = [''.join(['b',re_nm[i]]) for i in range(n_en)]\n",
    "\n",
    "    ## Calculating the joint density of all residuals and their backshifted values.  \n",
    "    # For each cross section\n",
    "    for j in range(1,ncs+1):\n",
    "        # Extracting indexes residuals and the back shifted versions for density est.\n",
    "        b1 = den.loc[den[cin]== j,[cin] +[tin]+ re_nm + reb_nm]\n",
    "        # Plug in bandwiths for density estimation\n",
    "        hb1 = c_mvd*(ntp-1)**(-1/(4+n_en))*np.std(b1.drop([cin]+[tin],axis =1).values,axis = 0)\n",
    "        # Initializing b2 temp df\n",
    "        b2 = b1.loc[:,[cin]+[tin]].copy()\n",
    "        # Adding a column of the mv density of all resids and their back shifts to b2 for crs j\n",
    "        b2['VA_den'] = mvden(b1.drop([cin]+[tin],axis = 1),b1.drop([cin]+[tin],axis = 1),hb1,k_mvd)\n",
    "        if j == 1:\n",
    "            # If first cross section intialize long panel version of temp b2 \n",
    "            b3 = b2\n",
    "        else:\n",
    "            # If not first cross section adding b2 to the bottom of b3\n",
    "            b3 = pd.concat([b3,b2],axis = 0)\n",
    "    # Merging the panel of mv densities into den\n",
    "    den = pd.merge(den,b3,on=[cin,tin], how = 'inner')\n",
    "\n",
    "    ## Calculating the bivariate density of each residual and its backshifted values\n",
    "    # For each residual since n_en = #of residuals V_i\n",
    "    for i in range(1,n_en+1):\n",
    "        # Variable names of indexes, ith residuals, and  ith backshifts\n",
    "        clms = [cin , tin , ''.join(['h_V',str(i)]) , ''.join(['bh_V',str(i)])]\n",
    "        # For each cross section\n",
    "        for j in range(1,ncs+1):\n",
    "            # Extracting indexes, ith residuals, and ith backshifts for jth cross section\n",
    "            c1 = den.loc[den[cin]==j, clms]\n",
    "            # Plug in bandwidths for bivariate density estimation\n",
    "            hc1 = c_uvd*(ntp-1)**(-1/6)*np.std(c1.drop([cin]+[tin],axis =1).values,axis = 0)\n",
    "            # Initializing c2 temp df\n",
    "            c2 = c1.loc[:,[cin]+[tin]].copy()\n",
    "            # Adding a column of bv densities of ith resid and their backshifts to c2 for crs j\n",
    "            c2[''.join(['V',str(i),'_den'])] =  mvden(c1.drop([cin,tin],axis = 1),\n",
    "                                                      c1.drop([cin,tin],axis = 1),hc1,k_uvd)\n",
    "            if j == 1:\n",
    "                # If first cross section intialize long panel version of temp c2 \n",
    "                c3 = c2\n",
    "            else:\n",
    "                # If not first cross section adding c2 to the bottom of c3\n",
    "                c3 = pd.concat([c3,c2],axis = 0)  \n",
    "        # Merging panel of densities for jth residual and backshifts\n",
    "        den = pd.merge(den,c3,on=[cin,tin], how = 'inner')\n",
    "\n",
    "    ## Constructing phi ratio each theta density ratio\n",
    "    # List of residuals (by index) to be included in the numerator of each ratio\n",
    "    d1 = [list(iter.filterfalse(lambda x: x==i, range(1,n_en+1))) for i in range(1,n_en+1)]\n",
    "    # List of list of densities included in the numerator or some ratios.\n",
    "    d2 = [[''.join(['V',str(d1[i][j]),'_den']) for j in range(n_en-1)] for i in range(n_en)]\n",
    "    # Constructing each ratio and adding to den df\n",
    "    for i in range(n_en):\n",
    "        den[''.join(['th',str(i+1)])] = np.prod(den.loc[:,d2[i]].values,axis = 1)/den.loc[:,'VA_den'].values\n",
    "    # List of names of all bv densities\n",
    "    e1 = [''.join(['V',str(i),'_den']) for i in range(1,n_en+1)]\n",
    "    # Constructing phi ratio\n",
    "    den['phi'] = np.prod(den.loc[:,e1].values,axis = 1)/den.loc[:,'VA_den'].values  \n",
    "\n",
    "    ## Construction differenced dataframe\n",
    "    # Names of all variables to be differenced\n",
    "    al_nm = [dep_nm] + en_nm + ex_nm\n",
    "    # New names with 'D' concatenated at beginning of string\n",
    "    Dal_nm = [ ''.join(['D',al_nm[j]]) for j in range(len(al_nm))]\n",
    "    # Initializing the differenced data frame\n",
    "    df = den.loc[:,[cin,tin]].copy()\n",
    "    # For each varaible to be differenced\n",
    "    for i in range(len(al_nm)):\n",
    "        # For each cross section\n",
    "        for j in range(1,ncs+1):\n",
    "            # For each cross section generate temp df with [cin tin D(var)]\n",
    "            f1 = pd.concat([dp.loc[dp[cin] == j,cin],\n",
    "                            dp.loc[dp[cin] == j,tin],\n",
    "                            dp.loc[dp[cin] == j,al_nm[i]] \n",
    "                            - dp.loc[dp[cin] == j,al_nm[i]].shift(1) ],axis = 1)[1:]\n",
    "            # Naming the colmns of temp matrix\n",
    "            f1.columns = [cin , tin , ''.join(['D',al_nm[i]])]\n",
    "            if j==1: \n",
    "                # If first cross section intialize long panel version of temp f1 \n",
    "                f2 = f1\n",
    "            else: \n",
    "                # If not first cross section adding f1 to the bottom of f2\n",
    "                f2 = pd.concat([f2,f1],axis = 0)\n",
    "        # Merging f2 into df\n",
    "        df = pd.merge(df,f2,on=[cin,tin], how = 'inner')\n",
    "\n",
    "    ## Constructing the array of H functions\n",
    "    # Initializing the H functions df\n",
    "    Hf = df.iloc[:,:2].copy()\n",
    "    # For all varaibes in primary regression \n",
    "    for i in range(n_en+n_ex+1):\n",
    "        # For each residual = # of endogenous variables\n",
    "        for j in range(1,n_en+1):\n",
    "            # For each cross section\n",
    "            for k in range(1,ncs+1):\n",
    "                # Initailizing temp H df\n",
    "                g1 = Hf.loc[Hf[cin] == k,[cin,tin]].copy()\n",
    "                # ith dependent variable for kth crs\n",
    "                dep = df.loc[df[cin] == k,df.columns[2:][i]]\n",
    "                # jth density ratio \n",
    "                tht = den.loc[den[cin] == k, ''.join(['th',str(j)])]\n",
    "                # jth regressors\n",
    "                reg = den.loc[den[cin] == k , [''.join(['h_V',str(j)]),''.join(['bh_V',str(j)]) ]]\n",
    "                # Plug in bandwidths\n",
    "                h = c_H*np.std(np.array(reg),axis = 0) \n",
    "                # Adding the estimated function to temp df\n",
    "                g1_nm = ''.join(['H',re.sub('^D','',df.columns[2:][i]),';',str(j)])\n",
    "                g1[g1_nm] = Nw_H_pan(dep,tht,reg,reg,h,k_H)\n",
    "                if k == 1:\n",
    "                    # If first cross section intialize long panel version of temp g1 \n",
    "                    g2 = g1\n",
    "                else:\n",
    "                    # If not first cross section adding g1 to the bottom of g2\n",
    "                    g2 = pd.concat([g2,g1],axis = 0)\n",
    "            # Merging g2 into Hf\n",
    "            Hf = pd.merge(Hf,g2,on=[cin,tin], how = 'inner')\n",
    "\n",
    "    ## Summing all for H functions\n",
    "    # Names of all variables in primary regression\n",
    "    pr_nm = [dep_nm]+en_nm+ex_nm\n",
    "    # Names of all summed H functions\n",
    "    prH_nm = [ ''.join(['H',i ]) for i in pr_nm ]\n",
    "    # Names of all difference variables\n",
    "    prD_nm = [ ''.join(['D',i ]) for i in pr_nm ]\n",
    "    # Names of full [ var - Hvar ]\n",
    "    prS_nm = [ ''.join(['S',i ]) for i in pr_nm ]\n",
    "    # Adding the summed H functions to Hf\n",
    "    for i in range(len(pr_nm)):\n",
    "        Hf[prH_nm[i]] = np.sum(Hf.filter(regex = ''.join(['^',prH_nm[i],';']), axis=1).values,axis =1)\n",
    "\n",
    "    # Initialized the subtracted df\n",
    "    Prf = Hf.loc[:,[cin]+[tin]].copy()\n",
    "    # Construcing the subtracted df\n",
    "    for i in range(len(pr_nm)): \n",
    "        Prf[prS_nm[i]] = df.loc[:,prD_nm[i]].sub(Hf.loc[:,prH_nm[i]])\n",
    "\n",
    "    # First matrix constructed by extracting the subtracted dependent variable\n",
    "    A = (Prf.loc[:,''.join(['S',dep_nm])].values)\n",
    "    # Reshape so it is 2 dimensional\n",
    "    A = A.reshape(A.shape[0],1)\n",
    "    # Second matrix constructed by droppin cin tin and subtracted dependent var\n",
    "    B = Prf.drop([cin ,tin , ''.join(['S',dep_nm])],axis = 1 ).values\n",
    "    # Constructing the diagonal phi matrix\n",
    "    C = np.diag(den.loc[:,'phi'])\n",
    "    # Denominator of the estimator\n",
    "    D = np.linalg.inv(B.T.dot(C).dot(B))\n",
    "    # Numerator\n",
    "    N = B.T.dot(C).dot(A)\n",
    "    # Final Estimated value\n",
    "    beta = D.dot(N).T.tolist()\n",
    "    \n",
    "    return [beta , sec_cf , sec_inc ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 5 Monte Carlo </h2>\n",
    "\n",
    "The data used in the following monte carlo exercise was created by 'psc_dgp.ipynb' where is was converted to a .json format and stored in '.../pan_sel_cntrl_repo/data' folder. The following sections load and convert this .json data into a usable form. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 5.1 Monte Carlo: JSON Loading </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_filename = 'pscdata_6_19_1923.json'\n",
    "data_file = '/Users/ericpenner/Google_Drive/Research/pan_sel_cntrl/data'\n",
    "input_file_full = ''.join([data_file,'/',input_filename])\n",
    "with open(input_file_full) as f_obj: \n",
    "    pscdata = json.load(f_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 5.2 Monte Carlo: Extracting metadata dictionary and creating the input dictionary </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initializing the data sets metadata dictionary \n",
    "inpt = pscdata[0][0].copy()\n",
    "# Removing data that should not be passed to estimator\n",
    "for i in ['c_inst','err_vpro','ex_vpro','inst_vpro','r_seed','frc', 'nds']:\n",
    "    del inpt[i]\n",
    "\n",
    "inpt['input_filename'] = input_filename\n",
    "\n",
    "# Indicator for  whether in this run the subset of instrument relvant to each crs is known.\n",
    "inpt['kwnsub'] = 0 \n",
    "\n",
    "# Setting Kernel and bandwidth constant variables.\n",
    "inpt['k_mvd'] = 9\n",
    "inpt['k_uvd'] = 9\n",
    "inpt['k_H'] = 9\n",
    "inpt['c_mvd'] = 1.5\n",
    "inpt['c_uvd'] = 1.4\n",
    "inpt['c_H'] = 1.5\n",
    "\n",
    "# Indicator for whether residuals are observed\n",
    "inpt['orcl'] = 0\n",
    "inpt['lasso'] = 1\n",
    "\n",
    "# List of list with the names of the relevant instruments for each crossection\n",
    "in_nm=[]\n",
    "for i in range(inpt['ncs']):\n",
    "    # If the subset is known then list of relevant inst. for each crs is supplied to estimator\n",
    "    if inpt['kwnsub'] == 1:\n",
    "        a=[ True if pscdata[0][1]['coeff'][0][i][k]!=0 else False \n",
    "            for k in range(inpt['n_exo'],inpt['n_exo']+inpt['t_inst'])]\n",
    "        in_nm.append(np.array(pscdata[0][1]['Dins_nms'][1:])[a].tolist())\n",
    "    # If the subset is unknown then list of all inst. will be supplied to est. for each crs\n",
    "    else:\n",
    "        in_nm.append(pscdata[0][1]['Dins_nms'][1:])\n",
    "inpt['in_nm'] = in_nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c_H': 1.5,\n",
       " 'c_mvd': 1.5,\n",
       " 'c_uvd': 1.4,\n",
       " 'cin': 'crs',\n",
       " 'dep_nm': 'Y',\n",
       " 'en_nm': ['Z1,1', 'Z1,2'],\n",
       " 'ex_nm': ['Z2,1', 'Z2,2'],\n",
       " 'in_nm': [['W1', 'W2', 'W3', 'W4', 'W5', 'W6', 'W7', 'W8', 'W9', 'W10'],\n",
       "  ['W1', 'W2', 'W3', 'W4', 'W5', 'W6', 'W7', 'W8', 'W9', 'W10'],\n",
       "  ['W1', 'W2', 'W3', 'W4', 'W5', 'W6', 'W7', 'W8', 'W9', 'W10'],\n",
       "  ['W1', 'W2', 'W3', 'W4', 'W5', 'W6', 'W7', 'W8', 'W9', 'W10'],\n",
       "  ['W1', 'W2', 'W3', 'W4', 'W5', 'W6', 'W7', 'W8', 'W9', 'W10'],\n",
       "  ['W1', 'W2', 'W3', 'W4', 'W5', 'W6', 'W7', 'W8', 'W9', 'W10'],\n",
       "  ['W1', 'W2', 'W3', 'W4', 'W5', 'W6', 'W7', 'W8', 'W9', 'W10'],\n",
       "  ['W1', 'W2', 'W3', 'W4', 'W5', 'W6', 'W7', 'W8', 'W9', 'W10'],\n",
       "  ['W1', 'W2', 'W3', 'W4', 'W5', 'W6', 'W7', 'W8', 'W9', 'W10'],\n",
       "  ['W1', 'W2', 'W3', 'W4', 'W5', 'W6', 'W7', 'W8', 'W9', 'W10']],\n",
       " 'input_filename': 'pscdata_6_19_1923.json',\n",
       " 'k_H': 9,\n",
       " 'k_mvd': 9,\n",
       " 'k_uvd': 9,\n",
       " 'kwnsub': 0,\n",
       " 'lasso': 1,\n",
       " 'n_end': 2,\n",
       " 'n_exo': 2,\n",
       " 'ncs': 10,\n",
       " 'ntp': 30,\n",
       " 'orcl': 0,\n",
       " 'sec_pan': 1,\n",
       " 't_inst': 10,\n",
       " 'tin': 't'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the input dictionary\n",
    "inpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 5.3 Monte Carlo: Exercise Execution </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136a35679d55427f83a32648eb6f32c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='success', description='Running:', max=1000.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = ipw.FloatProgress(min=0,max=pscdata[0][0]['nds']\n",
    "                 ,description = 'Running:'\n",
    "                 ,bar_style = 'success') # instantiate the bar\n",
    "display(f) # display the bar\n",
    "\n",
    "beta= []\n",
    "alpha = []\n",
    "for k in range(1,pscdata[0][0]['nds']+1):\n",
    "    data_err = pd.DataFrame(pscdata[k][0]['err_df'], columns = pscdata[0][1]['Derr_nms'])  \n",
    "    data_inst = pd.DataFrame(pscdata[k][0]['inst_df'], columns = pscdata[0][1]['Dins_nms'])\n",
    "    data_pan = pd.DataFrame(pscdata[k][0]['prim_df'], columns = pscdata[0][1]['Dlng_nms'])\n",
    "    psc_results = psc_est(data_pan,data_inst,data_err,inpt)\n",
    "    beta.append(psc_results[0])\n",
    "    alpha.append(psc_results[1])\n",
    "    if k == 1:\n",
    "        inc_out = psc_results[2]\n",
    "    f.value += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 5.4 Extracting True Secondary Equation Coefficients </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For each entry in in_nm removing the 'W' then changing the string index number to and int\n",
    "in_nm_ind = [[ int(inpt['in_nm'][i][j].replace('W','')) \n",
    "               for j in range(len(inpt['in_nm'][0]))] \n",
    "               for i in range(inpt['ncs'])]\n",
    "# Init a list of dicts where coeff for Z_{1,k} on 'Wj' is stored as dd[k]['Wj'] (if nonzero).  \n",
    "dd = [{} for i in range(inpt['n_end'])]\n",
    "# For each endogenous regressor\n",
    "for k in range(0,inpt['n_end']):\n",
    "    # For each cross section\n",
    "    for i in range(inpt['ncs']):\n",
    "        # For each position in in_nm[i]\n",
    "        for j in range(len(inpt['in_nm'][0])):\n",
    "            # Dictionary key\n",
    "            tnm = ''.join(['W',str(in_nm_ind[i][j])])\n",
    "            # Finding the coefficient value and assigning to tnm element\n",
    "            dd[k][tnm] = pscdata[0][1]['coeff'][k][i][ in_nm_ind[i][j]-1 + inpt['n_exo']]\n",
    "# Using the dictionary constructed above to construct the vector of true secondary eqn panel \n",
    "sc_coeff =[ pscdata[0][1]['coeff'][k][0][:inpt['n_exo']] + \n",
    "             [dd[k][''.join(['W',str(j)])]  for j in range(1,inpt['t_inst']+1) \n",
    "                         if ''.join(['W',str(j)]) in dd[k].keys()]\n",
    "                         for k in range(inpt['n_end'])] \n",
    "# Note that sc_coeff[k][j] if the true value of the jth coefficient, in the secondary \n",
    "# equation for the kth endogenous regressor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 5.5 Reshaping and Centering Alpha and Beta List of Lists </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>$\\beta_{1,1}$</th>\n",
       "      <th>$\\beta_{1,2}$</th>\n",
       "      <th>$\\beta_{2,1}$</th>\n",
       "      <th>$\\beta_{2,2}$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bias</th>\n",
       "      <td>0.153755</td>\n",
       "      <td>0.187322</td>\n",
       "      <td>-0.033465</td>\n",
       "      <td>0.033309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE</th>\n",
       "      <td>0.025223</td>\n",
       "      <td>0.036642</td>\n",
       "      <td>0.009835</td>\n",
       "      <td>0.009333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variance</th>\n",
       "      <td>0.001582</td>\n",
       "      <td>0.001552</td>\n",
       "      <td>0.008715</td>\n",
       "      <td>0.008224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          $\\beta_{1,1}$  $\\beta_{1,2}$  $\\beta_{2,1}$  $\\beta_{2,2}$\n",
       "Bias           0.153755       0.187322      -0.033465       0.033309\n",
       "MSE            0.025223       0.036642       0.009835       0.009333\n",
       "Variance       0.001582       0.001552       0.008715       0.008224"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>$\\alpha_{11,1}$</th>\n",
       "      <th>$\\alpha_{11,2}$</th>\n",
       "      <th>$\\alpha_{11,1}$</th>\n",
       "      <th>$\\alpha_{12,2}$</th>\n",
       "      <th>$\\alpha_{13,3}$</th>\n",
       "      <th>$\\alpha_{14,4}$</th>\n",
       "      <th>$\\alpha_{15,5}$</th>\n",
       "      <th>$\\alpha_{16,6}$</th>\n",
       "      <th>$\\alpha_{17,7}$</th>\n",
       "      <th>$\\alpha_{18,8}$</th>\n",
       "      <th>$\\alpha_{19,9}$</th>\n",
       "      <th>$\\alpha_{110,10}$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bias</th>\n",
       "      <td>-0.006333</td>\n",
       "      <td>0.007134</td>\n",
       "      <td>0.101687</td>\n",
       "      <td>-0.102530</td>\n",
       "      <td>-0.803639</td>\n",
       "      <td>0.206901</td>\n",
       "      <td>0.101246</td>\n",
       "      <td>0.591757</td>\n",
       "      <td>0.509515</td>\n",
       "      <td>-0.398274</td>\n",
       "      <td>-0.206521</td>\n",
       "      <td>-0.400001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variance</th>\n",
       "      <td>0.022582</td>\n",
       "      <td>0.023487</td>\n",
       "      <td>0.014396</td>\n",
       "      <td>0.019696</td>\n",
       "      <td>0.018793</td>\n",
       "      <td>0.026043</td>\n",
       "      <td>0.025371</td>\n",
       "      <td>0.025872</td>\n",
       "      <td>0.025418</td>\n",
       "      <td>0.020619</td>\n",
       "      <td>0.021030</td>\n",
       "      <td>0.014724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE</th>\n",
       "      <td>0.022622</td>\n",
       "      <td>0.023538</td>\n",
       "      <td>0.024736</td>\n",
       "      <td>0.030208</td>\n",
       "      <td>0.664629</td>\n",
       "      <td>0.068851</td>\n",
       "      <td>0.035622</td>\n",
       "      <td>0.376048</td>\n",
       "      <td>0.285023</td>\n",
       "      <td>0.179241</td>\n",
       "      <td>0.063681</td>\n",
       "      <td>0.174724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          $\\alpha_{11,1}$  $\\alpha_{11,2}$  $\\alpha_{11,1}$  $\\alpha_{12,2}$  \\\n",
       "Bias            -0.006333         0.007134         0.101687        -0.102530   \n",
       "Variance         0.022582         0.023487         0.014396         0.019696   \n",
       "MSE              0.022622         0.023538         0.024736         0.030208   \n",
       "\n",
       "          $\\alpha_{13,3}$  $\\alpha_{14,4}$  $\\alpha_{15,5}$  $\\alpha_{16,6}$  \\\n",
       "Bias            -0.803639         0.206901         0.101246         0.591757   \n",
       "Variance         0.018793         0.026043         0.025371         0.025872   \n",
       "MSE              0.664629         0.068851         0.035622         0.376048   \n",
       "\n",
       "          $\\alpha_{17,7}$  $\\alpha_{18,8}$  $\\alpha_{19,9}$  $\\alpha_{110,10}$  \n",
       "Bias             0.509515        -0.398274        -0.206521          -0.400001  \n",
       "Variance         0.025418         0.020619         0.021030           0.014724  \n",
       "MSE              0.285023         0.179241         0.063681           0.174724  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>$\\alpha_{21,1}$</th>\n",
       "      <th>$\\alpha_{21,2}$</th>\n",
       "      <th>$\\alpha_{21,1}$</th>\n",
       "      <th>$\\alpha_{22,2}$</th>\n",
       "      <th>$\\alpha_{23,3}$</th>\n",
       "      <th>$\\alpha_{24,4}$</th>\n",
       "      <th>$\\alpha_{25,5}$</th>\n",
       "      <th>$\\alpha_{26,6}$</th>\n",
       "      <th>$\\alpha_{27,7}$</th>\n",
       "      <th>$\\alpha_{28,8}$</th>\n",
       "      <th>$\\alpha_{29,9}$</th>\n",
       "      <th>$\\alpha_{210,10}$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bias</th>\n",
       "      <td>0.006304</td>\n",
       "      <td>-0.003466</td>\n",
       "      <td>-0.099784</td>\n",
       "      <td>0.098575</td>\n",
       "      <td>0.797930</td>\n",
       "      <td>0.204344</td>\n",
       "      <td>-0.100081</td>\n",
       "      <td>-0.607628</td>\n",
       "      <td>0.511929</td>\n",
       "      <td>0.400054</td>\n",
       "      <td>0.192282</td>\n",
       "      <td>0.403230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variance</th>\n",
       "      <td>0.020014</td>\n",
       "      <td>0.019790</td>\n",
       "      <td>0.013817</td>\n",
       "      <td>0.020131</td>\n",
       "      <td>0.018144</td>\n",
       "      <td>0.023008</td>\n",
       "      <td>0.026478</td>\n",
       "      <td>0.024434</td>\n",
       "      <td>0.024231</td>\n",
       "      <td>0.019696</td>\n",
       "      <td>0.020568</td>\n",
       "      <td>0.014404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE</th>\n",
       "      <td>0.020054</td>\n",
       "      <td>0.019802</td>\n",
       "      <td>0.023773</td>\n",
       "      <td>0.029848</td>\n",
       "      <td>0.654837</td>\n",
       "      <td>0.064764</td>\n",
       "      <td>0.036494</td>\n",
       "      <td>0.393646</td>\n",
       "      <td>0.286302</td>\n",
       "      <td>0.179738</td>\n",
       "      <td>0.057540</td>\n",
       "      <td>0.176998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          $\\alpha_{21,1}$  $\\alpha_{21,2}$  $\\alpha_{21,1}$  $\\alpha_{22,2}$  \\\n",
       "Bias             0.006304        -0.003466        -0.099784         0.098575   \n",
       "Variance         0.020014         0.019790         0.013817         0.020131   \n",
       "MSE              0.020054         0.019802         0.023773         0.029848   \n",
       "\n",
       "          $\\alpha_{23,3}$  $\\alpha_{24,4}$  $\\alpha_{25,5}$  $\\alpha_{26,6}$  \\\n",
       "Bias             0.797930         0.204344        -0.100081        -0.607628   \n",
       "Variance         0.018144         0.023008         0.026478         0.024434   \n",
       "MSE              0.654837         0.064764         0.036494         0.393646   \n",
       "\n",
       "          $\\alpha_{27,7}$  $\\alpha_{28,8}$  $\\alpha_{29,9}$  $\\alpha_{210,10}$  \n",
       "Bias             0.511929         0.400054         0.192282           0.403230  \n",
       "Variance         0.024231         0.019696         0.020568           0.014404  \n",
       "MSE              0.286302         0.179738         0.057540           0.176998  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reforming beta so that beta_re[j][i] is the centered ith estimated value of the jth coeff \n",
    "#  in the primary regression. \n",
    "beta_re = [[beta[i][0][j]-pscdata[0][1]['pcoeff'][j] \n",
    "            for i in range(pscdata[0][0]['nds'])]\n",
    "              for j in range(pscdata[0][0]['n_end']+pscdata[0][0]['n_exo'])]\n",
    "\n",
    "# Reforming alpha so that alpha_re[k][j][i] is the centered ith est value of the jth coeff, \n",
    "# in the secondary equation for the kth endogenous regressor.\n",
    "alpha_re = [[[alpha[i][k][j] - sc_coeff[k][j]  for i in range(pscdata[0][0]['nds'])]\n",
    "                             for j in range(len(inc_out)+pscdata[0][0]['n_exo'])] \n",
    "                             for k in range(inpt['n_end'])] \n",
    "\n",
    "# Mean each estimated coefficient's bias\n",
    "beta_m = [np.mean(beta_re[l]) for l in range(len(beta_re))]\n",
    "alpha_m = [[np.mean(alpha_re[k][l]) for l in range(len(alpha_re[k]))] \n",
    "                          for k in range(inpt['n_end']) ]\n",
    "\n",
    "# Sample varaince for each estimated coefficient\n",
    "beta_s = [np.std(beta_re[l])**2 for l in range(len(beta_re))]\n",
    "alpha_s =  [[np.std(alpha_re[k][l])**2 for l in range(len(alpha_re[k]))] \n",
    "                          for k in range(inpt['n_end']) ]\n",
    "\n",
    "# Calculating mean squared error\n",
    "beta_mse = [ abs(beta_m[i])**2+abs(beta_s[i]) for i in range(len(beta_m))]\n",
    "alpha_mse = [[abs(alpha_m[k][i])**2+abs(alpha_s[k][i]) for i in range(len(alpha_m[k]))] \n",
    "                                              for k in range(inpt['n_end'])]\n",
    "# Initializing summary dataframe with bias \n",
    "b1 = pd.DataFrame(beta_m,columns = ['Bias'])\n",
    "# Adding mse to summary df\n",
    "b1['MSE'] = beta_mse\n",
    "# Adding varaince to summary df\n",
    "b1['Variance'] = beta_s\n",
    "# Constructing row names \n",
    "beta_row = ([''.join(['$\\\\beta_{1,',str(i),'}$']) for i in range(1,pscdata[0][0]['n_end']+1)] \n",
    "        + [''.join(['$\\\\beta_{2,',str(i),'}$']) for i in range(1,pscdata[0][0]['n_exo']+1)])\n",
    "# Adding columns names to df\n",
    "beta_col = list(b1.columns)\n",
    "# Adding row names to df\n",
    "b1.index = list(beta_row)\n",
    "# List version of df for output\n",
    "beta_out = b1.values.tolist()\n",
    "\n",
    "# Constructing a list of summary data frames for coeff of each sec eqn's the same way as beta\n",
    "for k in range(inpt['n_end']):\n",
    "    a11 = pd.DataFrame(alpha_m[k],columns = ['Bias'])\n",
    "    a11['Variance'] = alpha_s[k]\n",
    "    a11['MSE'] = alpha_mse[k]\n",
    "    a11_row = ([''.join(['$\\\\alpha_{',str(k+1),'1,',str(i),'}$']) \n",
    "                             for i in range(1,pscdata[0][0]['n_exo']+1)] \n",
    "        + [''.join(['$\\\\alpha_{', str(k+1), inc_out[i].replace('W',''),',',str(i+1),'}$' ]) \n",
    "                             for i in range(len(inc_out))])\n",
    "    a11.index = a11_row\n",
    "    a11_out = a11.values.tolist()\n",
    "    if k == 0:\n",
    "        a1 = [a11]\n",
    "        alpha_out = [a11_out]\n",
    "        alpha_col = list(a11.columns)\n",
    "        alpha_row = [list(a11_row)]\n",
    "    elif k > 0:\n",
    "        a1.append(a11)\n",
    "        alpha_out.append(a11_out)\n",
    "        alpha_row.append(a11_row)\n",
    "        \n",
    "        \n",
    "# Dictionary or summary results for beta\n",
    "beta_results ={'beta_cntrd': beta_re, 'beta_sum_dat': beta_out, \n",
    "               'beta_sum_clmn': beta_col, 'beta_sum_row': beta_row }\n",
    "# Dictionary of summary results for alpha\n",
    "alpha_results = {'alpha_cntrd':alpha_re , 'alpha_sum_dat': alpha_out,\n",
    "                'alpha_sum_clmn': alpha_col, 'alpha_sum_row': alpha_row}\n",
    "\n",
    "display(b1.T)\n",
    "display(a1[0].T)\n",
    "display(a1[1].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 5.6 Exporting to JSON </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inpt['inc'] = inc_out\n",
    "psc_out = [inpt, beta_results, alpha_results]\n",
    "out_folder ='/Users/ericpenner/Google_Drive/Research/pan_sel_cntrl/output/'\n",
    "date = dt.datetime.now()\n",
    "output_filename = ''.join(['pscout_',str(date.month),'_'\n",
    "                           ,str(date.day),'_'\n",
    "                           ,str(np.random.randint(1000,2000)),'.json'])\n",
    "output_file_full = ''.join([out_folder,'/',output_filename])\n",
    "\n",
    "with open(output_file_full, 'w') as f_obj:\n",
    "    json.dump(psc_out, f_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 5.7 Interactive plotting of Alpha and Centered Beta List </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96474e6ecf1a4ecf993096e8cd576d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(), IntSlider(value=1, description='Coefficient:', layout=Layout(align_items='stretch', d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Plotting Function\n",
    "def betaden(w,beta,xl,indv,yl):\n",
    "    w = w-1\n",
    "    plt.close('all')\n",
    "    b = np.array(beta[w]).reshape(len(beta[w]),1)\n",
    "    h = [1.04*len(beta[w])**(-1/5)*np.std(b)]\n",
    "    bden = mvden(b,b,h,9).reshape(len(b),1)\n",
    "    bd = np.hstack((b,bden))\n",
    "    bd1 = bd[bd[:,0].argsort(),:]\n",
    "    f,ax = plt.subplots()\n",
    "    f.set_figheight(7)\n",
    "    f.set_figwidth(15)\n",
    "    ax.set_xlim((xl[0],xl[1]))\n",
    "    ax.set_ylim((0,yl))\n",
    "    ax.plot(bd1[:,0],bd1[:,1])\n",
    "    ax.grid(which = 'both')\n",
    "    ax.set_title(''.join(['Distribution of Estimated ',indv[w]]))\n",
    "    plt.show()\n",
    "\n",
    "aj = 0\n",
    "\n",
    "box_hlayout = ipw.Layout(display='flex',flex_flow='row',align_items='stretch'\n",
    "                         ,width='90%')\n",
    "wbs = ipw.IntSlider( min = 1 , max = len(beta_re) , value = 1, step = 1\n",
    "                    , description = 'Coefficient:',width = 'auto',layout = box_hlayout\n",
    "                    , style = {'description_width': 'initial'} )\n",
    "wbsx = ipw.FloatRangeSlider( value=[-0.4, 0.4], min=-0.5,max=0.5, step=0.05\n",
    "                            ,description='x limits:',disabled=False\n",
    "                            ,continuous_update=False, orientation='horizontal',readout=True\n",
    "                            ,readout_format='.1f', width = 'auto', layout=box_hlayout\n",
    "                            ,style = {'description_width': 'initial'})\n",
    "was = ipw.IntSlider( min = 1 , max = len(alpha_re[0]) , value = 1, step = 1\n",
    "                    ,description = 'Coefficient:'\n",
    "                    ,width = 'auto',layout = box_hlayout\n",
    "                    ,style = {'description_width': 'initial'} )\n",
    "wasx = ipw.FloatRangeSlider( value=[-1.4, 1.4], min=-1.5,max=1.5\n",
    "                            ,step=0.05,description='x limits:',disabled=False\n",
    "                            ,continuous_update=False, orientation='horizontal',readout=True\n",
    "                            ,readout_format='.1f', width = 'auto', layout=box_hlayout\n",
    "                            ,style = {'description_width': 'initial'})\n",
    "\n",
    "bout =  ipw.interactive_output(betaden ,{'w': wbs,'beta':ipw.fixed(beta_re),'xl': wbsx,\n",
    "                                         'indv':ipw.fixed(beta_row),'yl':ipw.fixed(12)})\n",
    "aout =  ipw.interactive_output(betaden,{'w': was,'beta':ipw.fixed(alpha_re[aj]),'xl': wasx,\n",
    "                                        'indv':ipw.fixed(alpha_row[aj]),'yl':ipw.fixed(6.5)})\n",
    "ipw.VBox([bout,wbs,wbsx,aout,was,wasx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "289px",
    "left": "910px",
    "right": "20px",
    "top": "120px",
    "width": "333px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
