\documentclass[9pt]{beamer}
\usepackage[bars]{beamerthemetree} 
\usepackage{graphicx,comment} 
\usepackage{natbib,float, multirow,tikz,caption,amsmath}
\usetikzlibrary{calc}
\usetikzlibrary{snakes}
\newcommand\convp{\stackrel{p}{\rightarrow}}
\renewcommand*{\familydefault}{\rmdefault} 
\usecolortheme{crane} 
\newcommand\convd{\stackrel{d}{\rightarrow}}
\newcommand\kh{K\left( \frac{X_i-x}{h_n} \right)}
\newcommand\khd{K\left( \frac{X_{id}-x}{h_n} \right)}
\newcommand\kpsi{K\left( \frac{\psi-x}{h_n} \right)}
\newcommand\xh{\left( \frac{X_i-x}{h_n} \right)}
\newcommand\xhd{\left( \frac{X_{id}-x}{h_n} \right)}
\newcommand\xpsi{\left( \frac{\psi-x}{h_n} \right)}
\newcommand\logf{logf_{y|x}}
\newcommand\sumin{\sum_{i=1}^n}
\newcommand\sumtn{\sum_{i=1}^n}
\newcommand\avein{\frac{1}{n}\sum_{i=1}^n}
\newcommand\ntoi{n \rightarrow \infty}
\newcommand\convd{\stackrel{d}{\rightarrow}}
\newcommand\convas{\stackrel{a.s.}{\rightarrow}}
\newcommand\convp{\stackrel{p}{\rightarrow}}
\newcommand\supg{\underset{x\in G}{\mbox{sup}}}
\newcommand\supt{\underset{\xi\in \xi}{\mbox{sup}}}
\newcommand\fpar{\frac{\partial}{\partial \xi}}
\newcommand\spar{\frac{\partial^2}{\partial \xi \partial \xi'}}
\newcommand{\mbf}[1]{\mathbf{ #1 } }
\newcommand{\dmbf}[1]{ \dot{\mathbf{ #1 }} }
\newcommand{\hmbf}[1]{ \hat{\mathbf{ #1 }} }
\newcommand{\tmbf}[1]{ \tilde{\mathbf{ #1 }} }
\newcommand{\mbs}[1]{\boldsymbol{ #1} } 
\newcommand{\dmbs}[1]{ \dot{\boldsymbol{ #1} } }
\newcommand{\hmbs}[1]{ \hat{\boldsymbol{ #1} } }
\newcommand{\tmbs}[1]{ \tilde{\boldsymbol{ #1} } }
\newcommand{\highlight}[1]{%
  \colorbox{green!30}{$\displaystyle#1$}}
\DeclareMathOperator{\diag}{diag}
\usefonttheme{structureitalicserif}
\title{Selection of Heterogeneous Instruments in Partially Linear Fixed Effects Panel Regression }
\subtitle{ or the clickbait version: \\ Can You Select Heterogeneous Instruments in a Partially Linear Fixed Effects Panel Regression? A Machine Learning Approach  }

\author{By: Eric Penner} 

\begin{document}

\begin{frame} 
\frametitle{Presentation Documents} 

\begin{itemize} 
	\item 
\end{itemize} 

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\frame{\titlepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\begin{frame} 
\frametitle{Introduction pt.1} 

Basic Elements
\begin{itemize} \addtolength{\itemsep}{\baselineskip}
    \item Linear in parameters fixed effects panel model
    \item Subset of regressors are endogenous
    \item Relevant instruments vary by cross section 
    \item Relevant instruments are unknown subset of larger collection
\end{itemize}
 
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame} 
\frametitle{Introduction pt.2} 


Applications / Motivation
\begin{itemize}\addtolength{\itemsep}{\baselineskip}
    \item Spatial distance separating cross sections 
    \item Lack of sufficient domain/out of sample information
    \item Growth and Foreign Aid Models: Burnside and Dollar (2000) AER  
    \item Sensitivity to Set of Instruments:    Leon-Gonzalez et. al. (2015) JoMa
    \item Explicitly incorporate model selection and battling overfitting
    \item Replacing ad hoc procedures
\end{itemize}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Basic Model} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{frame} 
\frametitle{Initial Model} 

\begin{align} 
Y_{jt} &= \beta_0 + Z_{1jt}'\beta_1 + Z_{2jt}'\beta_2 + e_j + \varepsilon_{jt} \\[10pt]
%
Z_{1jdt} &= \alpha_{jd0} + Z_{2jt}' \alpha_{1d} + W_{jdt}' \alpha_{2jd} + V_{jdt} \tag{2d} \\[10pt]
%
\addtocounter {equation} {1}
E(&\varepsilon_{jt} | Z_{1jt} , Z_{2jt}) = E(\varepsilon_{jt} | Z_{1jt}) \neq 0  \\[10pt]
%
E(& V_{jdt} | Z_{2jt},W_{jt}) = 0 
%
\end{align}

\begin{itemize} \addtolength{\itemsep}{\baselineskip}
	%\item $\{ Y_i,X_i,W_i\}_{i=1}^n$ is an i.i.d sequence of random variables
	    \item $Y_{jt}$ is a scalar random variable
	    \item $ Z_{1jt} \in \mathbb{R}^{p_1}$, $ Z_{2jt} \in \mathbb{R}^{p_2}$ are endogenous and exogenous variables respectively
	    \item $e_j$ is a fixed effect, $\alpha_{jd0}$ is a combined constant and fixed effect
	    \item $ W_{t} \in \mathbb{R}^w$ a vector of instrumental variables.
	    \item $W_{jdt} \in \mathbb{R}^{w_{jd}}$ unknown subvector of $W_t \in \mathbb{R}^w$. 	    \item $\varepsilon_{jt}$ and $V_{jdt}$ are a scalar error terms.
\end{itemize} 

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame} 
\frametitle{Primary Assumptions} 

	\begin{enumerate}[i.)] \addtolength{\itemsep}{\baselineskip}
	    \item Exclusion Restriction: $E( \varepsilon_{jt}|Z_{1jt}  ) = E( \varepsilon_{jt} | Z_{2jt},W_{t},V_{jt}) = E(\varepsilon_{jt} | V_{jt})$
	    \item Control Function: $\varepsilon_{jt} = f_j(V_{jt}) + u_{jt} $	    \item Orthogonality: $E[u_{jt} |Z_{1jt},Z_{2jt},W_t] = 0$ 
	    \item Additivity: $ f_j(V_{jt}) = \sum_{d} f_{jd}(V_{jdt})$ 
	    \item Panel Secondary Equation: $W_{jdt}'\alpha_{2jd} = W_t'M_{jd}\alpha_{2d}$ where $M_{jd}=diag(m_{jd})$ and
	    \begin{align*} 
	    m_{jd} = \begin{bmatrix} 1\{W_{1t} \in W_{jdt}\} & 1\{W_{2t} \in W_{jdt}\} & \cdots & 1\{W_{wt} \in W_{jdt}\}  \end{bmatrix}'
	    \end{align*}

	\end{enumerate}
\vspace{0.25cm}	
% Consequently,
%	\begin{align*} 
%	E( \varepsilon_{jt} | Z_{2jt},W_{t},V_{jt}) = E(\varepsilon_{jt} | V_{jt}) =  E(f_j(V_{jt}) + u_{jt} | V_{jt}) = f_j(V_{jt}) = \sum_{d=1}^{p_1} f_{jd}(V_{jdt}) \\
%	\end{align*}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame} 
\frametitle{First Differenced Model} 

Let $\Delta$ be the first difference in operator, and assume $\{Y_{jt},Z_{1jt},Z_{2jt},W_{t}\}_{t=1}^n$ is i.i.d 

\begin{align} 
\Delta Y_{jt} &= \Delta Z_{1jt}'\beta_1 + \Delta Z_{2jt}'\beta_2 + \sum_{d=1}^{p_1} \Delta f_{jd}(V_{jdt})  + \Delta u_{jt} \\[5pt]
%
Z_{1jdt} &= \alpha_{jd0} + Z_{2jt}' \alpha_{1d} + W_{t}'M_{jd} \alpha_{2d} + V_{jdt} \tag{2d} \\[10pt]
%
\addtocounter {equation} {1}
E(&\Delta u_{jt}|V_{jt},V_{j,t-1} ) =  0  \\[10pt]
%
E(& V_{jdt} | Z_{2jt},W_{jt}) = 0 
%
\end{align}
\vspace{0.5cm}
Residuals on residuals regression: Manzan and Zerom (2005) SaPL

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Identification}
\begin{frame} 
\frametitle{Projections and Notation} 
Density Ratio
\begin{align*} 
\phi_{jt} = \frac{ \prod_{d=1}^{p_1}p(V_{jdt},V_{jd(t-1)}) }{p(V_{jt},V_{j(t-1)})} 
\end{align*}
Conditional Expectations, for $k \in \{1,2\}$
\begin{align*} 
H_{jd}(\Delta Z_{kjt}) &= E[\phi_{jt} \Delta Z_{kjt} |V_{jdt},V_{jd(t-1)}] \hspace{1.25cm} 
%
H_{jd}(\Delta Y_{jt}) = E[\phi_{jt} \Delta Y_{jt} |V_{jdt},V_{jd(t-1)}] \\
%
H_j(\Delta Z_{kjt}) &= \sum_{d=1}^{p_1} H_{jd}(\Delta Z_{kjt})  \hspace{2.5cm}
%
H_j(\Delta Y_{jt}) = \sum_{d=1}^{p_1} H_{jd}(\Delta Y_{jt})  \end{align*} 
Vectors
\begin{align*}
H(\Delta Y_t)  &= [ \; H_1(\Delta Y_{1t}) \;\; H_2(\Delta Y_{2t}) \;\; \cdots \;\; H_q(\Delta Y_{qt}) \; ]' \\[7pt]
%
H(\Delta Z_{kt})  &= [ \; H_1(\Delta Z_{k1t}) \;\; H_2(\Delta Z_{k2t}) \;\; \cdots \;\; H_q(\Delta Z_{kqt}) \; ]' \\[7pt]
%
\Delta Y_t &= [ \; \Delta Y_{1t} \;\;  \Delta Y_{2t} \;\; \cdots \;\; \Delta Y_{qt} \;] ' \\[7pt]
\Delta Z_{kt} &= [ \; \Delta Z_{k1t} \;\;  \Delta Z_{k2t} \;\; \cdots \;\; \Delta Z_{kqt} \;] ' 
\end{align*}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame} 
\frametitle{Identification Lemma} 


\large \emph{Lemma: Identification } \normalsize \\
\vspace{0.25cm}

Let $\phi_{t} = diag\big( \{\phi_{jt}\;\}_{j=1}^q \big)$, and 
\begin{align*}
\Delta Z_t = \begin{bmatrix}  \Delta Z_{1t}' & \Delta Z_{2t}' \end{bmatrix}' \hspace{0.5cm} H(\Delta Z_{t}) = \begin{bmatrix}  H(\Delta Z_{1t})' & H(\Delta Z_{2t})' \end{bmatrix}'
\end{align*}
Then if,\\

\begin{enumerate}[i.)] 
\item for all $d\in \{1,2, \ldots , p_1\}$, $j\in \{1,2, \cdots , q\}$, and $t\in \{2, \ldots , T\}$
 $$E\big[ f_{jd}(V_{jdt})\big] = E\big[ f_{jd}(V_{jd(t-1)}) \big] 
$$
%
\item $E \Big( [\Delta Z_t - H(\Delta Z_t)]'\phi_{t}  [\Delta Z_t - H(\Delta Z_t)] \Big)$ is positive semi definite,
%
\end{enumerate}
\small
Then $\beta_1$ and $\beta_2$ are identified, in particular,
\begin{align*} 
\begin{bmatrix} \beta_1 \\[10pt] \beta_2 \end{bmatrix} = E \big( [\Delta Z_t - H(\Delta Z_t)]' \phi_{t} [\Delta Z_t - H(\Delta Z_t)] \big)^{-1}E \big( [\Delta Z_t - H(\Delta Z_t)]' \phi_{t} [\Delta Y_t - H(\Delta Y_t)] \big)
\end{align*} 
\normalsize

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Estimation}
\begin{frame} 
\frametitle{Secondary Equation Estimation pt.1 } 


Must Estimate $\hat{V}_{jdt}$ as a first step \\
\begin{itemize} \addtolength{\itemsep}{\baselineskip}
    \item $W_{jdt} \in \mathbb{R}^{w_{jd}}$ unknown subvector of $W_t \in \mathbb{R}^w$. 
    \item Subset selection and coefficient estimation with Lasso
\end{itemize}




\vspace{0.5cm}
Why Subset / Model Selection? \\

\begin{itemize} \addtolength{\itemsep}{\baselineskip}

	\item Overfitting $E(Z_{1jdt} | Z_{2jt},W_{t})$ 
       
       \item If $T<< w$

\end{itemize} 
\vspace{0.5cm}
 Lasso: Is the treatment worse than the original problem? \\

\begin{itemize} \addtolength{\itemsep}{\baselineskip}

	\item Underfitting $E(Z_{1jdt} | Z_{2jt},W_{t})$ 
       
      \item Overfitting on the sample 
     

\end{itemize} 



\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame} 
\frametitle{Sample Splitting} 

\begin{itemize}  \addtolength{\itemsep}{\baselineskip}
      \item Similar to Chernozhukov et al. (2017) NBER working paper 23564
	\item Let $n_p \in \mathbb{N}$ be the number of partitions to be generated
	\item Generate Partition $\{\mathcal{I}_g\}_{g=1}^{n_p}$ of $\{1,2,\cdots,T\}$ i.e.
	\begin{align*}
	\mathcal{I}_g \subset \{i\}_{i=1}^{T} \;\;\;\;\;\; \mathcal{I}_g \cap \mathcal{I}_{g'} = \emptyset \;\;\; \text{and} \;\;\;  \bigcup_{g=1}^{n_p} \mathcal{I}_g = \{i\}_{i=1}^{T}
	\end{align*}
       \item Model Selection and Projection Estimation with Training Set: $\mathcal{I}^c_g$
       \item Estimation of $[ \; \hat{\beta} _{g1}' \;\; \hat{\beta}_{g2}' \;]'$ with Testing Set: $\mathcal{I}_g$  
       \item Final Estimator:
       \begin{align*}
        [ \; \hat{\beta} _{1}' \;\; \hat{\beta}_{2}' \;]' = n_p^{-1} \sum_{g=1}^{n_p} [ \; \hat{\beta} _{g1}' \;\; \hat{\beta}_{g2}' \;]'
       \end{align*}
 
\end{itemize} 

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{frame} 
\frametitle{Secondary Equation Estimation pt.2} 

First Differenced Model
\begin{align*} 
\Delta Z_{1jdt} =\Delta Z_{2jt}' \alpha_{1d} + \Delta W_{t}'M_{jd} \alpha_{2d} + \Delta V_{jdt}
\end{align*}
Lasso Estimator with Training Set $\mathcal{I}^c_g$
\begin{align*} 
(\hat{\alpha}_{1d},\hat{\alpha}_{2jd})  = \arg \min \sum_{j=1}^q\sum_{i \in \mathcal{I}^c_g}\left( \Delta Z_{1jdi} -  \Delta Z_{2ji}'\alpha_{1} - \Delta W_{i}'\alpha_{2j} \right)^2 \;\; \text{ s.t. } \;\; \sum_{l=1}^w|\alpha_{2j,l}| \leq \lambda 
\end{align*}
for all $1 \leq j \leq q$.  

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame} 
\frametitle{Secondary Equation Estimation pt.3} 

Next define for some $\varepsilon > 0$

\begin{align*} 
\tilde{\alpha}_{2d,l} = \frac{\sum_{j=1}^q \hat{\alpha}_{2jd,l} 1\{ |\hat{\alpha}_{2jd,l}| > \varepsilon \} }{ \sum_{j=1}^q 1\{| \hat{\alpha}_{2jd,l}| > \varepsilon\}     + 1\{\sum_{j=1}^q 1\{ |\hat{\alpha}_{2jd,l}| > \varepsilon\} =0 \} }
\end{align*}

Now let
$$\tilde{\alpha}_{2d} = [ \; \tilde{\alpha}_{2d,1} \;\; \tilde{\alpha}_{2d,2} \;\; \cdots \;\; \tilde{\alpha}_{2d,w}  \; ]'$$

and 
\begin{align*} \tilde{m}_{jd} = \begin{bmatrix}  1\{|\hat{\alpha}_{2jd,1}| > \varepsilon \} &   1\{|\hat{\alpha}_{2jd,2}| > \varepsilon \} & \cdots &   1\{|\hat{\alpha}_{2jd,w}| > \varepsilon \} \end{bmatrix}' \end{align*} 

so that, for all $t\in \{1,2, \cdots , T\}$ 
\begin{align*}
\hat{V}_{jdt} = Z_{1jdt} - Z_{2jt}'\hat{\alpha}_{1d} - W_{t}'\tilde{M}_{jd}\tilde{\alpha}_{2d} - \#(\mathcal{I}^c_g) ^{-1}\sum_{i \in \mathcal{I}^c_g}  (Z_{1jdi} - Z_{2ji}'\hat{\alpha}_{1d} - W_{i}'\tilde{M}_{jd}\tilde{\alpha}_{2d}) 
\end{align*}

where $\tilde{M}_{jd} = diag(\tilde{m}_{jd})$

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame} 
\frametitle{Density Ratio Estimation} 

Density Estimation: for all  $j\in \{1,2,\cdots,q\}$, $d \in \{1,2, \cdots,p_1\}$, and $t \in \{2,\cdots,T\}$

\begin{align*} 
 \hat{p}(\hat{V}_{jdt},\hat{V}_{jd(t-1)}) 
&= (n(t) h_1^2)^{-1}\sum_{  i \in \mathcal{I}_g^c, i\neq t  }k\left( \frac{\hat{V}_{jdi} -\hat{V}_{jdt} }{h_1} \right)k\left( \frac{\hat{V}_{jd(i-1)} -\hat{V}_{jd(t-1)}}{h_2} \right)  \\
%
 \hat{p}(\hat{V}_{jt},\hat{V}_{j(t-1)}) 
&= (n(t)h_2^{2p_1})^{-1}\sum_{ i \in \mathcal{I}_g^c, i\neq t } \prod_{d=1}^{p_1} k\left( \frac{\hat{V}_{jdi} -\hat{V}_{jdt} }{h_2} \right) k\left(\frac{\hat{V}_{jd(i-1)} -\hat{V}_{jd(t-1)} }{h_2} \right) 
\end{align*}

where
$$
n(t)= \begin{cases} \#(\mathcal{I}_g^c)  &\mbox{if } t \in  \mathcal{I}_g \\ 
\#(\mathcal{I}_g^c)-1  &\mbox{if } t \in  \mathcal{I}_g^c \end{cases}
$$

\vspace{0.25cm}

Density Ratio Construction: 
\begin{align*} 
\hat{\phi}_{jt} = \frac{ \prod_{d=1}^{p_1}\hat{p}(\hat{V}_{jt,d},\hat{V}_{j(t-1),d}) }{\hat{p}(\hat{V}_{jt},\hat{V}_{j(t-1)})}
%
\hspace{1cm}
%
\hat{\theta}_{jdt} = \frac{ \prod_{l \neq d}^{p_1}\hat{p}(\hat{V}_{jlt},\hat{V}_{jl(t-1)}) }{\hat{p}(\hat{V}_{jt},\hat{V}_{j(t-1)})}
\end{align*}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame} 
\frametitle{H Function Estimation} 

H Function Estimation: for each $a\in \{1,2\}$, $j\in \{1,2,\cdots,q\}$, $d \in \{1,2, \cdots,p_1\}$, $\ell \in \{1,2,\cdots,p_a\}$, and $k \in \mathcal{I}_g$
\begin{align*} 
 \hat{H}_{jd}(\Delta Z_{aj\ell k}) = [n_{\mathcal{I}_g^c}h_3]^{-1} \sum_{  i \in \mathcal{I}_g^c }k\left( \frac{\hat{V}_{jdi} -\hat{V}_{jdk} }{h_3} \right)k\left( \frac{\hat{V}_{jd(i-1)} -\hat{V}_{jd(k-1)}}{h_3} \right)  \hat{\theta}_{jdi} \Delta Z_{aj\ell i}
 \end{align*}
and for each $j\in \{1,2,\cdots,q\}$, $d \in \{1,2, \cdots,p_1\}$, and $k \in \mathcal{I}_g$

\begin{align*} 
 \hat{H}_{jd}(\Delta Y_{j k}) = [n_{\mathcal{I}_g^c}h_3]^{-1} \sum_{  i \in \mathcal{I}_g^c }k\left( \frac{\hat{V}_{jdi} -\hat{V}_{jdk} }{h_3} \right)k\left( \frac{\hat{V}_{jd(i-1)} -\hat{V}_{jd(k-1)}}{h_3} \right)  \hat{\theta}_{jdi} \Delta Y_{j i}
 \end{align*}
 
 then construct 
    \begin{align*} 
     \hat{H}_{j}(\Delta Z_{aj\ell k}) &= \sum_{d=1}^{p_1}  \hat{H}_{jd}(\Delta Z_{aj\ell k})   \hspace{1.5cm}
%
\hat{H}_j(\Delta Y_{jk}) = \sum_{d=1}^{p_1} \hat{H}_{jd}(\Delta Y_{jk}) 
    \end{align*}
the rest is book keeping. 
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame} 
\frametitle{Final Estimator for Testing Set $\mathcal{I}_g$ } 

Let $\hat{\phi} = diag\big( \{\hat{\phi}_k\}_{k \in \mathcal{I}_g} \big)$

    \begin{align*} 
\begin{bmatrix} \hat{\beta}_{g1} \\[10pt] \hat{\beta}_{g2} \end{bmatrix} = \Big( [\Delta Z - \hat{H}(\Delta Z)]' \hat{\phi} [\Delta Z - \hat{H}(\Delta Z)] \Big)^{-1} \Big( [\Delta Z - \hat{H}(\Delta Z)]' \hat{\phi} [\Delta Y - \hat{H}(\Delta Y)] \Big)
    \end{align*}


Comparing the sampling distribution of $\hat{\beta}_1$ and $\hat{\beta}_2$.
\begin{itemize}  \addtolength{\itemsep}{\baselineskip}
    \item Oracle v.s. Known Subset v.s. Unknown Subset v.s. Lasso
    \item Varying total number of instruments available $w$
    \item Varying number of cross sections $q$
    \item Varying number of time periods $T$ 
\end{itemize}



\end{frame}

\section{Monte Carlo Data Generating Process}
\begin{frame} 
\frametitle{Equivalences and Covariances} 

Let 
\begin{itemize} \addtolength{\itemsep}{\baselineskip}
     \item $n_{tp} \equiv T$ be the total number of time periods.
     \item $n_{end} \equiv p_1$ be the number of endogneous regressors
     \item  $n_{exo} \equiv p_2$ be the number of exogenous regressors  
     \item  $n_{tinst} \equiv w$ be the total number of available instruments
     \item $ n_{cinst} \equiv w_j$  the number of instruments relevant to each crossection
 \end{itemize}   
    
    
$$
\begin{align*} 
\rho_{er} &= \begin{bmatrix} \rho_{er,1} & \rho_{er,2} & \cdots & \rho_{er,n_{end}} \end{bmatrix} \\[10pt]
\rho_{inst} &= \begin{bmatrix} \rho_{inst,1} & \rho_{inst,2} & \cdots & \rho_{inst,n_{inst}-1} \end{bmatrix}\\[10pt]
\rho_{ex} &= \begin{bmatrix} \rho_{ex,1} & \rho_{ex,2} & \cdots & \rho_{ex,n_{ex}-1} \end{bmatrix}  
\end{align*}
$$


be vectors of covariances.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame} 
\frametitle{Error Covariance Matrix } 

For each cross section

$$
\begin{align*}
V_{er} &= \begin{bmatrix} 
1 & \rho_{er,1} & \rho_{er,2} & \cdots & \rho_{er,n_{end}} \\[8pt]
\rho_{er,1} & 1  & \rho_{er,1} &\cdots & \rho_{er,n_{end}-1} \\[8pt]
\rho_{er,2} & \rho_{er,1} & 1 & \cdots & \rho_{er,n_{end}-2} \\[8pt]
\vdots & &&\ddots&  \\[8pt]
 \rho_{er,n_{end}} & \rho_{er,n_{end}-1} & \rho_{er,n_{end}-2} & \cdots &  1 
\end{bmatrix}
\end{align*}
$$

For all cross sections

$$
\begin{align*}
CV_{er} &= 
\begin{bmatrix}
V_{er} & \mathbf{0}_{(n_{end}+1 \times n_{end}+1)} & \cdots & \mathbf{0}_{(n_{end}+1 \times n_{end}+1)}  \\[8pt]
\mathbf{0}_{(n_{end}+1 \times n_{end}+1)} & V_{er} & \cdots & \mathbf{0}_{(n_{end}+1 \times n_{end}+1)}  \\[8pt]
\vdots & \vdots & \ddots & \vdots \\[10pt]
\mathbf{0}_{(n_{end}+1 \times n_{end}+1)} & \mathbf{0}_{(n_{end}+1 \times n_{end}+1)} & \cdots & V_{er}
\end{bmatrix} 
\end{align*}
$$

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame} 
\frametitle{ Exogenous Variable Covariance Matrix} 

For each cross section

$$ 
\begin{align*}
V_{ex} &= \begin{bmatrix} 
1 & \rho_{ex,1} & \rho_{ex,2} & \cdots & \rho_{ex,n_{ex}-1} \\[8pt]
\rho_{ex,1} & 1  & \rho_{ex,1} &\cdots & \rho_{ex,n_{ex}-2} \\[8pt]
\rho_{ex,2} & \rho_{ex,1} & 1 & \cdots & \rho_{ex,n_{ex}-3} \\[8pt]
\vdots & &&\ddots&  \\[8pt]
 \rho_{ex,n_{ex}-1} & \rho_{ex,n_{ex}-2} & \rho_{ex,n_{ex}-2} & \cdots &  1 
\end{bmatrix}
\end{align*} 
$$

For all cross sections

$$
\begin{align*}
CV_{ex}  = 
\begin{bmatrix}
V_{ex} & \mathbf{0}_{(n_{ex} \times n_{ex})} & \cdots & \mathbf{0}_{(n_{ex} \times n_{ex})}  \\[8pt]
\mathbf{0}_{(n_{ex} \times n_{ex})} & V_{ex} & \cdots & \mathbf{0}_{(n_{ex} \times n_{ex})}  \\[8pt]
\vdots & \vdots & \ddots & \vdots \\[8pt]
\mathbf{0}_{(n_{ex} \times n_{ex})} & \mathbf{0}_{(n_{ex} \times n_{ex})} & \cdots & V_{ex}
\end{bmatrix} 
\end{align*}
$$

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame} 
\frametitle{Common Instrument Covariance Matrix} 

$$
V_{inst} = \begin{bmatrix} 
1 & \rho_{inst,1} & \rho_{inst,2} & \cdots & \rho_{inst,n_{tinst}-1} \\[10pt]
\rho_{inst,1} & 1  & \rho_{inst,1} &\cdots & \rho_{inst,n_{tinst}-2} \\[10pt]
\rho_{inst,2} & \rho_{tinst,1} & 1 & \cdots & \rho_{inst,n_{tinst}-3} \\[10pt]
\vdots & &&\ddots&  \\[10pt]
 \rho_{inst,n_{tinst}-1} & \rho_{inst,n_{tinst}-2} & \rho_{inst,n_{tinst}-3} & \cdots &  1 
\end{bmatrix}
%
$$

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame} 
\frametitle{Exogenous Variable Generation} 

Let 
$$ 
\begin{align*} 
Z_{2jt} &= \begin{bmatrix} Z_{2jt,1} & Z_{2jt,2} & \cdots & Z_{2jt,n_{ex}} \end{bmatrix}' \\[10pt]  
\tilde{V}_{jt} &= \begin{bmatrix}  \varepsilon_{j}  & V_{j1t} & V_{j2t}& \cdots & V_{jn_{end}t}\end{bmatrix}' \\[10pt]
\end{align*} 
$$

Then generate $\{Z_{1t},Z_{2t},W_t\}_{t=1}^{ntp}$ from the following distributions

\begin{align*}
& \begin{bmatrix} W_{1t} & W_{2t}& \cdots & W_{t,n_{inst}} \end{bmatrix} \sim N(\mathbf{0}_{n_{inst} \times 1}, CV_{inst}) \\[10pt]
%
&\begin{bmatrix} Z_{21t}' & Z_{22t}' & \cdots & Z_{2n_{cs}t}' \end{bmatrix}' \sim N(\mathbf{0}_{n_{cs} \cdot n_{exo} \times 1}, CV_{ex}) \\[10pt]
%
&\begin{bmatrix} \tilde{V}_{1t}' & \tilde{V}_{2t}' & \cdots & \tilde{V}_{n_{cs}t}' \end{bmatrix}' \sim N(\mathbf{0}_{n_{cs} \cdot (n_{end} +1) \times 1}, CV_{er})
\end{align*}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame} 
\frametitle{Endogenous Variable Generation} 

\begin{itemize}\addtolength{\itemsep}{\baselineskip}
    \item Randomly Draw $\alpha_{1d} \in [1,-1]^{n_{exo}} $ for each $d \in \{1,2,\cdots,n_{end} \}$    
    \item  Randomly Draw $\alpha_{2d} \in [1,-1]^{n_{tinst}} $ for each $d \in \{1,2,\cdots,n_{end} \}$  
    \item  Randomly draw a set of integers from $\mathcal{C}^{n_{tinst}}_{n_{cinst}}$ ways that that you can choose $n_{cinst}$ instruments from $n_{tinst}$ total instrument, for each  $j\in \{1,2,\cdots , n_{cs}\}$  
    \item  Map that set of integers to a binary vector $m_j$ indicating the integers drawn above. 
    \item  Let $M_j = \text{diag}(m_j)$, and generate the following
 
    
    $$ Z_{1jd} =  \alpha_{0jd} + Z_{2jt}' \alpha_{1d} + W_{t}'M_j \alpha_{2d} + V_{jt,d} \hspace{.3cm} \text{ where } \hspace{.3cm} \alpha_{0jd} = 1/2+j/2 $$ 
  

\end{itemize}   
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame} 
\frametitle{Regressand Generation} 

\begin{itemize}

     \item Draw the coefficient vector $[ \; \beta_1' \;\; \beta_2' \;]' \in [1,-1]^{n_{end} +n_{end}}$, and generate the following 
$$ Y_{jt} &= \beta_0 + Z_{1jt}'\beta_1 + Z_{2jt}'\beta_2 + e_j + \varepsilon_{jt}\;\;\;\; \text{ where } \;\;\;\;  e_{j} = 1+j/2  $$

Note due the manner that the error vector is generated, 

\begin{align*} 
E(\varepsilon_{jt} |V_{jt} ) = \rho_{er}' V_{er(2,2)}^{-1} \begin{bmatrix} V_{j1t}  \\   V_{j2t} \\ \vdots \\ V_{jqt} \end{bmatrix} 
\end{align*}
where 
\begin{align*} 
V_{er} = \begin{bmatrix} 1 & \rho_{er}' \\
 \rho_{er} & V_{er(22)} 
 \end{bmatrix}
\end{align*}



\end{itemize} 

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame} 
\frametitle{...} 

\begin{itemize} 
	\item 
\end{itemize} 

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%














